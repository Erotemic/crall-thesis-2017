% +--- CHAPTER --- 
\begin{comment}
    ./texfix.py --fpaths chapter3-matching.tex --outline --asmarkdown --numlines=999 -w
    ./texfix.py --fpaths chapter3-matching.tex --outline --asmarkdown --numlines=999 -w
    ./texfix.py --fpaths chapter3-matching.tex --reformat 
    # http://jaxedit.com/mark/
\end{comment}



\chapter{Identification using a ranking algorithm}\label{chap:ranking}

    This chapter addresses the problem of computer-assisted animal identification, where an algorithm suggests
    likely possibilities, but a human reviewer always makes the final decision. Given, a single annotation
    depicting an unknown animal and a database of previously identified annotations, the task is to determine a
    ranking of database individuals most likely to match the unknown animal. A manual reviewer determines which ---
    if any --- of the top ranked results are correct.

    An \glossterm{annotation} is a rectangular region  of interest around a specific animal within an image. Each
    known annotation is associated with a \name{} label denoting its individual identity. A \glossterm{\name{}}
    refers to a group of annotations known to be the same individual. The identification process assigns a \name{}
    label to an unknown annotation either as (1) the \name{} label of a matched database annotation or (2) a new
    \name{} label if no matches are found.

    %A static context is chosen for this chapter in order to introduce
    %  and determine the effectiveness of the algorithm that identifies a
    %  query by ranking the \names{} in the database.
    %In the static context, only a single query annotation is used to
    %  perform identification, all \name{} labels in the database are
    %  assumed to be correct, and annotations are not added to or removed
    %  from the database.
    %This is done to determine what properties of query annotations,
    %  configurations of the database, and parameters of the
    %  identification algorithm have the highest impact on identification
    %  accuracy.
    %In~\cref{chap:application}, the identification algorithm is
    %  extended for use in a dynamic context.
    %In the dynamic context, annotations are added and removed from the
    %  database, multiple annotations are used to perform identification,
    %  and the database is assumed to contain errors.

    The ranking algorithm is based on the feature correspondences between a query annotation and a set of database
    annotations. In each annotation a set of patch-based features is detected at keypoint locations. Then the
    visual appearance of each patch is described using SIFT~\cite{lowe_distinctive_2004}. A nearest neighbor
    algorithm establishes a set of feature correspondences between query and annotations in database. A scoring
    mechanism based on local \naive{} Bayes nearest neighbor (LNBNN)~\cite{mccann_local_2012} produces a score for
    each feature correspondence. These scores are then aggregated into a single score for each \name{} in the
    database, producing a ranked list of \names{}. Identification is performed by applying a classifier (decision
    algorithm) to the scores in this ranked list. If the top ranked \name{} has a ``high'' score it is likely to be
    the same individual depicted in the query annotation. If the top ranked \name{} has a ``low'' score it is
    likely that the query individual is not depicted in any database annotation. An example ranked list returned by
    the algorithm is illustrated in~\cref{fig:rankedmatches}. In the baseline algorithm this identification
    decision is left to a user.

    The outline of this chapter is as follows: \Cref{sec:annotrepr} discusses the initial processing of an
    annotation which involves image normalization, feature extraction, and feature weighting.
    \Cref{sec:baselineranking,sec:sver} describes the baseline ranking and scoring algorithm. The first of these
    sections focuses on establishing feature correspondences, and the second focuses on verifying the
    correspondences. \Cref{sec:exempselect} describes the process for selecting \exemplars{}.
    \Cref{sec:rankexpt} provides an experimental evaluation of the ranking algorithm.
    %These experiments inform the direction we take in our proposal to extend
    %  this algorithm.
    \Cref{sec:staticsum} summarizes this chapter.

    \rankedmatches{}

\input{sec-3-1-annotrepr.tex}
\input{sec-3-2-dbsearch.tex}
\input{sec-3-3-sver.tex}


\section{Exemplar selection}\label{sec:exempselect}
    To scale one-vs-many matching to larger databases and to allow the LNBNN mechanism to find appropriate
    normalizers we restrict the number of examples of each individual in the database to a set of exemplars.

    Exemplars that represent a wide range of viewpoints and poses are automatically chosen using a modified version
    of the technique presented in~\cite{oddone_mobile_2016}. The idea is to treat exemplar selection as a maximum
    weight set cover problem. For each individual, the input is a set of annotations. A similarity score is
    computed between pairs of annotations. To compute covering sets we first choose a threshold, each annotation is
    assigned a covering set as itself and the other annotations it matches with a similarity score above that
    threshold. The maximum number of exemplars is restricted by setting a maximum weight. Searching for the optimal
    set cover is NP-hard, therefore we use the greedy %
    $(1 - \frac{1}{e})$-approximation algorithm~\cite{michael_guide_1979}. The algorithm is run for several
    iterations in order to find a good threshold that minimizes the difference between the weight of the set cover
    and the maximum weight limit. The similarity score between annotations can be computed using the LNBNN scores,
    but a better choice is the of the algorithm we will later describe in \Cref{chap:pairclf} to produce the
    probability that a pair of annotation correctly matches.

\input{sec-3-4-expt.tex}



\section{Rank-based identification summary}\label{sec:staticsum}

    In this chapter we have addressed the problem of animal identification using a computer-assisted algorithm
      based on LNBNN that ranks a labeled database of \names{} by their similarity to a single query annotation.
    This algorithm beings by extracting local patch-based features from cropped and normalized chips.
    Features from database annotations are indexed for fast nearest neighbor search using a kd-tree.
    A mechanism based on LNBNN is used to compute a matching score for each database annotation.
    A shortlist of top scoring results have their feature correspondences spatially verified and then are
      re-scored.
    We have shown how this algorithm can be applied to individual animal identification and demonstrated that in
      a majority of cases the correct match is ranked first by our algorithm for plains zebras, Gr√©vy's zebras,
      Masai giraffes, and humpback whales.

    Because we have used the algorithm to curate the groundtruth we do not claim the reported accuracies in our
      experiments to be quantitatively absolute.
    However, qualitative evidence for the algorithm's overall success is provided by the facts that
    (1) we were able to use the algorithm to identify a significant number of individuals from different species
      and
    (2) our approach provides more accurate rankings than standard instance recognition techniques.
    We therefore make the conclusion that the algorithm is effective at identifying medium to high quality images
      of animals with distinguishing patterns when taken from similar viewpoints.
    %In addition we have comparative evidence that this problem is more
    %  difficult than location recognition.

    While we have demonstrated that the ranking algorithm accurately ranks correct matches when they exist, there
      are several limitations to this approach.
    \begin{enumln}
        \item All results must be manually verified, which can be a time consuming.
            %process for large datasets.
            %We introduced a mechanism for ranking the individuals in a database
            %based similarity to a single query, but provided no means of verifying
            %which --- if any --- of the top ranked results matched.
        \item There is no mechanism for recovering from errors once they occur.
        \item There is no mechanism to determine when identification is complete.
    \end{enumln}
    In the following chapters we seek to address these issues.
    In \Cref{chap:pairclf} we introduce an algorithm to make automatic decisions based on results from this
      algorithm, and in \cref{chap:graphid} we introduce a graph-based framework that determines identification
      confidence and introduces error recovery mechanisms.



% L___ CHAPTER ___
