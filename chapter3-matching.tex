% +--- CHAPTER --- 
\begin{comment}
    ./texfix.py --fpaths chapter3-matching.tex --outline --asmarkdown --numlines=999 -w
    ./texfix.py --fpaths chapter3-matching.tex --outline --asmarkdown --numlines=999 -w
    ./texfix.py --fpaths chapter3-matching.tex --reformat 
    # http://jaxedit.com/mark/
\end{comment}



\chapter{Identification using a ranking algorithm}\label{chap:ranking}

    This chapter addresses the problem of computer-assisted animal identification, where an algorithm suggests
    likely possibilities, but a human reviewer always makes the final decision. Given, a single annotation
    depicting an unknown animal and a database of previously identified annotations, the task is to determine a
    ranking of database individuals most likely to match the unknown animal. A manual reviewer determines which ---
    if any --- of the top ranked results are correct.

    An \glossterm{annotation} is a rectangular region  of interest around a specific animal within an image. Each
    known annotation is associated with a \name{} label denoting its individual identity. A \glossterm{\name{}}
    refers to a group of annotations known to be the same individual. The identification process assigns a \name{}
    label to an unknown annotation either as (1) the \name{} label of a matched database annotation or (2) a new
    \name{} label if no matches are found.

    %A static context is chosen for this chapter in order to introduce
    %  and determine the effectiveness of the algorithm that identifies a
    %  query by ranking the \names{} in the database.
    %In the static context, only a single query annotation is used to
    %  perform identification, all \name{} labels in the database are
    %  assumed to be correct, and annotations are not added to or removed
    %  from the database.
    %This is done to determine what properties of query annotations,
    %  configurations of the database, and parameters of the
    %  identification algorithm have the highest impact on identification
    %  accuracy.
    %In~\cref{chap:application}, the identification algorithm is
    %  extended for use in a dynamic context.
    %In the dynamic context, annotations are added and removed from the
    %  database, multiple annotations are used to perform identification,
    %  and the database is assumed to contain errors.

    The ranking algorithm is based on the feature correspondences between a query annotation and a set of database
    annotations. In each annotation a set of patch-based features is detected at keypoint locations. Then the
    visual appearance of each patch is described using SIFT~\cite{lowe_distinctive_2004}. A nearest neighbor
    algorithm establishes a set of feature correspondences between query and annotations in database. A scoring
    mechanism based on Local \Naive{} Bayes Nearest Neighbor (LNBNN)~\cite{mccann_local_2012} produces a score for
    each feature correspondence. These scores are then aggregated into a single score for each \name{} in the
    database, producing a ranked list of \names{}. Identification is performed by applying a classifier (decision
    algorithm) to the scores in this ranked list. If the top ranked \name{} has a ``high'' score it is likely to be
    the same individual depicted in the query annotation. If the top ranked \name{} has a ``low'' score it is
    likely that the query individual is not depicted in any database annotation. An example ranked list returned by
    the algorithm is illustrated in~\cref{fig:rankedmatches}. In the baseline algorithm this identification
    decision is left to a user.

    The outline of this chapter is as follows: \Cref{sec:annotrepr} discusses the initial processing of an
    annotation which involves image normalization, feature extraction, and feature weighting.
    \Cref{sec:baselineranking,sec:sver} describes the baseline ranking and scoring algorithm. The first of these
    sections focuses on establishing feature correspondences, and the second focuses on verifying the
    correspondences. \Cref{sec:exempselect} describes the process for selecting \exemplars{}.
    \Cref{sec:experiments} provides an experimental evaluation of the ranking algorithm.
    %These experiments inform the direction we take in our proposal to extend
    %  this algorithm.
    \Cref{sec:staticsum} summarizes this chapter.

    \rankedmatches{}

\input{sec-3-1-annotrepr.tex}
\input{sec-3-2-dbsearch.tex}
\input{sec-3-3-sver.tex}


\section{Exemplar selection}\label{sec:exempselect}
    To scale one-vs-many matching to larger databases and to allow the LNBNN mechanism to find appropriate
    normalizers we restrict the number of examples of each individual in the database to a set of exemplars.

    Exemplars that represent a wide range of viewpoints and poses are automatically chosen using a modified version
    of the technique presented in~\cite{oddone_mobile_2016}. The idea is to treat exemplar selection as a maximum
    weight set cover problem. For each individual, the input is a set of annotations. A similarity score is
    computed between pairs of annotations. To compute covering sets we first choose a threshold, each annotation is
    assigned a covering set as itself and the other annotations it matches with a similarity score above that
    threshold. The maximum number of exemplars is restricted by setting a maximum weight. Searching for the optimal
    set cover is NP-hard, therefore we use the greedy %
    $(1 - \frac{1}{e})$-approximation algorithm~\cite{michael_guide_1979}. The algorithm is run for several
    iterations in order to find a good threshold that minimizes the difference between the weight of the set cover
    and the maximum weight limit. The similarity score between annotations can be computed using the LNBNN scores,
    but a better choice is the of the algorithm we will later describe in \Cref{chap:pairclf} to produce the
    probability that a pair of annotation correctly matches.

\input{sec-3-4-expt.tex}



\section{Rank-based identification summary}\label{sec:staticsum}

    In this chapter we have addressed the problem of animal identification
      using a computer-assisted algorithm that ranks a labeled database of
      \names{} by their similarity to a single query annotation.
    %In this section we have introduced an algorithm that ranks known
    %  database \names{} by their similarity to a single query annotation.
    This algorithm beings by extracting local patch-based features from
      cropped and normalized chips.
    Features from database annotations are indexed for fast nearest neighbor
      search using a kd-tree.
    An mechanism based on LNBNN is used to compute a matching score for each
      database annotation.
    Based on these scores potential matches have their feature correspondences
      spatially verified and then are re-scored.
    We have shown how this algorithm can be applied to individual animal
      identification and demonstrated that in a majority of cases the correct
      match is ranked first by our algorithm.

    %This remainder of this section summarizes the conclusions drawn about the
    %  static identification algorithm in a broad sense and discusses possible
    %  directions for future work.

    Because we have used the algorithm to curate the groundtruth we do
      not claim the reported accuracies in our experiments to be
      quantitatively absolute.
    However, the fact that we were able to use the algorithm to
      identify a significant number of individuals from different species
      is qualitative evidence for the algorithm's overall success.
    From our experiments we conclude that the algorithm is effective at
      identifying medium to high quality images of animals with
      distinguishing patterns when taken from the same viewpoint.
    %In addition we have comparative evidence that this problem is more
    %  difficult than location recognition.


    %Plains zebras:
    %LNBNN: 75.6% @ rank 1
    %ASMK: 69.69% @ rank 1

    %Grevy's zebras: 
    %LNBNN: 84.94% @rank 1
    %ASMK: 70.87% @rank 1
    In our ranking experiments we only did extensive testing of the LNBNN algorithm.
    We have briefly experimented with using the vocabulary based SMK ranking algorithm (using the VLAD variant)
      and found LNBNN to provide superior results.
    In our preliminary experiments we found that the SMK algorithm was able to correct rank $69.69\percent$ of
      plains zebras and $70.87\percent$ of Grevy's zebras correctly at rank $1$.
    Comparable versions of LNBNN solutions achieved $75.6\percent$ and $69.7\percent$.

    While we have demonstrated that the ranking algorithm accurately ranks
      correct matches when they exist, there are several limitations to this
      approach.
    \begin{itemln}
        \item All results must be manually verified, which can be a time
          consuming process for large datasets.
            %We introduced a mechanism for ranking the individuals in a database
            %based similarity to a single query, but provided no means of verifying
            %which --- if any --- of the top ranked results matched.
        \item There is no mechanism for recovering from errors once they
          occur.
        \item There is no mechanism to determine when identification is
          complete.
    \end{itemln}
    In the following chapters we seek to address these issues.
    In \Cref{chap:pairclf} we introduce an algorithm to make automatic
      decisions based on results from this algorithm, and in \cref{chap:graphid}
      we introduce a graph-based framework that determines identification
      confidence and introduces error recovery mechanisms.



% L___ CHAPTER ___
