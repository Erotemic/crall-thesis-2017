
\section{Deep convolutional neural networks}\label{sec:dcnn}
    Convolutional networks have been around for over more than two decades~\cite{lecun_gradient_based_1998,
      fukushima_neocognitron_1988}.
    However, they did not receive major attention from computer vision researchers until 2012 when a deep
      convolutional neural network (DCNN)~\cite{krizhevsky_imagenet_2012} outperformed the best support vector
      machines (SVMs)~\cite{vapnik_statistical_1998} by over $10\percent$ in the ImageNet category recognition
      challenge~\cite{russakovsky_imagenet_2014}.
    Since then, many successful category recognition techniques based on DCNNs have been
      published~\cite{simonyan_very_2014, chatfield_efficient_2014, chatfield_return_2014, oquab_learning_2014,
      szegedy_going_2014, long_convnets_2014, he_spatial_2014, dean_fast_2013}.
    DCNNs have also been shown to produce excellent results when applied to other computer vision problems such
      as: %
    instance recognition~\cite{razavian_cnn_2014, razavian_baseline_2015, liu_learning_2015,
      held_deep_2015,arandjelovic_netvlad_2016,radenovic_cnn_2016}, %
    fine-grained recognition~\cite{branson_bird_2014, donahue_decaf_2013, catherine_wah_similarity_2014}, %
    detection~\cite{girshick_rich_2014, sermanet_overfeat_2013, li_wan_end_end_2015}, %
    face verification~\cite{huang_learning_2012, taigman_deepface_2014, sun_deep_2013}, %
    and learning similarity between feature patches~\cite{osendorfer_convolutional_2013, han_matchnet_2015,
      ng_exploiting_2015, zagoruyko_learning_2015, han_matchnet_2015}.
    The sudden success of deep nets has been attributed
    (1) a larger volume of available of training data, and
    (2) implementations using faster GPUs~\cite{krizhevsky_imagenet_2012}.
      
    Several techniques are employed to increase accuracy, reduce over-fitting,  and reduce training time.
    Data augmentation is used to artificially increase the amount of training
      data~\cite{ciresan_multi_column_2012, ciresan_high_performance_2011, simard_best_2003}.
    The dropout technique has been shown to reducing over-fitting~\cite{dahl_improving_2013,
      srivastava_dropout_2014}.
    At training time outputs of hidden units are randomly suppressed which forces the network to learn a more
      robust representation.
    It has been shown that dropout can be viewed as a form of model averaging~\cite{hinton_improving_2012}.
    Rectified linear units (ReLU) have been shown to be a faster alternative to the standard sigmoid activation
      functions~\cite{vinod_rectified_2010, dahl_improving_2013}.
    An ReLU is similar to a hinge function and simply outputs the signal of a unit if it is positive and outputs
      a zero otherwise.
    Leaky rectified linear units (LReLU) further improve network accuracy by including a ``leakiness'' term while
      maintaining the speed of ReLUs~\cite{maas_rectifier_2013}.
    While a ReLU strictly suppresses a feature activation if it is negative a LReLU returns a small negative
      signal (by multiplying by a constant) instead of zero.

    A deep neural network is constructed by stacking several layers of units (neurons) together.
    Data is used to initialize the activations of an input layer, and the information is forward propagated
      through the network.
    Weights are chosen to optimize a loss function --- \eg{} categorical cross-entropy error or triplet
      loss~\cite{schroff_facenet_2015} --- which is chosen depending on the application.
    Optimization of the loss function is performed using back-propagation~\cite{rumelhart_learning_1986} ---
      typically using mini-batches and stochastic gradient descent with momentum~\cite{sutskever_importance_2013}.
    Traditionally each layer in a neural network is fully connected --- each pair of units between the previous
      layer and the current layer has its own edge weight ---  to the previous layer.
    However, in computer vision networks are constructed using convolutional layers.

    A DCNN connects the input layer to a stack of convolutional layers~\cite{krizhevsky_imagenet_2012}.
    A convolutional layer differs from a fully connected layer in that it is sparsely connected and that most of
      the edge weights between layers are shared~\cite{lecun_gradient_based_1998, fukushima_neocognitron_1988,
      serre_robust_2007}.
    Each convolutional layer is broken into several channels.
    Each channel is given its own weight matrix with a fixed width and height.
    This matrix of weights is convolved with the input layer to produce a feature activation map, one for each
      channel.
    Convolutional layers often use several pooling layers that aggregate information over a small area, reduce
      the size of the feature map, and increase robustness to transformations.
    Common pooling operations are max-pooling~\cite{serre_robust_2007, krizhevsky_imagenet_2012} and
      maxout~\cite{goodfellow_maxout_2013}.
    The convolutional layers may also be connected to a stack of fully connected layers.
    In this case, hierarchies of feature maps are built in the low level convolutional layers, and then fully
      connected layers learn decision boundaries between these features~\cite{zeiler_visualizing_2014}.

    Because of weight sharing convolutional networks must learn significantly fewer parameters than fully
      connected networks.
    This allows convolutional networks to be trained much faster.
    Fewer weights also acts as a form of regularization for the network.
    Intuitively learned convolutional filters are similar to Gabor filters~\cite{gabor_theory_1946}, which are a
      naturally suited for extracting features from images.
    Even without learning weights, convolutions can be used to extract powerful features for
      matching~\cite{revaud_deep_2015}.
    The popular SIFT and HoG features~\cite{mahendran_understanding_2014} can even be implemented as
      convolutional networks.
    Despite the lack of hard theoretical insight into the inner workings of these networks, their empirical
      performance cannot be denied.

  \subsection{Discussion --- deep convolutional neural networks}\label{subsec:dcnndiscuss}
        Because of the astounding success of convolutional networks in almost every area in computer vision, we have
        investigated their use in animal identification. Specifically, we have investigated two approaches.

        The first approach used deep convolutional feature descriptors as a replacement for the
        SIFT~\cite{lowe_distinctive_2004} descriptor following the patch based scheme in~\cite{zagoruyko_learning_2015}.
        The basic idea is to have two patches fed through the same (Siamese)~\cite{chopra_learning_2005} architecture
        and then compare their resulting encodings. This comparison can be as simple as Euclidean distance, or as
        complex as a learned distance measure. Training can be performed on pairs of patches, labeled as correct or
        incorrect, using the discriminative loss function~\cite{lecun_loss_2005}. Unfortunately, due to issues with the
        quality and quantity of our training data our convolutional replacements for the SIFT descriptor have not been
        successful.

        The second approach aimed to use Siamese networks to directly compare two images of an animal to determine if
        they were the same or different, similar to the method used in DeepFace\cite{taigman_deepface_2014} for face
        verification. However, without the large training datasets and specialized alignment procedures used in Deep
        Face, we were unable to produce promising results.
        
        Due to these issues, this \thesis{} does not further pursue techniques based on DCNNs. We include this
        discussion to note the potential of deep learning applied to animal identification and to strongly suggest
        further investigation of these techniques in the future research. Of particular interest for future
        research is the matching technique presented in \cite{rocco_convolutional_2017}. This method is
        particularly promising because it learns to match and align images by mimicking a classic computer vision
        pipeline while using only synthetic training data. This may be able to overcome the issues mentioned above,
        however further investigation is needed.

