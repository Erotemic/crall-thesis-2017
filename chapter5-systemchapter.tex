\begin{comment}
    ./texfix.py --reformat --fpaths chapter5-systemchapter.tex --numlines=1
    ./texfix.py --fpaths chapter5-systemchapter.tex --outline --numlines=1 --section Examples
    ./texfix.py --fpaths chapter5-systemchapter.tex --outline --asmarkdown --numlines=99  --section Examples
    ./texfix.py --fpaths chapter5-systemchapter.tex --outline --asmarkdown --numlines=99 
\end{comment}

\chapter{The image analysis system architecture}\label{chap:system}


\section{Introduction}\label{sec:systemintro}
    In this chapter we describe the \IBEIS{} image analysis system
      architecture.
    The goal of this architecture is to aid in the development, debugging,
      testing, integration, and deployment of vision algorithms.
    This architecture must be compatible with dynamic web technologies.
    This is achieved through a stateless software framework built around
      registering algorithm dependencies.
    We will consider the problems and challenges of developing this
      architecture and discuss design goals.
    We then introduce the \glossterm{\depcache{}} --- a framework that we have
      designed to achieve these goals.
    Our framework is distinguished from existing task dependency
      managers~\cite{bernhardsson_luigi_2016} because it addresses the niche of
      maintaining hierarchical dependencies of dynamic objects.
    In the main body of this chapter we will describe the \depcache{} and
      propose the work needed complete this framework.

    \subsection{Problem statement}
    In the \IBEIS{} image analysis system there are two primary object types
      --- \emph{images} and \emph{\annots{}}.
    These \glossterm{primary objects} define a dynamic state, because primary
      objects will be added, removed and modified.
    The purpose of image analysis is to infer information about these primary
      objects --- such the location of the animals in images and the identities
      of these animals.
    %The system will be used in both an academic and practical setting.
    %The system must function in a dynamic on-line context --- \ie{} when
    %  primary objects are added, removed, or modified.
    %New algorithms will be added 
    %The image analysis system will be in active development while also being
    %  used in production.
    %---  and as new algorithms are incorporated.
    The system must have the capability to %
    run reproducible experiments, %
    test the feasibility of existing algorithms on new species, %
    adapt existing algorithms to new species, %
    and integrate new algorithms.
    %Multiple algorithms will be run on these primary objects at the same time,
    %  therefore access of their derived attributes must be stateless yet these
    %  attributes should not be continuously recomputed.
    For these reasons it is critical that the system is designed with an
      organizational scheme that maintains the provenance --- lineage --- of the
      algorithms, their inputs, and their results.
    Specifically the system design must organize and maintain:
    (1) the dynamic primary data ingested from or set by external sources %
    --- such as image URIs, GPS data, and \annot{} bounding boxes ---
    (2) the derived data computed using computer vision algorithms %
    --- such as SIFT descriptors, \annot{} matches, and trained detection
      models --- and
    (3) the computer vision algorithms themselves.
    %For example, m

    It is challenging to design such an organizational scheme in a dynamic
      context, especially when considering that algorithms are often run with
      different configurations.
    Between the various configurations of computer vision algorithms, the
      system can generate a lot of derived data.
    It becomes necessary to store algorithm results in an intermediate cache.
    However, it becomes error-prone and tedious for a developer to prototype
      an algorithm when that developer must also manually cache and mentally
      maintain derived features and trained models.
    This is especially true when there are dependencies between intermediate
      algorithms or when there is need to tune, compare, or optimize the
      hyperparameter settings of an algorithm.
    When running experiments in this setting it becomes easy to load incorrect
      data and produce invalid results if caches are not properly invalidated.
    All of this is made even more complex considering that the state of the
      primary data is in flux.
    Therefore, we are motivated to develop automated organization methods.
    The following list enumerates the challenges that we propose to address
      with our system design.
    \begin{itemize}
        \item Maintenance of a dynamic database of primary \annot{} and image
          information.

        \item Maintenance of derived data computed by algorithms with multiple
          configurations, including the provenance of this data.

        \item Integration of new algorithms and modification of existing
          algorithms.

        \item Regular deployment of the system into the field and regular
          system demos.

        \item The training, updating, and distribution of learned models.

        \item Execute feasibility studies to test if the system is able to
          identify a new species of animal and determine which hyperparameter
          settings to use.

        \item Ensure reproducibility of academic experiments.
    \end{itemize}

    Throughout this chapter we will refer to two examples.
    In the first example a developer requests the descriptors of a single
      annotation.
    In the second example a developer requests a nearest neighbor indexer for
      a set of database annotations.
    These examples will be used to illustrate these problems.

    \subsection{Design goals}
    We propose to addresses these challenges by designing a declarative
      framework that manages primary object dependencies and abstracts them away
      from the developer.
    Declarative development means that a developer only has to specify how a
      task is completed.
    The details of control flow and execution are implemented and handled by
      the framework.
    This naturally separates the concerns of system design from algorithm
      development.

    The declarative design pattern~\cite{vlissides_design_1995} allows for the
      automation and standardization of system tasks such as caching, testing,
      and hyperparameter optimization.
    This means that such a framework can consistently maintain
      \glossterm{provenance} --- the lineage of a unit of derived information
      --- across all aspects of the system.
    The following list enumerates specific goals set for the design of this
      framework:

    \begin{itemize}

        \item \textbf{Abstracted caching}:
            To address the challenge of maintaining caches the goal is to
              develop a cache standardization that is abstracted away from ---
              but not inaccessible to --- the developer.
            The developer will only need to specify how to compute a derived
              property of an object.
            This involves specifying algorithm input, algorithm output,
              configuration settings, and a function which computes the output
              given the input.
            %In essence this is the information specified whenever a
            %  function is written.
            %The input and configuration are defined by the function
            %  signature and the output is the function return type.
            The framework can decide how cache results based on this
              registered information.
            Data access and maintenance is handled by the framework.

        \item \textbf{Statelessness}:
            To ensure that our system can be called by concurrent web-based
              frameworks it must be stateless.
            In other words requests should be independent of each other and
              the same inputs should always produce the same outputs.
            However, the larger IBEIS system is not stateless --- the images
              and annotations that exist in the system define a dynamic state.
            The control of this dynamic state is external to the image
              analysis module.
            Despite the fact that annotations and images are stateful, the
              analysis of these these primary objects can still be stateless.

            To see this, consider that combinations of dynamic primary objects
              (with configuration parameters) define the possible inputs to any
              image analysis algorithm.
            If one of these primary objects changes it can simply be
              interpreted as a new input state.
            In this way the universe of annotations and images can be viewed
              as static, and our framework can be stateless by simply operating
              on a subset of it.
            To accomplish this all that is needed is an id that can uniquely
              specify any possible state that a primary object can be in.
            In theory this can be done using a tuple of properties, such as
              pixel information or bounding boxes.
            In practice this can be achieved using a hash.

            %Consider the example of computing the descriptors of an annotation
            %  in the large IBEIS system.
            %If the user edits the annotation bounding box, then that state has
            %  changed.
            %The next time the user asks for the descriptors of that annotation
            %  they will expect a different result.
            %At first, this seems like it breaks the rules of statelessness.
            %However, if that slightly changed annotation is viewed as a
            %  completely different annotation, then the input to the request is
            %  different.

        \item \textbf{New algorithm integration}:
            It should be straightforward to develop new or integrate existing
              algorithms into the system.
            New algorithms should be able to make use of or extend previous
              algorithms.
            Existing algorithms should be able to be integrated without major
              re-engineering.
              %immediately.
            %even if existing functionality is duplicated
            %internally.
            %Existing algorithms should be able to be integrated even if
            %  existing functionality is duplicated internally.
            Algorithm integration should be as simple as registering a
              function and should require minimal boilerplate overhead.

        \item \textbf{Automated testing and optimization}:
            A declarative style of algorithm registration supports integrated
              testing and hyperparameter optimization.
            By declaring inputs and outputs of an algorithm it is possible for
              an abstract framework to automatically perform basic unit testing
              of an algorithm.
            Furthermore, by declaring possible configuration values and an
              objective function the framework can search for optimal
              hyperparameter settings.

        \item \textbf{Reproducibility}:
            All experiments and trained models should be reproducible.
            If the original input is known, it should always be possible to
              retrain a model or replicate the results of experiment.
            Whenever feasible, a manifest of original input to an algorithm
              should be maintained.
            In some cases including this manifest may not be feasible or
              necessary.
            For example, when a trained parametric model is deployed on a new
              server it may be cumbersome to include a reference to the data
              that model was trained with.
            However, in all cases at least a hashed fingerprint of the
              original input should be maintained.
            This ensures that it is possible to decide/verify if some input is
              the same as the original input to a model or experiment.
            %(For this reason we strongly recommend that any randomized
            %  algorithms are initialized with a known seed).

        \item \textbf{Dynamic model augmentation}:
            For some trained models --- such as non-parametric nearest
              neighbor indexers --- it should be possible to augment these
              models without recomputing the model from scratch if similar
              models exist in the cache.
            The developer should be able to declare an additional function
              that modifies the model given some new input.
            By default it should not be assumed that the modification of these
              models would produce the same output as if all data was given at
              the same time.
            Therefore, in this case the order of augmentation (the order in
              which the input was received) should be recorded.

            Dynamic model augmentation allows a stateless framework to
              simulate a stateful workflow.
            Consider using the image analysis API that is adding new
              \exemplars{} to the \masterdatabase{}.
            Before a new query can be executed, the nearest neighbor indexer
              should be updated.
            By dynamically augmenting the previous indexer with the newly
              added \exemplars{}, this simulates a stateful workflow where there
              is a single master nearest neighbor indexer that can be updated
              when new \exemplars{} are added.
            %By doing this in a stateless framework we
            %This is especially important for non-parametric models --- like
            %  nearest neighbor indexers --- that must be updated 
    \end{itemize}

    \subsection{Overview and related work}
    Given these challenges and design goals we propose the
      \glossterm{\depcache{}} --- a framework that represents a set of
      algorithms, inputs, outputs, configurations, and their dependencies as a
      directed acyclic multi-graph.
    It is common to use directed acyclic graphs to implement pipelines as
      demonstrated by various task dependency managers such as GNU
      Make~\cite{stallman_gnu_2004}, Dryad~\cite{isard_dryad_2007}, and
      Luigi~\cite{bernhardsson_luigi_2016}.
      %and Apache
      %Hadoop~\cite{shvachko_hadoop_2010}.
    Ours differs from the aforementioned managers because instead of
      specifying general task dependencies we address a more specific niche
      problem.
    Our \depcache{} specifies hierarchical property dependencies of our
      system's dynamic primary objects (images and \annots{}).
    This allows for dynamic and stateless analysis of an otherwise stateful
      set of primary objects.
    %Given this dependency structure we can statelessly compute properties of
    %  primary objects with a dynamic state.

    Specifying the dependant properties of a primary object is well suited for
      computer vision applications where there are often many choices of
      algorithms and features that are repeatedly applied to many instances of
      the same primary object (\eg{} an image).
    %Therefore we view our framework as addressing a niche.
    In this niche our framework is a more natural way to specify dependencies
      and requires less boilerplate code than other frameworks.
    Furthermore, because declarative function registration abstracts pipeline
      execution away from the developer, our system can be extended to use more
      mature systems --- such as Luigi~\cite{bernhardsson_luigi_2016} --- in the
      backend without any change to the minimal registration required to build a
      computer vision pipeline.

    We envision the completed \depcache{} as an extensible plug-in layer that
      integrates multiple independent image analysis algorithms into a unified
      framework and optimizes the hyper-parameters of those algorithms.
    An interactive experimentation harness allows for the reproducible
      comparisons of algorithms and presentation of results.
    Doctests are embedded in key places in the code to provide examples and
      intuitive entry points for algorithm testing and debugging.
    A scalable caching and storage layer allows for the system to be run on a
      variety of machines ranging from laptops to cloud clusters.
    A publishing layer allows for the controlled distribution of trained
      models to external instances of the system.
    This analysis system can be applied on top of any set of dynamic primary
      objects, and used to statelessly respond to requests concerning these
      primary objects.

    The remainder of this chapter is outlined as follow.
    First we describe the current system architecture
      in~\cref{sec:systemcurrent}.
    Then in~\cref{sec:systempropose} we describe functionality that we propose
      to implement to improve and complete the system.
    Finally we summarize and conclude the chapter
      in~\cref{sec:systemconclusion}.
      

\section{The current system}\label{sec:systemcurrent}

    %Much of the groundwork for this system architecture has been
    %  completed.
    %We first we describe system components that that currently exist.
    %The groundwork for all of these features has been laid, and many of these
    %features have already been developed.

    The current system architecture addresses the challenge of maintaining
      data derived from primary objects using the \depcache{}.
    This serves as an abstract framework for computing, storing, and accessing
      the results of algorithms.
    The underlying data structure is a directed acyclic multi-graph.
    The graph represents a set of algorithms and configurations as nodes and
      the algorithm inputs and outputs as edges.
    The use of multi-edges allows models that depend on multiple inputs to be
      specified in the same framework.
    Thus the structure of the graph can be used to infer algorithm
      dependencies and the minimal set of inputs required to access or compute a
      uniquely derived property.

    %feature extraction algorithms,
    %comparison algorithms, and learned models.
    %The definition of edges allows the \depcache{} to 
    %The algorithms are used to define the nodes and the algorithm's
    %  inputs, configurations, and outputs are used to define the edges of
    %  this graph.
    %Depending on the type of edges
    %By defining the inputs, configurations, and outputs of an algorithm
    %  the \depcache{} is able to construct this graph and
    %  deterministically derive complex hierarchies of object properties.
    %This cache is an framework for handling data that can be
    %  deterministically derived given its input and configuration
    %  parameters.
    %We use 

    \subsection{Dependency graphs}
    Each \depcache{} is responsible for the properties of a single primary
      object type.
    Thus, for \IBEIS{} image analysis there are two primary objects:
    images and \annots{}.
    The current version of the two \depcaches{} are illustrated
      in~\cref{fig:doubledepc}.
    Note that the two graphs are connected via a decision node that indicates
      an algorithm or user has decided to create an \annot{} from a candidate
      detection.
    This is shown only to demonstrate that the output of one \depcache{} can
      be used to create a primary objects in another \depcache{}.
    The two graphs are not explicitly connected in the system because user
      decisions are considered external sources of information.

    \doubledepc{}

    The construction of a dependency graph is outlined as follows.
    Nodes on this multi-graph represent an algorithm that computes a derived
      property of the primary object.
    A node's incoming edges represent the required inputs to the algorithm,
      and the outgoing edges represent the output of the algorithm.
    A node may use any of its ancestor nodes in its computation.
    An edge can either be one-to-one or many-to-one, and specifying multiple
      edges between two nodes \emph{is} permitted.
    A single one-to-one edge represents a derived property belonging to a
      single instance of the parent node.
    A single one-to-many edge represents a model where multiple instances of
      the parent object are used to construct the output of a node.
    A set of multi-edges represents an $n$-wise comparison between parent
      objects where $n$ is the number of edges in the set.
    Different combinations and hierarchies of these edge relationships can be
      used to construct all dependencies needed by the image analysis system.

    To ground these concepts consider the following examples from the graph
      in~\cref{fig:doubledepc}.
    An example of a one-to-one edge is the edge between  $\tt{chip}$ and
      $\tt{feat}$.
    This indicates that a single set of keypoints and SIFT descriptors is
      computed from a single chip.
    An example of a one-to-many edge is the edge between $\tt{featweight}$ and
      $\tt{neighbor\_indexer}$.
    The one-to-many edge is marked with a ``*''.
    This indicates that multiple weighted features from a set of \annots{} are
      used to construct a single nearest neighbor indexer.
    An example of a multi-edge input is shown between $\tt{featweight}$ and
      $\tt{vsone}$.
    The second edge in the pair is marked with a ``2''.
    This indicates that a pair of weighted features is compared to produce a
      pairwise one-vs-one similarity score.


    % Algorithm integration
    \subsection{Algorithm integration}
    The \depcache{} is designed such that independently developed algorithms
      can be registered at various levels of granularity.
    In other words, either a single function that executes an entire algorithm
      pipeline can be registered or appropriate stages of that algorithm's
      internal pipeline can be registered.
    This allows for immediate integration of existing algorithms.
    After the immediate integration, efficient integration of that algorithm's
      pipeline can take place at a more gradual pace.

    An example of this granular algorithm integration can be seen
      in~\cref{fig:doubledepc}.
    The one-vs-one algorithm is integrated with no caching granularity because
      the algorithm is registered as a single node.
    On the other hand, the one-vs-many algorithm has three levels of
      granularity:
    nearest neighbor search, spatial verification, and final one-vs-many
      scoring.

    In our current system, we have integrated external algorithms from two
      researchers.
    All of these works are reflected in~\cref{fig:doubledepc}.
    The first is Zach Jablon's algorithm that matches the trailing edges of
      humpback whale flukes~\cite{jablons_identifying_2016}.
    This algorithm consists of nodes the between $\tt{notch\_tips}$ and
      $\tt{dtw\_distance}$.
    The second is Jason Parham's work on detection of animals and quality and
      viewpoint classification~\cite{parham_photographic_2015}.
    These include all algorithms in the image \depcache{} as well as the
      $\tt{annot\_labeler}$.

    \egregister{}

    To demonstrate how an algorithm is integrated into the system using,
      consider the code snippet in~\cref{fig:egregister}.
    In this example the developer writes a function that specifies how to
      compute the keypoints and descriptors of a preprocessed chip.
    This function is registered  with the \depcache{} using a decorator.
    The inputs, outputs, and configurations are all declared in the decorator.
    In this example the docstring shows how the property can be accessed using
      the $\tt{get(\ldots)}$ method of the \depcache{}.
    The details of access, computation, and storage are left up to the
      \depcache{}
    %This is the minimal information needed to incorporate this function into
    %  the dependency graph.

    \renewcommand{\tablename}{table name}
    \newcommand{\colname}{column name}

    \subsection{Access, computation, and storage}
    Considering the previous example, we provide details about how the
      \depcache{} uses this declared information to construct the dependency
      graph and the storage containers.
    Each function registered with the \depcache{} is declared with a
      string-id, a set of input dependencies, and a set of typed outputs.
    This is used to define a node which we represent using an SQLite table.
    The string-id defines the \tablename{} and a column is defined for each
      output property, each parent input, and the configuration parameters.
    The parent input columns store integer row-ids from other tables.
    This means that this node implicitly inherits all the relevant
      configuration parameters of its ancestors.
    The configuration column stores a hash of configuration parameters
      directly relevant to this property.
    The tuple of parent input row-ids and a configuration uniquely specifies a
      row in a table.
    Data can be accessed either using this ``super-key'' or using the native
      row ids.
    This table will store the properties computed by this function.

    The registered function must have a specific structure.
    The function arguments must be a \depcache{} object, a list for each
      dependency, and a configuration dictionary.
    The dependency inputs are corresponding lists of input row-ids from other
      tables.
    This allows the function to natively access any parent property.
    The function must yield a tuple of outputs --- corresponding to the
      registered outputs --- for each item in the input lists.

    Even though the function is written with respect to its parents, the
      properties of the \depcache{} can be accessed --- by the
      $\tt{get(\ldots)}$ method --- using row-ids of primary objects in the
      dependency graph.
    This means that code written that accesses a property does not need to be
      modified if the dependencies are slightly changed.
    Using the $\tt{get(\ldots)}$ method simply issues a request for the
      property a dynamic primary object.
    The \depcache{} performs a stateless computation on the primary object
      using the dependency graph.
    %a stateless analysis of the primary objects using the .
    %that always reflects the most
    %  up-to-date information in terms of the primary objects and the algorithm
    %  dependencies.
    %Existing code will

    %This means that the property can be used without knowledge of its
    %  dependencies other than its configuration.

    %developer can know little of the actual dependencies
    %  of an attribute but still access that attribute.

      %row-ids of source nodes in the dependency graph.
    %In most cases the source node is a primary object.
    %The only exception is when a node depends on a trained model.
    %In this case the trained model can either be specified using multiple
    %  primary row-ids or the node can be treated as a ``virtual-source'' and
    %  specified via UUID{}.
    %A virtual-source simply refers to any node that can be specified directly
    %  by short-circuiting its ancestor dependencies.
    %All nodes with a single incoming one-vs-many edge (nodes that correspond
    %  to trained models) can be accessed in this special way.
    %In this example we only detail the case where primary object row-ids are
    %  used.

    \subsubsection{Stateless responses}

        We now discuss how the \depcache{} internally statelessly responds to
          requests.
        %We will first describe these processes in general.
        To ground the discussion, we provide two high level examples.
        In the first example the developer requests the descriptors of an
          \annot{}.
        This is is the simpler of the two cases because accessing this
          properties only involves one-to-one dependencies.
        In the second example the developer requests a nearest neighbor
          indexer.
        This is the more complex case because it involves many-to-one
          dependencies and multiple paths between the primary object and the
          requested property in the dependency graph.

        %\subsection{Graph construction and access}
        %We first describe how the dependency graph and cache storage
        %  containers are constructed.
        %Then we detail how the \depcache{} computes and stores or reloads
        %  information given a request.

        Consider a scenario where a developer establishes one-vs-many feature
          correspondences --- the procedure described in~\cref{sub:featmatch}.
        The developer wishes to use affine invariant features and $8$ kd-trees
          for the approximate nearest neighbor search.
        In the following snippet, the variable $\tt{qaid}$ is a single of
          query \annot{} id, $\tt{daid\_list}$ is a list of database \annot{}
          ids, and $\tt{depc}$ is the \depcache{} object.
        % ---
        % chktex-file 32
        % chktex-file 36
            \begin{comment}
        python << endpython
        import ibeis
        ibs = ibeis.opendb('testdb1')
        depc = ibs.depc
        qaid = 1
        daid_list = [2, 3, 4]
        #<NNIndexer at 0x7fb7efbb0340>
        #[array([[..., 
        #        [ 2,  1,  4, ...,  3,  1,  0]], dtype=uint8)]
        endpython
        \end{comment}
        \begin{pythoncode*}{gobble=12}
            # Case 1: Loading the descriptors from query annotations
            qvecs = depc.get('feat', qaid, 'vecs', config={'AI': True})
            # Case 2: Loading a neighbor indexer of a database of annotation
            nnindexer = depc.get('neighbor_index', daid_list, 'indexer', config={'trees': 8, 'AI': True})
            # Usage of the loaded objects to find one-vs-many correspondences
            neighbor_idxs, neighbor_dists = nnindexer.knn(qvecs, K=4)
        \end{pythoncode*}
        % ---
        The first line requests the affine-invariant descriptors of a query
          \annot{}.
        This particular \annot{} returns a $2205 \times 128$ numpy array
          representing the SIFT descriptors of the query annotation.
        Next, all of the affine invariant features from the database
          annotations are indexed by a forest of $8$ kd-trees.
        The $\tt{nnindexer}$ is a custom object returned by the registered
          $\tt{neighbor\_index}$ function.
        Once these objects are loaded the developer accomplishes the goal of
          establishing one-vs-many feature correspondences.

        In this example the developer does not need to worry about
          re-computing these objects or loading an incorrect cached version.
        If the underlying annotations have changed, then the \depcache{} has
          already implicitly invalidated old indexers and new indexers are
          constructed automatically.
        If we assume that the neighbor indexer was registered with an
          augmentation function then this snippet can be efficiently run in a
          loop even if the database \annots{} slightly change.
        The developer can easily change any parameters in this code snippet
          without having to do any code refactoring or external computations.
        Thus, the developer can focus on writing modifying the algorithm
          without worrying about system issues.
        The remainder of this section outlines how the \depcache{} offloads
          this work from the developer.

        Both of the previous examples access properties using the
          $\tt{get(\ldots)}$ method.
        This method is called with four arguments:
        (1) the \tablename{} of the requested property,
        (2) the primary object row-ids,
        (3) the \colname{} of the specific attribute to access, and
        (4) a dictionary specifying the non-default configuration parameters
          of the requested property and all ancestor properties.
        The primary-ids are similar to fancy indexing in
          numpy~\cite{van_der_walt_numpy_2011}, \eg{} specifying a single
          primary-id returns a single result, but specifying a list of
          primary-ids returns multiple results.

        Stateless analysis is achieved by mapping specified primary object to
          unique primary object-ids that specify the dynamic state of the
          object.
        In the case of images, this is either a hash of the file bytes or a
          randomly generated UUID{}.
        In the case of annotations this is a hash of the parent image and the
          bounding box information.
        At this point all other derived properties of a primary object simply
          depend on this primary-id and the configurations of its ancestors in
          the dependency graph.

        When the get method is called the \depcache{} enumerates all paths in
          the dependency graph from the primary object to the target property.
        It walks these paths in topological order to map the given primary-ids
          to the native-ids of the requested table.
        On its walk, the \depcache{} ensures that all properties along each
          path are computed using each nodes' registered functions.
        Therefore, at the end of the walk the requested property is guaranteed
          to exist and the \depcache{} has computed a mapping from the
          primary-ids to the native-ids of the requested property.
        Thus the \depcache{} can simply look up the requested property and
          return it.

        Consider this process for the first line in the example.
        We are given the ids for \annot{}.
        Note that the primary-ids for the \annots{} are a hash of the
          information that will be used by subsequent properties.
        In this case it is a hash of its parent image's hash and the
          annotation bounding box.
        %\devcomment{this still needs to be generalized}
        We wish to access the descriptors of an \annot{}.
        In the \depcache{} the descriptors are a column in the $\tt{feat}$
          table.
        Therefore we must walk the path between the primary object and the
          requested node:
        $\tt{annot}$ \rarrow{} $\tt{chip}$ \rarrow{} $\tt{feat}$.
        We start the walk at the $\tt{annot}$ node.
        We can move forward when we know the mapping from the given
          primary-ids to the native-ids of the current node.
        The mapping for the first node is simply the identity.
        Therefore we look ahead to the next node in the path --- the
          $\tt{chip}$ node.
        We take a subset of the parameters in the given configuration
          dictionary that belong to the $\tt{chip}$ table.
        Parameters that were not specified are given default values.
        At this point, we have all inputs needed --- \ie{} the parent row-ids
          and a configuration --- to either access or compute the properties at
          the $\tt{chip}$ node.
        %Note that the configuration dictionary in the example could have
        %  specified the size or normalization of the extracted chip.
        The parent row-ids and the hash of the configuration parameters
          uniquely specifies a row in the $\tt{chip}$ table.
        If this row does not exist, then we also have the information needed
          to call the registered function.
        Therefore we create the row if it doesn't exist and access the native
          row-ids of the $\tt{chip}$ table corresponding to the parent-ids and
          the configuration.
        We now know the mapping from the primary-ids to the $\tt{chip}$-ids,
          so we move forward.
        The same procedure is applied to the $\tt{feat}$ node, except now
          $\tt{chip}$ is the parent instead of $\tt{annot}$.
        After this procedure completes the walk has reached the end of the
          path, and we have computed the native $\tt{feat}$-ids that correspond
          to the primary \annot{}-ids and the given configuration.
        We can now simply index into the feature table using the
          $\tt{feat}$-ids and return the data at the requested $\tt{vecs}$
          column.

        The second line of the example requests a nearest neighbor indexer for
          a set of database \annots{}.
        The steps to access this data are more complex for several reasons,
          the two most immediate being:
        (1) there is more than one path from the primary object to the
          requested property, and
        (2) one of the properties in the path has a one-to-many relationship.
        %\emph{Note that the currently shown dependency graph is slightly
        %  bugged, this section will describe what should be shown}.
        The first path is:
        $\tt{annot}$ \rarrow{} $\tt{chip}$ \rarrow{} $\tt{feat}$ \rarrow{}
          $\tt{featweight}$ \rmultiarrow{} $\tt{nnindexer}$.
        The second path is:
        $\tt{annot}$ \rarrow{} $\tt{probchip}$ \rarrow{} $\tt{featweight}$
          \rmultiarrow{} $\tt{nnindexer}$.
        Both of these paths contain a one-to-many relationship denoted by,
          \rmultiarrow{}, the starred arrow.
        To handle the case where there is more than a single path, all paths
          between the source and the requested property are enumerated.
        The topological ordering of the nodes define what order properties are
          computed in.
        This ensures that all properties are computed before any node that
          takes two inputs (\eg{} the feature weights take inputs from
          $\tt{probchip}$ and $\tt{feat}$) is requested.
        When a path encounters a one-to-many relationship the ``dimensionality
          of the input'' increases.
        If a path contains a single one-to-many relationship, then instead of
          passing in a single annotation, the developer must pass in a set of
          annotations.
        Furthermore, if a path has two one-to-many relationships a single
          property would be specified by a list of lists of primary row-ids.
        This nesting increases for each one-to-many relationship in a path.
        However, this multiple one-to-many relationship case does not
          currently appear in any of our dependency graphs.

        A third reason why the second example is more complex is because the
          neighbor indexer is (ideally) registered with an augmentation
          function.
        This gives the \depcache{} a choice if a requested model does not
          exist:
        either compute a new indexer or augment an existing indexer.
        Recall that cached models are saved with a manifest of the information
          used to construct them.
        The \depcache{} first loads these manifests and searches for the most
          similar set to the given inputs.
        Because kd-trees can support addition and removal, the similarity
          between the requested input, $\set{A}$, and a manifest, $\set{B}$, is
          the number of inputs in common minus the number of changes that must
          be made --- \ie{} the size of the intersection minus the size of the
          symmetric difference:
        $\card{\set{A} \isect \set{B}} - \card{\set{A} \symdiff \set{B}}$.
        If the similarity of the most similar saved model is under a threshold
          or it does not exist then the augmentation function is ignored and the
          model is recomputed.
        Otherwise the most similar model is loaded and passed to the
          augmentation function where the augmented model is computed.
        %Closest is defined \wrt{} to the model.
        %Some models support addition and removal of inputs, while others may
        %  only support addition of inputs.

        %\paragraph{Not covered in these examples}
        %These examples highlight some of the core functionality of the
        %  \depcache{}.
        %However, there are some features that were not discussed that we
        %  briefly summarize but do not expand on in this document.
        %The case where a table has a one-to-one and a one-to-many input edge.
        %This case induces multiple paths starting at the sources of the
        %  dependency graph.
        %The case where a UUID is specified to access a model instead of a set
        %  of primary-ids.
        %This case induces a virtual-source on the graph to avoid specifying
        %  all inputs to access parametric models.

        %There is one more complexity not specified in these examples This is
        %  the case where a table has more than one input that has a one-to-one
        %  input and a non one-to-one input.
        %Consider a node that computes the feature correspondences of a query
        %  annotation in a set of database annotations.
        %The input to this node would be the weighted features of a query
        %  annotation and the \emph{set} of weighted database features.
        %Just one set of annotations is not sufficient to specify this
        %  property.
        %This case can be handled because this is the same as having multiple
        %  paths except that there are more than one inputs at the source of the
        %  graph.

        %Note that this case applies to comparing any basic property to a
        %  model.
        %Two sets of inputs must be applied.
        %The set of database annotations is essentially the training set for a
        %  model.
        %Because a nearest neighbor index is non-parametric it makes sense to
        %  specify the training set as input.
        %In some cases (as with non-parametric models like neural network
        %  detectors) it may not be feasible to specify the training set as
        %  input.
        %This is the case where a UUID can be specified instead of the training
        %  set.
        %While these examples touch on basic
        %The discussion of the internal workings While these examples do not
        %  completely demonstrate

        %The examples presented in this section highlight some of the core
        %  functionality of the \depcache{}.
        %They show some of the flexibility provided by the system architecture.
        %The discussion demonstrates how much work the \depcache{} can offload
        %  from the developer, which serves to illustrates the value of such a
        %  declarative framework.
    \subsection{The test harness}
    A secondary component of the system is the test harness, which uses the
      \depcache{} to compare and present algorithm configurations.
    Currently, the test harness is coupled with the individual identification
      algorithms, and it is not part of the \depcache{}.
    However, not much work would be needed to decouple and integrate the test
      harness with the \depcache{}.
    %For the purposes of this document we treat the test harness as if
    %  it was integrated with the \depcache{}.
    The test harness uses the configurations defined between the primary
      object and a target algorithm to compare multiple configurations of that
      algorithm.
    This allows a developer to easily perform manual search over algorithm
      hyperparameters.
    Interactive visualization, tuning, and presentation of algorithm results
      is supported using IPython notebooks~\cite{perez_ipython_2007}.
    All of the results reported in~\cref{sec:experiments} were generated in a
      reproducible manner using the test harness.

    \subsection{Current limitations}
    While the current system architecture satisfies many of the goals
      (abstracted caching, statelessness, testing, reproducibility, integration,
      optimization) enumerated in~\cref{sec:systemintro}, there are still
      components that are missing and improvements to be made (%
    better multi-edge support, %
    automatic hyperparameter optimization, %
    integration of primary objects into the \depcache{}, %
    model augmentation, %
    API improvements, \etc{}).
    The next section discusses the most important of these issues and
      introduces our proposal to complete the system architecture.

\section{Proposed work}\label{sec:systempropose}
    In this section we describe the steps towards the completion of the
      proposed architectural design.
    However, there are many more undiscussed issues that must be addressed to
      perfect the design of the \depcache{}.
    %We leave this 
    The work that we propose here is only the necessary work needed to ensure
      a working system for addressing animal identification in a dynamic
      context.
    Namely, the system must have the ability to dynamically update search data
      structures in a stateless context.
    The steps necessary to achieve this are outlined in the following list.
    %These proposed steps involve the implementation of new features and
    %  the reworking of existing features.

    %As a system in development, many of the desired features are
    %  half-implemented or not written in a robust manner.
    %To complete the dependency cache the following should be
    %  implemented.
    %For instance, the single image identification algorithm is 
    %As such it is not easy to fully quantify what work still needs to be done 
    %without

    \begin{itemize}

        \item \textbf{One-to-many edge support}:
        Currently the support for one-to-many edges in the \depcache{} is
          limited.
        This restricts the types of models that can be trained and used within
          the system.
        Additional work is needed to rework this feature, standardize how
          multi-input properties are accessed from the \depcache{} API{}, and
          standardize how models are specified as algorithm dependencies.
            
        %    An extension to fully support one-to-many and pairwise
        %  dependency inputs.
        %Current functionality of this feature is hacked in and not
        %  extensible.
        %Additional work is also needed to 

        \item \textbf{Model augmentation}:
        Along with the reworking of the one-to-many edges, the system should
          also allow for model augmentation.
        For some models --- like the nearest neighbor indexer --- model
          augmentation is necessary to avoid performance penalties from constant
          recomputation.
        The developer should be able to declare a function that describes how
          data is added to or removed from models.
        To maintain provenance the model id should reflect the original input
          as well as the sequence of additions and deletions (unless adding data
          to a model is the same as if the model was trained jointly with the
          initial and new data).
        Model augmentation is how we propose to address the issue of
          dynamically updating our search data structures.

        %This feature can be implemented by 
        %Some models like nearest neighbor indexers should allow for
        %  indexer augmentation.
        %This requires that a function is specified that determines how
        %  data should be added or removed from a model and if that
        %  changes the model id.
        %\Ie{} is a model built with $train([1, 2, 3])$ the same as
        %  $train([1, 2]).add([3])$?
        %The default answer to this question is no.

        %\item \textbf{Model publication}:
        %Training a model is typically an offline procedure.
        %If a new instance of image analysis is deployed on a remote
        %  machine it should not be forced to train models before it can
        %  compute properties derived from them.
        %The new instance will likely not have access to the training
        %  data needed to run the model.
        %Once a developer has trained a model it should be published to
        %  a public server.
        %When a remote instance of image analysis needs to use a
        %  specific model and does not have it, it should first check the
        %  remote server and download the pre-trained model.

        %\item One-vs-one algorithms should be able to specify if the
        %  comparison is symmetric and have the \depcache{} take advantage
        %  of that.

        %\item \textbf{Hyper parameter optimization}:
        %Currently the only method of hyper-parameter optimization is
        %  interactive manual and grid search.
        %It should be possible to implement a random or Bayesian method
        %  by allowing the developer to declare a ``loss function''.
        %The system could minimize this loss function by iterating
        %  through feasible configurations specified in the configuration
        %  class registered with the algorithm.

        %\item Algorithms should specify which native properties of the
        %    source object they depend on in order to more efficiently
        %      handle cache invalidation.

        %\item Invalidated cache items should be cleaned up.

        %\item \textbf{Primary object storage}:
        %    The current \depcache{} is only able store derived properties of
        %      objects.
        %    However, it must be able to access the dynamically changing
        %      information of primary objects --- \eg{} the bounding box of an
        %      \annot{} --- in order to compute a unique id that identifies the
        %      relevant parts of a primary object's state.
        %    The current \depcache{} handles this using hard-coded external
        %      functions that compute these necessary hashes.
        %    These functions are registered with the \depcache{} which allows
        %      it to perform its job.
        %    For the system to be self-contained it must contain methods to
        %      add, remove, and update primary objects.

    \end{itemize}

    \section{Summary of the system architecture}\label{sec:systemconclusion}

        This chapter has introduced a system architecture designed to address
          the challenges of individual animal identification in a dynamic
          context.
        This is accomplished using an abstract framework called the
          \depcache{}.
        The \depcache{} is a declarative framework for algorithm caching,
          testing, and hyperparameter optimization.
        The declarative design lends itself to maintaining data provenance and
          allows for the standardization of API calls and caching structure.
        The system maps objects with dynamic state (\eg{} images and
          \annots{}) to unique ids and performs stateless analysis of these
          objects.
        We have described features that currently exist in the system as well
          as proposed features that should be developed.
          %to ensure the system can
          %support the demands of individual animal identification in a dynamic
        %context.

        We wish to note some additional challenges to designing an
          organization framework that were not discussed in the introduction of
          this chapter.
        One of these is maintaining the state history of primary objects.
        We did not design our framework to handle the case where the history
          of primary objects needs to be maintained (\ie{} the user is allowed
          to change EXIF information and undo those changes).
        The reason this challenge was not mentioned is because it  lies
          outside the scope of image analysis and is addressed by the Wildbook
          module of \IBEIS{}~\cite{j_bonner_markrecapture_2013}.
        Another challenge not explicitly mentioned is that of distributed
          computing.
        There exist other mature data management systems that likely do a
          better job of implementing the data management backend.
        Our contribution is the declarative frontend framework for computer
          vision applications.
        This framework is built so the backend can be replaced with new or
          more mature technology (such as Luigi) that can utilize distributed
          data management systems like Apache
          Hadoop~\cite{shvachko_hadoop_2010}.
        %as it comes out
        %/ we become aware of it.

        The current system is able to accommodate most cases for testing and
          algorithm development required to complete this thesis.
        To complete the system we will add support for multi-edges and model
          augmentation.
          %, model publication, hyper parameter optimization.
        Future work should improve the backend of the system to support
          distributed computing.

        %The design of this system assumes that all computations are
        %  local.
        %The main reason for this is because the system is targeted at
        %  algorithm developers.
        %These developers typically have access to a local dataset and
        %  continually test and tune algorithms.
        %Distributed computing is necessary when the algorithms are
        %  being applied in a service with many users or for very large
        %  scale tests.
        %Future work is needed to extend this system to a distributed
        %  setting.
        %It is our goal to ensure this system has a low barrier to
        %  entry, thus distributed computing should be an option that is
        %  configurable but not necessary to get any part of the basic
        %  system running.
