\chapter{Graph identification}\label{chap:graphid}
\newcommand{\nT}{N}


We frame the problem in terms of constructing what we refer to as an
  \emph{identity graph}.
Each vertex of the graph corresponds to a cropped region surrounding a single
  animal in a single photo.
This cropped region, referred to as an \emph{annotation}, is extracted from an
  image using a detection
  algorithm~\cite{parham_detecting_2016,redmon_you_2016}.
Edges are formed when two annotations are determined to show the same animal.
Each connected component of a correctly constructed graph will be all the
  annotations from an individual animal.
Therefore, stated abstractly, the focus of this paper is determine a correct, consistent set of
  edges in the identity graph.
We address these problems by prioritizing edges for review based on both their
  pairwise probability scores and their ability to affect the consistency of the
  graph, by developing graph measures of redundancy, consistency, and
  completeness, and by developing deterministic and probabilistic convergence
  criteria based on the measures.
These allow us to construct a more accurate identity graph while reducing the
  amount of human effort.
  



The review procedure is formalized around the notion of a %
\glossterm{decision graph} $G = (V, E)$ whose nodes are annotations and whose
  edges are suggested by the LNBNN ranking algorithm and decided upon by a
  combination of the probabilities output by the pairwise algorithm and by
  manual reviewing.
It is important to note that alternative ranking and pairwise probability
  algorithms could be used in place of the two algorithms proposed here.


The edge set $E = E_p \cup E_n \cup E_i$ is composed of three disjoint sets.
Each edge in $E_p$ is \emph{positive}, meaning that it connects two
  annotations determined to be from the same individual.
Each edge in $E_n$ is \emph{negative}, meaning that it connects annotations
  determined to be from different individuals.
Finally, each edge in $E_i$ is \emph{incomparable}, meaning that it connects
  two annotations where it has been determined that there is not enough
  information to tell to tell if they are from the same individual (\eg{} when
  one annotation shows the left side of an animal and another other shows the
  right side).
The goal of graph identification is to construct these edges.
  

Constructing this decision graph naturally addresses the problem of
  identifying individual animals.
Each connected component of the subgraph $G_p = (V, E_p)$ corresponds to a
  unique individual.
Producing an accurate set of these \glossterm{positive connected components}
  (PCCs) is the ultimate goal of the algorithm.
To ensure that PCCs are correct we make use of the negative edges, which
  provide direct evidence that two annotations are different individual.
An important case is when a negative edge is contained within a PCC.
This is an inconsistency and implies that a mistake has been made.
All such inconsistencies must be resolved.
Finally, we add a measure of redundancy to both the positive and negative
  subgraphs to allow the algorithm to detect and recover from mistakes, either
  in the automatic or manual decision making.
Incomparable edges play a minor but important role by preventing uninformed
  decisions in scenarios where positive or negative decisions cannot be made.


\begin{algorithm}
    \begin{enumerate}[noitemsep,nolistsep]
    \item Generate and prioritize candidate edges 
    \item Insert candidate edges into a priority queue 
    \item Repeat until the priority queue is empty
    \begin{enumerate}[noitemsep,nolistsep]
        \item Pop an edge from the priority queue
        \item Make a decision and add the edge to the graph
        \item If the edge causes an inconsistency drop into inconsistency recovery mode
        \item Update the priority queue based on the new edge
        \item If candidate edges require refresh, goto step 1
    \end{enumerate}
    % What happens when the graph is not complete, but the top candidate
    % generation yields no new edges, but the graph is not redundant/complete?
    \end{enumerate}
\caption[Algorithm Overview]{Overview of graph identification}
\label{alg:AlgoOverview}
\end{algorithm}


% Algorithm overview
The process is outlined in Algorithm~\ref{alg:AlgoOverview}.
Akin to a segmentation algorithm~\cite{fulkerson_class_2009} that starts with
  an over-segmentation of an image, the identification graph starts with an
  empty set of edges, %
$G = (V, \{ \})$, so in essence each annotation starts by itself as an
  individual animal.
Throughout the algorithm, the graph is maintained in a \emph{consistent}
  state, which means that each positive connected components has no internal
  negative edges.
 
Candidate edges are generated by the LNBNN algorithm, and probability measures
  (positive, negative, or incomparable) are predicted for each edge.
Each edge is then entered into a priority queue (\Cref{sec:cand}).
Next, the algorithm enters a loop where the next candidate edge is selected, a
  decision is made about this edges --- either automatically (as much as
  possible) or by the user --- and it is added to the graph
  (\Cref{sec:decision}).
The algorithm proceeds toward convergence by removing candidate edges from the
  priority queue, either directly from the top of the queue or indirectly by
  eliminating candidate edges that are no longer needed (\cref{sec:redun}).
A candidate edge is no longer needed when there are sufficient redundancies in
  the edge set within or between its PCCs.
A PCC is \emph{complete} when there is a negative edge between it an all other
  PCCs.

Each new edge addition could trigger two important events:
\begin{enumerate}[label={(\arabic*)}]

    \item a \emph{merge} --- addition of a positive edge between different
      PCCs combines them into one PCC, and

    \item an \emph{inconsistency} --- addition of either a negative edge
      within a PCC or a positive edge between PCCs that already have a negative
      edge between them creates an inconsistent PCC.

\end{enumerate}
Handling a merge is largely a matter of bookkeeping.
Finding an inconsistency, however, drops the user into inconsistency recovery
  mode where a cycle of hypothesizing one or more edges to fix and manually
  verifying these with the user until consistency is restored.

Finally, the outer loop of the overall algorithm allows the LNBNN procedure to
  generate additional candidate edges --- since as it starts it tends to pull
  out the most likely candidates first and more subtle matches as the PCCs begin
  to form (\Cref{sec:refresh}).
The priority queue will gradually be emptied as each PCC obtains a
  sufficiently redundant set of positive edges and enough negative edges to be
  complete.
Ensuring completeness requires examining $O(|V|^2)$ edges, however, so in
  practice we develop a learned probabilistic completeness measure.
%If sufficient training data is not available simple heuristics can be used to
%  terminate.
Details are described in the following sections.

% will be emptied when each PCC is sufficiently re
% Obtaining sufficient redundancy and completeness in order to empty the priority queue can,
% Ensuring all PCCs are complete leads to the need to ,
%  so to prevent this we develop a probabilistic measure (\Cref{sec:coverge})
%  that triggers much earlier convergence when positive edges are no longer
%  likely to be found.



\section{Candidate edge generation and priorities}\label{sec:cand}
To generate candidate edges the we issue each annotation as an LNBNN query
  (see Section~\ref{sec:s3}), forming edges from the resulting ranked lists.
For each candidate edge we use the pairwise algorithm (see
  Section~\ref{sec:s4}) to estimate the positive, negative, and incomparable
  probabilities.
Any edge whose maximum probability is above the threshold for automatic
  decision making is ranked according to this probability.
All other edges are ordered by their positive probability.
This ensures automatic decision making is first, followed by an ordering of
  the edges needed for manual review that are most likely to be positive and
  therefore add the most to the graph.

\section{Making decisions}\label{sec:decision}

%Threshold or manual review.
%Briefly discuss importance of minimizing manual review.
%Very short here.

For each proposed new edge, its positive, negative, and incomparable state
  probabilities are produced by the pairwise algorithm.
Each of these three states have an associated hyperparameter threshold, and if
  a single probability is greater than its threshold that edge is automatically
  added to the edge set corresponding to the predicted state.
Otherwise, the edge is sent for manual labeling and then added to the
  appropriate edge set.
% After each decision we determine the effects of the new review and update
%   candidate edge priorities as we will discuss in\cref{sec:redun}.
% However, if the edge causes an inconsistency we drop into inconsistency mode.
% Inconsistency recover will be discussed in \cref{sec:incon}.

\section{Positive and negative redundancy}\label{sec:redun}
%One paragraph on notion.
%One paragraph on algorithm.
%One paragraph on book-keeping and elimination from priority queue.

Before describing the removal of edges from the priority queue and the
detection and correction of inconsistencies, we formalize the notion of
redundancy that is the basis for both of these.
% Before we remove edges from the priority queue, we enforce a minimum level of redundancy.
%This $k$-redundancy criteria is tied to the number of mistakes that must be
%  made in order for part of the graph to incorrectly appear consistent or
%  complete.
%For a PCC (or pair of PCCs) to simultaneously contain a mistake and be
%  $k$-redundant then at least $k$ consistent mistakes must be made.
We define both a positive and a negative redundancy criteria:
\begin{enumerate}[label={(\arabic*)},noitemsep,nolistsep]

    \item %$k$-positive-redundancy --- % 
    A PCC is $k$-positive-redundant if it contains no cut-sets involving fewer
      than $k$ positive edges % (this is edge-connectedness).

    \item %$k$-negative-redundancy --- % 
    A pair of PCCs $C$ and $D$ is $k$-negative-redundant if there are $k$
      negative edges between $C$ and $D$.

    %which can be determined in $O(n_1 n_2)$ time.
    %(by looping over adjacency sets of nodes
    %in $C$ and performing set intersection with nodes in $D$ to get the edges
    %between $C$ and $D$).

    %$k$-negative-redundant if there are $k$ negative edges between them.
\end{enumerate}
To understand these criteria better, consider what it means for an incorrect
  PCC that has been determined to be $k$-positive-redundant to have an
  undiscovered error.
The error means that the PCC really should be split into (at least) two
  separate PCCs.
Suppose these PCCs correspond to animals $C$ and $D$.
If the combined PCC is $k$-positive-redundant then are $k$ separate
  undiscovered mistakes connecting $C$ and $D$, and no negative edges.
This may be plausible if $C$ were identical twins, but these tend not to occur
  for species where the distinguishing markings (\eg{} hip and shoulder of
  zebras) are mostly random.
  %\footnote{The exception to this is near identical images --- taken at most a few seconds apart --- and we implement a simple heuristic to eliminate these redundancies.}
In our current implementation we use $k=2$.

For positive-redundancy, determining that the minimal size cut-set in a PCC is
  at least $k$ can be done in linear time (in the number of component vertices)
  for $k \leq 3$~\cite{wang_simple_2015}.
Determining if two components are $k$-negative-redundant is a simple matter of
  bookkeeping.

% -positive-redundant 
% can be done in linear time in the size of the PCC 
% The $k$-positive-redundancy-update procedure procedure finds all
%  $k$-edge-connected components within a PCC $C$ and removes edges internal to
%  those components from the priority queue.
% When $k\leq3$ this can be done in $O(n)$.
%
% The $k$-negative-redundancy-update procedure determines if two PCCs $C$ and
%  $D$ with sizes $n_1$ and $n_2$ are $k$-negative-redundant in $O(n_1 n_2)$ time.
  % %using adjacency lists and set intersections.
% If $C$ and $D$ have enough negative edges between them, all other edges
%  between $C$ and $D$ are removed from the priority queue.

Note, that some PCCs (or pairs) do not have sufficient positive edges that
  would make them redundant.
This could be because the PCCs are too small or because other edges are marked
  as incomparable.
For the purposes of convergence such cases are marked as redundant.

When a positive edge is added within a single PCC, we check for
  $k$-positive-redundancy.
If this passes, all remaining internal edges for that PCC may be  removed from
  the priority queue.
When a negative edge is added between a pair of PCCs, we run the
  $k$-negative-redundancy check on the pair, and if this passes, all remaining
  edges between the PCCs may be removed from the priority queue.
When a positive edge is added between a pair of PCCs, the two PCCs are merged
  into a single new PCC $C'$, and the above $k$-negative-redundancy check must
  be run between $C'$ and all other PCCs having a negative edge connecting to
  $C'$.
It can be shown that if the graph is in a consistent state, that these are the
  only updates required.

\section{Recovering from inconsistencies}\label{sec:incon}
Whenever a decision is made that either adds a negative edge within a PCC or
  adds a positive between two PCCs with at least one negative edge between them,
  the graph becomes inconsistent.
In both of these cases we add the edge and create the result that there is a
  single PCC $C$ with internal negative edges.
The goal of inconsistency recovery mode is to change the labels of edges in
  order to make $C$ consistent.
An inconsistency implies that a mistake was made, but does not necessarily
  determine which edge contains the mistake.
Therefore, we develop an algorithm to hypothesize the edge(s) most likely to
  contain the mistake(s) using a minimum cut.
We describe the case where $C$ only contains one negative edge, but the
  general case replaces min-cut with
  multicut~\cite{vazirani_approximation_2013}.

The procedure alternates between steps of generating ``mistake hypothesis''
  edges, and presenting these to the user for review.
The ``hypothesis generation algorithm'' returns a set of negative edges or a
  set of positive edges, which if the re-labeled as positive or negative
  respectively would cause $C$ to become consistent.
The algorithm starts by creating an instance of the min-cut using the subgraph
  of $C$ containing only positive edges and the endpoints of the negative edge
  as the terminal nodes.
The weight for each edge is its initial priority plus the number of times that
  edge was manually reviewed.
%The weight of the edges is the initial priority for automatically decided edges and a constant, slightly below 1 for manually-decided edges.
The minimum cut returns a set of edges that disconnects the terminal nodes.
We compare the total weight of cut positive edges with the weight of the
  terminal negative edge (weighted using the same scheme).
If the positive weight is smaller the algorithm suggests that the cut positive
  edges should be relabeled as negative.
Otherwise, it suggests that the negative edge should become positive.

The user reviews each edge and the algorithm changes the label of the edge
  until the reviewer disagrees with the algorithm's suggestion or the review set
  is empty.
%\footnote{For our purposes here an \emph{incomparable} label by the user is counted as consistent.} 
If consistency has not been restored, the algorithm must be repeated.
%In the latter case, consistency has been restored.
%In the former case, a new inconsistency has been created and the algorithm
%  must be repeated.
When this happens, the weights of the newly reviewed edges are increased by
  $1$ in order to force the algorithm to look elsewhere for a cut.
This repeats until all inconsistencies are eliminated.
If this results in splitting one PCC into two or more, then the
  $k$-positive-redundancy and $k$-negative-redundancy tests must be repeated,
  potentially re-adding edges to the priority queue.

%As a final note, in order to ensure that this algorithm detects errors, the
%  candidate edges must contain edges that are likely to cause inconsistencies.
%Therefore, the initial set of candidate edges (described in \cref{sec:cand})
%  is always augmented with a edges that would make each non-positive-redundant PCC
%  positive-redundant.

%must ensure that inconsistencies can occur and errors can be found, 

%To ensure that the As a final note on redundancy, we elaborate on the additional edges.
  

% This algorithm generates hypothesis edges for a user to review until all
%   inconsistencies are eliminated.
% If this results in splitting $C$ into multiple PCCs, then previous implicit
%   reviews within or incident to this subgraph may no longer be valid.
% Therefore we recompute $k$-positive-redundancy within each new PCC, implicitly
%   reviewing edges where the criteria is satisfied and re-adding edges where it
%  is no longer valid.
% A similar process happens for $k$-negative-redundancy between each pair of new
%   PCCs as well as between each new PCC and all other PCCs previously
%   $k$-negative-redundant with $C$.


%multicut~\cite{vazirani_approximation_2013} 

%When $k=2$ for $k$-positive-redundancy, it is only possible for a single
%  negative edge to exist within a PCC. In this case 

%the algorithm creates 

%The algorithm starts by creating an instance of the
%  multicut~\cite{vazirani_approximation_2013} using the subgraph of $C$
%  containing only positive edges.
%Each edge is weighted by its assigned positive pairwise probability plus the
%  number of times that edge was manually reviewed.
%The terminal pairs are the negative edges.
%A feasible multicut returns a subset of  that disconnects all terminal pairs.
%Multicut is NP-hard, but it can be approximated by taking the union of
%  min-cuts between each terminal pair.
%To transform the multicut into a mistake hypothesis, we compare the total
%  weight of cut positive edges with the total weight of the terminal negative
%  edges (weighted using the same scheme).
%If positive weight is smaller we suggest that the cut positive edges should be
%  relabeled as negative.
%Otherwise, we suggest the negative edges should be relabeled as positive.

%Inconsistency recovery proceeds as follows.
%Generate a mistake hypothesis, and order the edges by positive probability.
%Present each edge hypothesis to the user in order.
%If the user agrees with the hypothesis, then change the edge label, increment
%  its review count, and continue.
%If the user disagrees, then generate a new hypothesis (using new weights and
%  labels) and restart.
%It can be shown that this process is guaranteed to converge on a consistent
%  graph state.
%Once $C$ is consistent, re-add it to $G$ and return to the main loop.

%Fixing inconsistencies can result in splitting $C$ into multiple PCCs and
%  invaliding implicit reviews inferred from $k$-redundancy either within or
%  incident to this subgraph.
%Therefore we recompute $k$-positive-redundancy within each new PCC, implicitly
%  reviewing edges where the criteria is satisfied and re-adding edges where it
%  is no longer valid.
%A similar process happens for $k$-negative-redundancy between each pair of new
%  PCCs as well as between each new PCC and all other PCCs previously
%  $k$-negative-redundant with $C$.


\section{Refreshing candidate edges}\label{sec:refresh}

As the review process executes we want to continue to review positive matches
  as long as we are discovering them.
However, at some point the candidate edges may no longer contain positive
  results, but undiscovered positive matches may still exist.  This is because LNBNN, working initially with each annotation having a separate label, can miss more subtle but correct matches, especially when there are several annotations for an animal and subtle viewpoints.
As the labeling improves, so does the reliability of the LNBNN.
Therefore, we define a refresh criteria to determine when we should recompute
  candidate edges.

The goal is to refresh if there has been a significant number of positive
  reviews, but new results are consistently negative.
If we have not found any positive edges then we do not want to refresh.
We keep track of the fraction of positive review decisions as a moving average
  of manual decisions.
We also maintain the total number of positive reviews made since the last
  candidate edge generation.
Thus the candidate edges are refreshed whenever the number of positive reviews
  is above a threshold and the positive review fraction is below a threshold

As the last outer iteration of the overall algorithm before convergence,
  triggered when the LNBNN ranking algorithm fails to produce positive edges,
  candidate edges between untested pairs of annotations are added within PCCs
  that are not $k$-positive-redundant and between PCCs that are not
  $k$-negative-redundant.
This is because the ranking algorithm itself is imperfect and the missed
  matches tend to affect small PCCs disproportionately, which are the last to
  satisfy redundancy tests.


\section{Probabilistic convergence}\label{sec:coverge}

The goal of probabilistic convergence is to determine if a PCC $C$ is
  negative-redundant with all other components with high probability.
When all components are positive-redundant and satisfy this, then all edges
  will be removed from the priority queue and the algorithm will converge.
We consider the probability $\Pr(E_c \given \nT_C)$ that an undiscovered
  positive edge exists ($E_C$) given $C$'s existing set of outgoing negative
  edges ($\nT_C$).
Under mild conditions % if we assume that $\Pr(E_c \given \nT_C) < .5$, 
we can show that the probability $\Pr(\nT_C \given E_C)$ of observing the
  negative edges bounds this p given that an undiscovered match exists can be
  used as a surrogate.
We can learn this probability offline by measuring the frequency that correct
  results are at a given rank in a PCC's ranked list (constructed by aggregating
  the ranked lists of all annotations in the PCC).


%\begin{comment}
%To predict $\Pr(\nT_C \given E_C)$ we first combine the LNBNN scores from each
%  annotation in $C$ into a single ranked list for the entire PCC.
%This can be done by sorting the maximum LNBNN score to each database PCC.
%Let $R_C$ denote the ranks of every PCC marked as negative with $C$.
%In an offline step we learn a probability mass function $\phi$ that predicts
%  the probability that a correct match appears at a given rank for the PCC $C$.
%The predicted probability is %
%$\Pr(\nT_C \given E_C) = 1 - \sum_{r \in R_C} \phi(r)$.

%To learn $\phi$, we measure the probability that a correct match appears at a
%  given rank, given a correct match exists.
%To do this initialize an accumulator.
%For each $C$ in the training set, divide it into a query $C_q$ and target
%  $C_t$.
%The target and the rest of the PCCs in the training set become database PCCs.
%Use LNBNN to score each annotation in $C_q$ against the database PCCs.
%Determine the best rank that $C_t$ appears in each ranked list, and increment
%  the corresponding index in the accumulator.
%Repeat this process for all PCCs in the training set and for multiple
%  partitions of each PCC.
%Normalizing and smoothing the accumulation array results in the PMF $\phi$.
%%Smoothing the accumulation array results in the PMF $\phi$.
%%Estimate the PMF $\phi$ by applying kernel density estimation to the
%%  accumulation array.
%In order to prevent marginalization across important attributes (such as the
%  number of exemplars in a PCC), construct multiple PMFs for different numbers
%  of exemplars in a query.
%\begin{comment}


\subsection{Exemplar selection}\label{sec:exempselect}
    To scale one-vs-many matching to larger databases and to allow the LNBNN
    mechanism to find appropriate normalizers we restrict the number of
    examples of each individual in the database to a set of exemplars.

    Exemplars that represent a wide range of viewpoints and poses are
      automatically chosen using a modified version of the technique presented
      in~\cite{oddone_mobile_2016}.
    The idea is to treat exemplar selection as a maximum weight set cover
      problem.
    For each individual, the input is a set of annotations.
    A similarity score is computed between pairs of annotations.
    To compute covering sets we first choose a threshold, each annotation is
      assigned a covering set as itself and the other annotations it matches
      with a similarity score above that threshold.
    The maximum number of exemplars is restricted by setting a maximum weight.
    Searching for the optimal set cover is NP-hard, therefore we use the
      greedy %
    $(1 - \frac{1}{e})$-approximation algorithm~\cite{michael_guide_1979}.
    The algorithm is run for several iterations in order to find a good
      threshold that minimizes the difference between the weight of the set
      cover and the maximum weight limit.

    The similarity score can be computed using the one-vs-many algorithm, but
      in our work we develop a probabilistic one-vs-one algorithm that better
      suits this purpose.
 
