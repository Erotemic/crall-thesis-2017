%\newcommand{\annotscoreop}{\opname{annot\_score}}
%\newcommand{\namescoreop}{\opname{name\_score}}
\newcommand{\annotscoreop}{\opname{K_{\tt annot}}}
\newcommand{\amechscoreop}{\opname{K_{\csum}}}
\newcommand{\fmechscoreop}{\opname{K_{\nsum}}}

\section{Matching against a database of individual animals}\label{sec:baselineranking}

    To identify a query annotation, it is matched against a database of known \names{}.
    \Aan{\name{}} is a set of annotations known to depict the same animal.
    The basic matching pipeline can be summarized in \three{} steps:
    establish feature correspondences \rpipe{} %
    score feature correspondences \rpipe{} %
    aggregate feature correspondence scores across the \names{}.
    Correspondences between a query annotation's features and \emph{all} database annotation features are
      established using an approximate nearest neighbor algorithm.
    This step also establishes a normalizing feature which is used to measure the distinctiveness of a query
      feature.
    Each feature correspondence is scored based on the feature weights established in the previous section and a
      measure of the distinctiveness of the query feature.
    The feature correspondence scores are then aggregated into a \glossterm{\namescore{}} for each \name{} in the
      database.
    The \namescores{} induce a ranking on \names{} in the database where database \names{} with higher ranks are
      more likely to be correct matches.

   \subsection{Establishing initial feature correspondence}\label{sub:featmatch}

        \paragraph{offline indexing}
            Before feature correspondences can be established, an offline algorithm indexes descriptors from all
              database annotations for fast approximate nearest neighbor search.
            All database descriptor vectors are stacked into a single database array of vectors, %
            $\AnyDB$, % should this be removed?
              %
            and these descriptors are indexed by an inverted file.
            The inverted file maps each descriptor in the stacked array back to its original annotation and
              feature.
            This database array is indexed for nearest neighbor search using a forest of
              kd-trees~\cite{silpa_anan_optimised_2008} using the FLANN library~\cite{muja_fast_2009}, which were
              reviewed in~\cref{sec:ann}.
            This allows for the efficient implementation of a \codeobj{neighbor index function}  %
            $\NN(\AnyDB, \desc, \K)$  %
            %$\NN(\desc, \K)$  %
            that returns the indices in $\AnyDB$ of the $\K$ approximate nearest neighbors of a query feature's
              descriptor $\desc$.
            %that returns the database indices of the $\K$ approximate
            %  nearest neighbors of a query feature's descriptor
            %$\desc$.

        \paragraph{Approximate nearest neighbor search}

            Matching begins by establishing multiple feature correspondences between each query feature and
              several visually similar database features.
            For each query descriptor vector $\desc_i \in \X$ the $\K + \Knorm$ approximate nearest neighbors are
              found using the \coderef{neighbor index function}.
            These neighbors sorted by ascending distance are:
            \begin{equation}
                \NN(\AnyDB, \desc_i, \K + \Knorm) \eqv \dotarrIII{j}{\K}{\K + \Knorm}
                %\NN(\desc_i, \K + \Knorm) \eqv \dotarrIII{j}{\K}{\K + \Knorm}
            \end{equation}
            The $\K$ nearest neighbors, $\dotsubarr{\desc}{{j_1}}{{j_\K}}$, are the initial feature
              correspondences to the $i$\th{} query feature.
            The remaining $\Knorm$ neighbors, $\dotsubarr{\desc}{{j_{\K + 1}}}{j_{\Knorm}}$, are candidate
              normalizers for use in LNBNN scoring.

        \paragraph{Normalizer selection}
            A single descriptor $\descnorm_{i}$ is selected from the $\Knorm{}$ candidate normalizers and used in
              computing the LNBNN score for all (up to $\K$) of the $i$\th{} query descriptor's correspondences in
              the database.
            The purpose of a normalizing descriptor is to estimate the local density of descriptor space, which
              can be interpreted as a measure of the query descriptor's distinctiveness \wrt{} the database.
            The normalizing descriptor is chosen as the most visually similar descriptor to the query that is not
              a correct match.
            In other words, the query descriptor's normalizer should be from an individual different from the
              query.
            The intuition is there will not be any features in the database that are close to distinctive
              features in the query except for the features that belong to the correct match.

            The selection process described in the original formulation of LNBNN is to simply choose the $\K +
              1$\th{} nearest neighbor, which amounts to setting $\Knorm=1$.
            The authors of LNBNN find that there is no benefit to using a higher value of
              $\Knorm$~\cite{mccann_local_2012}.
            However, this does not account for the case when the $\K + 1$\th{} nearest neighbor belongs to the
              same class as one of the nearest $\K$ neighbors.
            Therefore, we employ a slightly different selection process.
            To motivate our selection process, consider the case when there are more than $\K$ images of the same
              individual from the same viewpoint in the database and a distinctive feature from a new annotation of
              that individual is being scored.
            In this case $\K$ correspondences will be correctly established a distinctive the query feature and
              $\K$ database features.
            However, if the normalizer is chosen as the $\K + 1$ neighbor, then these correspondences will be
              inappropriately downweighted.
              
            % probably need to reword.
            Consider the example in~\cref{fig:knorm}.
            In this case there are two examples \Cref{sub:knorma,sub:knormb} of the query images in the database.
            The figure shows the case where $\K=3$ and $\Knorm=1$.
            Even though there is an incorrect match, the LNBNN scores of the correct matches are an order of
              magnitude higher than the score for the incorrect match.
            Now, consider the case where the number of correct matches in the database is greater than $\K$ by
              setting $\K=1$.
            In this case the normalizing descriptor is the ``same'' feature as the query feature and the nearest
              match drops from $0.066$ to $0.007$.

            \knorm{}

            To avoid this case, a normalizing feature is carefully chosen to reduce the possibility that it
              belongs to a potentially correct match.
            More formally, the normalizing descriptor is chosen to be the descriptor with the smallest distance
              to the query descriptor that is not from the same \name{} as any of the chosen correspondences.
            Let $\nid_j$ be the \name{} associated with the annotation containing descriptor $\desc_j$.
            Let %
            %$\multiset{N}_i \eqv \curly{\nid_j \quad \forall j \in \NN(\desc_i, \K + \Knorm)}$
            $\multiset{N}_i \eqv \curly{\nid_j \where j \in \NN(\AnyDB, \desc_i, \K + \Knorm)}$
              %\NN(\desc_i, \K + \Knorm)}$
              %
            be the set of \names{} matched by the $i$\th{} query feature.
            The descriptor that normalizes all matches of query descriptor $\desc_i$ is:
              \begin{equation}
                  \descnorm_{i} \eqv 
                  \argmin{\desc_j \in \dotsubarr{\desc}{{j_{\K + 1}}}{j_{\Knorm}}}
                  \elltwosqrd{\desc_j - \desc_i} \where \nid_j \notin \multiset{N}_i
              \end{equation}

    \subsection{Feature correspondence scoring}
        Each feature correspondence is given a score representing how likely it is to be a correct match.
        While the L2-distance between query and database descriptors is useful ranking feature correspondences
          based on visual similarity, the distinctiveness of the match is more useful for ranking the query
          annotation's similarity to a database annotation~\cite{lowe_distinctive_2004,
          arandjelovic_dislocation_2015, mccann_local_2012}.
        However, highly distinctive matches from other objects --- like background matches --- do not provide
          relevant information about a query annotation's identity and should not contribute to the final score.
        Therefore, each feature correspondence is scored using a mechanism that combines both distinctiveness and
          likelihood that the object belongs to the foreground.
        For each feature correspondence $m = (i, j)$ with query descriptor $\desc_i$ and matching database
          descriptor $\desc_j$, several scores are computed which are then combined into a single feature
          correspondence score $s_{i,j}$.

        \paragraph{LNBNN score}\label{sec:lnbnnscore}

            Using the normalizing feature, $\descnorm_{i}$, LNBNN compares a query feature's similarity to the
              match and query feature's similarity to the normalizer.
            This serves as an estimate of local feature density and measures the distinctiveness of the feature
              correspondence.
            A match is distinctive when the query-to-match distance is much smaller than the query-to-normalizer
              distance, \ie{} the local density of descriptor space around the query is sparse.
            The LNBNN score of a feature match is computed as follows:
            \begin{equation}\label{eqn:lnbnn}
                \fs_{\LNBNN} \eqv \frac{\elltwo{\desc_i - \descnorm_{i}} - \elltwo{\desc_i - \desc_j}}{Z}
            \end{equation}
            All descriptors used in this calculation are L2-normalized to unit length --- \ie{} to sit on the
              surface of a unit hypersphere.
            The $Z$ term normalizes the score to ensure that it is in the range $\rangeinin{0, 1}$.
            If descriptor vectors have only non-negative components (as in the case of
              SIFT~\cite{lowe_distinctive_2004}) then the maximum distance between any two L2-normalized
              descriptors is $Z\tighteq\sqrt{2}$.
            If descriptors vectors have negative components (like those that might extracted from a deep
              convolutional neural network~\cite{zagoruyko_learning_2015}) then the maximum distance between is
              $Z\tighteq2$.

        \paragraph{Foregroundness score}
            To reduce the influence of background matches, each feature correspondence is assigned a score based
              on the foregroundness of both the query and database features.
            The geometric mean of the foregroundness of query feature, $w_i$, and database feature, $w_j$, drives
              the score to $0$ if either is certain to be background.
            % show python -m ibeis --db testdb3 --query 325 -y
            \begin{equation}
                \fs_{{\tt fg}} \eqv \sqrt{w_i w_j}
            \end{equation}

        \paragraph{Final feature correspondence score}
            The final score of the correspondence $(i, j)$ captures both the distinctiveness of the match as well
              as the likelihood that the match is part of the foreground.
              \begin{equation}\label{eqn:featscore}
                  \fs_{i,j} \eqv \fs_{{\tt fg}} \fs_{\LNBNN} 
              \end{equation}

    \subsection{Feature score aggregation}\label{subsec:namescore}

        So far, each feature in a query annotation has been matched to several features in the database and a
          score has been assigned to each of these correspondences based on its distinctiveness and foregroundness.
        The next step in the identification process is to aggregate the scores from these patch-based
          correspondences into a single \glossterm{\namescore} for each \name{} in the database.
        Note that this \name-based definition of scoring is a key difference between animal identification and
          image retrieval, where a score is assigned to each image in the database.
        In animal identification the analogous concept is an \glossterm{\annotscore} --- a score assigned to each
          annotation in the database~\cite{philbin_object_2007}.
        This distinction between a score from a query annotation to a database annotation is important because
          the goal of the application is to classify a new query annotation as either a known \name{} or as a new
          \name{}, not to determine which annotations are most similar.

        This subsection presents two mechanisms to compute \namescores{}.
        The first mechanism is \csumprefix{} and computes a \namescore{} in two steps.
        This mechanism aggregates feature correspondences scores into an \annotscore{} for each annotation in the
          database.
        Then the \annotscores{} are aggregated into a score for each \name{} in the database.
        The second mechanism is \nsumprefix{}.
        This mechanism aggregates feature correspondences scores matching multiple database annotations directly
          into a \namescore{}.
        These mechanisms are respectively similar to the image-to-image distance and the image-to-class distance
          described in~\cite{boiman_defense_2008}.

        \paragraph{The set of all feature correspondences}
        All scoring mechanisms presented in this subsection are based on aggregating scores from features
          correspondences.
        The set of all feature correspondences for a query annotation $\X$ is expressed as:
        \begin{equation}
            %\Matches \eqv \{(i, j) \where \desc_i \in \X \AND j \in \NN(\desc_i, \K)\}
            \Matches \eqv \{(i, j) \where \desc_i \in \X \AND j \in \NN(\AnyDB, \desc_i, \K)\}
        \end{equation}

        % FIXME: SAME AS IMAGE-TO-IMAGE
        \paragraph{Annotation scoring}
            An \annotscore{} is a measure of similarity between two annotations.
            An \annotscore{} between a query annotation and a database annotation is defined as the sum of the
              feature correspondence scores matching to the features from that database annotation.
            %However, the \annotscore{} will allow us to compare our
            %  techniques against other instance recognition techniques.
            Let $\daid_j$ be the database annotation containing feature $j$.
            Let
            %
            $\Matches_{\daid} \eqv \curly{(i, j) \in \Matches \where \daid_j = \daid}$
            %
            denote all of the correspondences to a particular database annotation.
            The \annotscore{} between the query annotation $\qaid$ and database annotation $\daid$ is:
            \begin{equation}
                \annotscoreop(\qaid, \daid) \eqv \sum_{(i, j) \in \Matches_{\daid}} \fs_{i, j}
            \end{equation}

        % FIXME: SAME AS IMAGE-TO-CLASS
        \paragraph{Name scoring (1) --- \csumprefix{}} %

            The \cscoring{} mechanism aggregates \annotscores{} into \namescores{} by taking as the score highest
              scoring annotation for each \name{}.
            In our experiments we refer to this version of \namescoring{} as \csum{}.
            Let $\nid$ be the set of database annotations with the same \name{}.
            The \cscore{} between a query annotation and a database \name{} is the maximum over all annotations
              scores in that \name{}:
            \begin{equation}
                \amechscoreop(\qaid, \nid) 
                \eqv
                \max_{\daid \in \nid}
                \paren{
                    \annotscoreop\paren{\qaid, \daid}
                }
                %\opname{annot\_score}(\qaid, \daid) \eqv \sum_{m_a \in \Matches_{\daid}} \fs_{i, j}
            \end{equation}

         \paragraph{Name scoring (2) --- \nsumprefix{}} %
            The \cscoring{} mechanism accounts for the fact that animals will be seen multiple times, but it does
              not take advantage of complementary information available when \aan{\name{}} has multiple
              annotations.
            The following aggregation mechanism combines scores on a feature level to correct for this.
            It allows each query feature at a specific location to vote for a given \name{} at most once.
            Thus, when a query feature (or multiple query features at the same location) corresponds to database
              features from multiple views of the same animal, only the best correspondence for that feature will
              contribute to the score.
            In our experiments we refer to this version of \namescoring{} as \nsum{}.

            The first step of computing \aan{\namescore{}} for a specific \name{} is grouping the feature
              correspondences.
            Two feature correspondences are in the same group if the query features have the same location and
              the database features belong to the same \name{}.
            The next step is to choose the highest scoring correspondence within each group.
            The sum of the chosen scores is the score for \aan{\name{}}.
            This procedure is illustrated in~\cref{fig:namematch}.

            \newcommand{\MatchesGroup}{\Matches^{G}}

            Formally, consider two feature correspondences $\mI\tighteq(\iI, \jI)$ and $\mII\tighteq(\iII,
              \jII)$.
            Let $\pt_{\iI}$ and $\pt_{\iII}$ be the $xy$-location of the query feature in a correspondence.
            Let $\nid_{\jI}$ and $\nid_{\jII}$ be the \name{} of the database annotations containing the matched
              features.
            The group that contains feature correspondence $\mI$ is defined as:
            \begin{equation}
                \MatchesGroup_{\mI} \eqv \curly{\mII \in \Matches  \where
                \paren{
                    \paren{\pt_{\iI} \eq \pt_{\iII}} \AND 
                    \paren{\nid_{\jI} \eq \nid_{\jII}}
                }
            }
            \end{equation}
            The correspondence with the highest score in each connected component is flagged as chosen.
            Ties are broken arbitrarily.
            \begin{equation}
                \ischosen(\mI) \eqv 
                \bincase{
                \paren{
                    \fs_{\mI} > \fs_{\mII} 
                    \quad \forall \mII  \in \MatchesGroup_{\mI}
                } 
                \OR
                \card{\MatchesGroup_{\mI}} \eq 1
                }
            \end{equation}

            Let $\Matches_{\nid} \eqv \{(i, j) \in \Matches \where
              \nid_j = \nid\}$ denote all of the correspondences to a particular
              \name{}.
            The \nscore{} of \aan{\name{}} is:
            \begin{equation}
                \fmechscoreop(\qaid, \nid) 
                \eqv 
                \sum_{m \in \Matches_{\nid}} \ischosen(m) \; \fs_m
            \end{equation}

            \namematch{}
