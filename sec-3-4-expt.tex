
\section{Experiments}\label{sec:experiments}

    This section presents an experimental evaluation of the identification algorithm using annotated images of
      plains zebras, Grévy's zebras, Masai giraffes, and humpback whales.
    The input to each experiment is
    (1) a dataset,
    (2) a subset of query and database annotations from the database,
    (3) a pipeline configuration.
    The datasets are described in~\cref{sub:datasets}.
    The subsets of query and database annotations are carefully chosen to measure the accuracy of the algorithm
      under different conditions and to control for time, quality, and viewpoint.
    The pipeline configuration is a set of parameters --- \eg{} the level of feature invariance, the number of
      nearest neighbors, and the \namescoring{} mechanism --- given to the identification algorithm.
    We will vary these pipeline parameters in order to measure their effect on the accuracy of the ranking
      algorithm.

    For each query annotation, the identification algorithm returns a ranked list of \names{} with a score for
      each name.
    The accuracy of identification is measured using the cumulative match
      characteristic~\cite{decann_relating_2013} which can be understood as the probability that a query correctly
      finds a match at a specified rank under the assumption that a correct match exists in the database.
    We are primarily returned with only the first point in this curve --- the fraction of queries with a correct
      result at rank $1$ --- because often a user of the system will only review the first result of a query.

    Additionally, we like to automatically review results returned by the ranking algorithm, so we will measure
      the separability of correct and incorrect results using the scores returned by the ranking algorithm.
    The separability of scores is measured by first recording the score of the \groundtrue{} \name{} and the
      highest scoring \groundfalse{} \name{} for each query and then plotting the receiver operator characteristic
      (ROC) curve and measuring the area under the curve (AUC).
    %Other measures that are reported include the percentage of true positive
    %  and false positive results that fall within certain categories (\eg
    %  temporal windows).

    The outline of this section is as follows.
    First, \cref{sub:datasets} introduces and describes each dataset.
    Our first experiment in \cref{sub:exptbase} establishes the accuracy of our ranking algorithm on several
      datasets using a default pipeline configuration.
    We then compare our approach to an alternative \cref{sub:exptsmk} SMK approach.
    The next subsections perform in depth experiments on the parameter settings of our algorithm.
    \Cref{sub:exptfg} tests the effect of the foregroundness weight on identification accuracy.
    \Cref{sub:exptinvar} investigates the effect of the level of feature invariance and viewpoint.
    \Cref{sub:exptscoremech} compares the \csumprefix{} and the \nsumprefix{} \namescoring{} mechanism.
    \Cref{sub:exptk} varies the $\K$ parameter (the number of nearest neighbors used in establishing feature
      correspondences) and investigates the relationship between $\K$ and database size in terms of both the number
      of annotations and the number of exemplars per name.
    \Cref{sub:exptfail} discusses the failure cases of our ranking algorithm.
    \Cref{sub:exptsep} presents an evaluation of the score separability for the pipeline configuration with the
      highest accuracy determined for each species.
    Finally,~\cref{sub:exptsum} summarizes this section.


    \subsection{Datasets}\label{sub:datasets}

        All of the images in the datasets used in these experiments were taken by photographers in the field.
        Each dataset is labeled with groundtruth in the form of annotations with name labels.
        Annotations (bounding boxes) have been drawn to localize animals within the image.
        A unique \name{} label has been assigned to all annotations with the same identity.
        Some of this groundtruth labeling was generated independently.
        However, large portions of the datasets were labeled with assistance from the matching algorithm.
        While this may introduce some bias in the results, there was no alternative because the amount of time
          needed to independently label a large dataset is prohibitive.

        There are two important things to note before we describe each dataset.
        First, in order to control for challenging factors in the images such as quality and viewpoint some
          experiments sample subsets of the datasets we describe here.
        Second, note that there do exist labeling errors in some datasets.

        \DatabaseInfo{}

        \timedist{}

        The number of names, annotations, and their distribution within each database are summarized in the
          following tables.
        In these tables we distinguish between \glossterm{singleton} and \glossterm{resighted} names.
        Singleton names are individuals with only a single encounter, and resighted names contain more than one
          encounter.
        We make this distinction because resighted names have known correct matches across a significant time
          delta.
        Note, that singleton names may still have more than one annotation, but those annotations are all from
          the same encounter.
        We have pre-filtered each database to remove annotations that are unidentified, are missing timestamps,
          or are labeled as ``junk'' quality.

        \Cref{tbl:DatabaseStatistics} summarizes the number of annotations and individuals in each database as
          well as the number of times (distinct encounters) each individual was sighted.
        \Cref{tbl:AnnotationsPerQuality} summarizes the quality labels of the annotations.
        \Cref{tbl:AnnotationsPerViewpoint} summarizes the viewpoint labels of the annotations.
        Distributions of when images in each dataset were taken are illustrated in \cref{fig:timedist}.
        The name and a short description of each dataset is given in the following list.

        \begin{itemln}
            \item \textbf{Plains zebras}.
            Our plains zebras dataset is an aggregate of several smaller datasets.
            There is variation in how the data was collected and preprocessed.
            Some of the images are cropped to the flank of the animal, while others are cropped to encompass the
              entire body.
            The constituent datasets were collected in Kenya at several locations including Nairobi National
              Park, Sweetwaters, and Ol Pejeta.
            More than $90\percent$ of the groundtruth generated for this dataset was assisted using the matching
              algorithm.
            This dataset contains many imaging challenges including occlusion, viewpoint, pose, quality, and time
              variation.
            There are some annotations in this dataset without quality or viewpoint labelings and some images
              contain undetected animals.
            This data was collected between 2006 and 2015, but the majority of the data was collected in
              2012--2015.

            \item \textbf{Grévy's zebras}.
            This is another aggregated dataset.
            The original groundtruth for this dataset was generated independently of the matching algorithm,
              however the matching algorithm revealed several groundtruth errors that have since been corrected.
            The Grévy's dataset was collected in Mpala, Kenya.
            Most of the annotations in this database have been cropped to the animal's flank.
            This dataset contains a moderate degree of pose and viewpoint variation as well as occlusion.
            This data was collected between 2003 and 2012, but the majority was collected in 2011 and 2012.

            \item \textbf{Masai Giraffes}.
            These images of Masai Giraffes were all taken in Nairobi National Park during the \GZC{} between
              February 20, 2015 and March 2, 2015.
            All groundtruth was established using the matching algorithm followed by manual verification.
            This dataset contains a high degree of pose and viewpoint variation, as well as occlusion.
            Because of their long necks, it is difficult to ensure that only a single giraffe appears in each
              annotation.
            This results in many \glossterm{photobombs} --- pairs of annotations where a background animal in one
              annotation matches the foreground animal in the other --- when matching.

            \item \textbf{Humpback Whales}.
            The humpback dataset was collected by FlukeBook over nearly 15 years.
            Images were contributed by both marine and citizen scientists.
            The original groundtruth was established using both manual and automated methods that are disjoint
              from these techniques considered here, however our software was used to correct mistakes.
            The annotations in this dataset have not been manually reviewed.
            Some are cropped to the fluke while others encompas the entire image.
            Quality and viewpoint labels do not exist for this dataset.
        \end{itemln}

    \subsection{Baseline experiment}\label{sub:exptbase}
      
        \BaselineExpt{}

        This first experiment determines the accuracy of the identification algorithm using the baseline pipeline
          configuration.
        The baseline pipeline configuration uses affine invariant features oriented using the gravity vector,
          $\K\tighteq4$ as the number of feature correspondences assigned to each query feature, and \nscoring{}
          (\nsum{}).
        In this test we control for several biases that may be introduced by carefully selecting a subset of our
          datasets.
        We only use annotations that
        (1) are known (\ie{} have been assigned a name),
        (2) are comparable the species primary viewpoint (\eg{} left, front-left, and back-left for plains
          zebras),
        (3) have not been assigned a quality of ``junk''.
        Furthermore, to account for the fact that some \names{} contain more annotations than others, we
          constrain our data selection such that there is only one correct exemplar in the database for each query
          annotation.

        Of these annotations, we group them into encounters.
        For each encounter we sample one annotation with the highest quality.
        Names with only one encounter are added to the database as distractors.
        For the other names, we randomly sample two encounters --- regardless of quality --- one for the database
          and one to use as a query.
        This defines a set of query and database annotations that are separated by time, testing the ability of
          our system to match animals across gaps of time using only a single image per individual.
        The CMC curves for this baseline test are illustrated in~\cref{fig:BaselineExpt}.

        The results of this baseline experiment demonstrates that our algorithm is able to reliably find matching
          annotations in a database with many other images.
        The accuracy is over $60\percent$ for all species considered.
        Subsequent experiments will restrict our focus to Grévy's plains zebras in order to investigate detailed
          parameter choices of the ranking algorithm as well as alternative ranking algorithms.

    \subsection{SMK as an alternative}\label{sub:exptsmk}  

        Before we investigate the parameter choices of the LNBNN ranking algorithm, we briefly evaluate the
          performance of an alternative ranking algorithm, namely VLAD-flavored SMK.
        The SMK algorithm is a vocabulary based algorithm, that is representative of more traditional approaches
          to instance recognition problems.
        In contrast to the raw descriptors used in LNBNN, SMK assigns each descriptor to a visual word and builds
          a weighted histogram of visual words (accumulated residual vectors in the case of VLAD) to represent each
          annotation as a sparse fixed length vector.
        We have experimented with several configurations of VLAD and report our best results here.

        \SMKExpt{}

        In our SMK implementation, we pre-trained an $8000$ word vocabulary using mini-batch k-means on the
          stacked descriptors from all database annotations.
        Note that typically the vocabulary is trained using a disjoint external dataset in order to prevent
          overfitting.
        However, we \naively{} train using the database annotations to be indexed, understanding that this will
          inflate the accuracy measurements.
        Each word in the vocabulary is weighted with its inverse document frequency.
        We use the vocabulary to compute an inverted index that maps each visual word to annotations containing
          that word in the database.
        Initial feature correspondences for a descriptor are computed using single assignment to a visual word
          and then creating a correspondence to every feature in that word's inverted index.
        We use spatial verification to filter spatially invalid correspondences, and re-score the remaining
          matches.

        The results of the SMK experiment are illustrated in \cref{fig:SMKExpt}.
        The query and database annotations are the same in each experiment.
        Despite the bias in the SMK vocabulary, we measure that LNBNN provides the most accurate rankings.
        For plains zebra's there is a difference of $10\percent$ in the number of correct matches at rank $1$,
          and for Grévy's zebras the difference is over $20\percent$.


    \subsection{Foregroundness experiment}\label{sub:exptfg}

        \ForegroundExpt{}

        In this experiment we test the effect of our foregroundness weights --- weighting the score of each
          features correspondence with a foregroundness weight --- on identification accuracy.
        When foregroundness is enabled, each feature correspondence is weighted using a foregroundness measure
          learned using a deep convolutional neural network~\cite{parham_photographic_2015}.

        Running this experiment with using the query / database sample as outlined in the baseline experiment
          does not result in a noticeable difference in scores because the purpose of the foregroundness measure is
          to down weight matches between scenery objects (\eg{} trees, grass, bushes) that appear in multiple
          annotations.
        The baseline database sample contains only a single images from each encounter and only two encounters
          per individual.
        This means that it will be unlikely for an annotation in the query set and another annotation in the
          database set to have a similar background.

        To more clearly illustrate the effect of the foregroundness measure we use a different sampling strategy.
        We group all encounters by which occurrence they belong to.
        Annotations within the same occurrence are more likely to share background.
        We sort the occurrences by size in descending order, and then iterate through each occurrence.
        We sample the highest quality annotation in encounter in the occurrence as long as that name has not been
          sampled more than two times.
        At the end of this process we have at most two annotations for each name.
        We randomly choose annotations to be query and database annotations from each name with two annotations.
        The rest are used as confusers.
        We execute the ranking algorithm twice with foregroundness both enabled and disabled.
        The rest of the pipeline configuration is the same as the baseline test.

        The accuracy of the foregroundness is illustrated in~\cref{fig:ForegroundExpt}.
        %For plains zebras, using the foregroundness measure results in a significant $3.79\percent$ increase in
        %  identification accuracy.
        %For Grévy's zebras there is also a significant $3.3\percent$ increase.
        %This experiment clearly shows the importance of eliminating background feature correspondences.
     
    \subsection{Invariance experiment}\label{sub:exptinvar} %
        In this experiment we vary the feature invariance configuration.
        This influences the location, shape, and orientation of keypoints detected in each annotation, which in
          turn influences which regions in each annotation are matchable using SIFT descriptors extracted at each
          keypoint.
        The best invariance settings will be depend on properties of the data.

        In our experiments we test different settings by enabling (denoted as T) or disabling (denoted as F) the
          parameters affine invariance (AI), and our query-side rotation heuristic (QRH).
        Initially we also tested rotation invariance, but found that it provided the poorest results for all
          datasets by a significant margin, likely because the gravity vector assumption is mostly satisfied in all
          images.
        Therefore, we exclude rotation invariance from our experiments.

        In configurations where \pvar{AI=F}, keypoints are circular with a radius defined by the scale at which
          it was detected.
        When \pvar{AI=F}, the keypoint shape is adapted into an ellipse to normalize for small viewpoint changes.
        When \pvar{QRH=F}, each keypoint is assigned its orientation as normal, but when \pvar{QRH=T}, each
          keypoint in a query annotation is replaced by 3 keypoints, one rotated slightly to the left, another
          slightly to the right, and the last is the original keypoint.
        The four specific configuration that we test are outlined in the following list:

        \begin{itemln}

            \item \NoInvar{} (\pvar{AI=F,QRH=F}): % 
                This configuration uses circular keypoints and assumes the gravity vector.

            \item \AIAlone{} (\pvar{AI=T,QRH=F}): % 
                This is the baseline setting that assumes the gravity vector and where each feature's shape is skewed
                  from a circle into an ellipse.

            \item \QRHCirc{} (\pvar{AI=F,QRH=T}): %
                This is a novel invariance heuristic where each {database} feature assumes the gravity vector, but
                  {query} feature is $3$ orientations:
                the gravity vector and two other orientations at $\pm15\degrees$ from the gravity vector.
                Ideally, this will allow feature correspondences to be established between features seen from
                  slightly different orientations.

            \item \QRHEll{} (\pvar{AI=T,QRH=T}): %
                This is the combination of \QRHCirc{} and \AIAlone{}.

        \end{itemln}

        \InvarExpt{}

        % Invar Conclusions
        The example in~\cref{fig:kptstype} illustrates the difference between \AIAlone{} and \QRHCirc{} features
          for plains and Grévy's zebras.
        The accuracy of the invariance experiment is shown in~\cref{fig:InvarExpt}.
        For plains zebras, the \QRHCirc{} scores are significantly better than all other invariance settings.
        Interesting, affine invariance results in worse performance when QRH is on, but if the QRH is off then
          affine invariance improves accuracy.
        This suggests that the QRH better handles matching the coarse patterns seen on the plains zebras across
          pose and viewpoint variations than using affine invariance, which can tend to adapt itself around
          non-distinctive diagonal stripes.
        Even though affine keypoints provide more precise localization, the area they describe is often smaller
          than a circular keypoint.
        It makes sense that affine keypoints would not describe coarse features as well as a circular keypoint
          covering a larger area.
        %
        The results for Gravy's zebras demonstrate similar levels of accuracy for \AIAlone{} and \QRHEll{}.
        Affine invariance seems to be the most important setting for matching Grévy's zebras.
        The distinctive details on Grévy's zebras are finer then plains zebras and are well captured by affine
          keypoints.
        While the QRH does improve accuracy for Grévy's zebras the density of the distinctive keypoints means
          that it is less important because it is more likely that a two annotations will have at least one
          distinctive region aligned an in common.

        \kptstype{}

    \subsection{Scoring mechanism experiment}\label{sub:exptscoremech}  

        % TODO: change experiment so only one annotation per name is chosen for
        % each confuser

        % Database setup for name scoring
        The purpose of the scoring mechanism is to aggregate scores of individual feature correspondences across
          multiple annotations into a single score for each name --- \ie{} an annotation-to-name similarity, which
          is analogous to the image-to-class distance used in~\cite{boiman_defense_2008}.
        We test the identification accuracy of the two name scoring mechanisms that were described earlier
          in\cref{subsec:namescore}:
        (1) \cscoring{} (denoted as \csum{}) and
        (2) \nscoring{} (denoted as \nsum{}).

        \NScoreExpt{}

        Because the scoring mechanism is meant to take advantage of multiple \exemplars{} per \name{} we vary the
          number of \exemplars{} per database \name{} between $1$, $2$, and $3$.
        Varying the number of \exemplars{} will cause each database to contain a different number of annotations.
        To normalize difference in database size we include additional confuser annotations (annotations that do
          not match any query) in the smaller databases to maintain a constant database size across experiments.
        Each \exemplar{} is chosen from a separate encounter.

        The accuracy of the scoring mechanism experiment is shown in~\cref{fig:NScoreExpt}.
        The results of this test does suggest that \nsum{} results in slightly more accurate ranking, but the
          overall difference in accuracy is relatively small (about $1-3\percent$).
        Intuitively, the \nsum{} mechanism should produce better ranks because it does not double count incorrect
          feature correspondences while still accumulating scores from multiple correspondences to different
          correct annotations.

        Perhaps the more interesting result of this experiment is the effect of increasing the number of
          exemplars in the database from $1$ to $2$.
        There is a drastic improvement in ranking accuracies in both species.
        The accuracy of plains zebras increases by $10\percent$ and for Grévy's zebras the gain is almost
          $20\percent$.
        It makes sense that this should be the case.
        If there are more examples of an individual in the database then the probability that the query is
          similar to at least one of them should increase as long as there is sufficient variation.
        This suggests that even if a new query new annotation initially fails to rank the correct result,
          subsequent annotations added to the system of the same individual will be more likely to correctly match
          a previous annotation.
        As more annotations of that individual are added the likelihood that the ranking algorithm will make a
          connection between all instances of that individual will increase.


    \subsection{K experiment}\label{sub:exptk}  

        % Introduce varied parameters
        In this experiment we investigate the effect $\K$ (the number of nearest neighbors used in establishing
          feature correspondences) on identification accuracy.
        We vary $\K$ between the values $1, 2, 4$, and $6$.
        In all of these experiments we set the number of normalizing neighbors to be $\Knorm=1$.

        \KExpt{}

        Two database factors that may influence the best choice of $\K$ are the number of annotations in the
          database and the number of annotation per name in the database.
        If there are more correct matches for a query annotation it would be beneficial to allow it to match more
          annotations.
        Likewise, if there are more overall annotation in the database, then it might be beneficial to search
          deeper into all of the database descriptors to find the correct matches.
        Therefore, in addition to varying $\K$ we also vary the number of annotations per individual and the
          overall number of annotations in the database.

        We use a protocol similar to the one used in the scoring mechanism experiment to sample databases.
        The difference is that we use the extra confusers annotations to vary the total number of annotations in
          the database.
        However, controlling for these factors constrains the number of annotations we can use.
        For Grévy's, we can vary the total database size between $476$ and $774$.
        For plains we have more confuser annotations allowing us to test database sizes of $578$ and $1650$.
        We vary the number of \exemplars{} per name between $1$ and $2$

        The results of this experiment are illustrated in ~\cref{fig:KExpt}.
        Similarly to the previous experiment, the number of exemplars per name is the most significant variable
          impacting accuracy.
        Furthermore, when there are more exemplars in the database the choice of $\K$ starts become less
          significant.
        The results also show that accuracy does slightly decrease when the database becomes larger, but
          magnitude of the decrease is between $1\percent$ and $3\percent$.
        Interesting the optimal choice of $\K$ is not consistent between species when there is only one exemplar
          per name.
        For Grévy's zebras using a lower $\K$ results in better results, but for plains zebras there is a
          signifiant loss when $\K=1$ and the database size is large.
        This is likely due to the nature of the distinguishing patterns on the different zebras.
        When matching the detailed patterns of the Grévy's zebras, it is better to use a low $\K$ to reduce
          noise, but for coarser plains zebras patterns a low $\K$ might not find a correct match immediately.
        Thus, the choice of $\K$ is a trade-off between precision and recall that depends on the type of texture
          patterns that are being matched.
        
        %Our second test varies the size of the database as well as the value of $\K$.
        %The effect of $\K$ and database size on matching accuracy is shown in~\cref{fig:DBSizeExpt}.
        %The results for all species show that the number of \exemplars{} per \name{} is the most important factor
        %  in this experiment.
        %Interestingly, the number of annotations in the database is only a minor factor in identification
        %  accuracy.
        %% plains
        %The results for plains zebras show a small positive relationship between the number of annotations in the
        %  database and $\K$.
        %This may be because many plains zebra features are not globally distinctive in a large database and a
        %  feature's correct correspondence may not be the nearest neighbor.
        %For smaller database sizes lower values of $\K$ produce more accurate results.
        %% Grévy's
        %For Grévy's zebras lower values of $\K$ seem better for all database sizes.
        %It also seems that the best values of $\K$ should be set to the number of \exemplars{} per \name{}, which
        %  is expected when there is little confusion between features.
        %% giraffes
        %For Masai giraffes, the amount of data again makes it difficult to draw conclusions.

        %\DBSizeExpt{}

        Overall the experiments on the setting of $\K$ does not yield definitive choice for this parameter.
        However, it appears that $\K$ only has a small influence on identification accuracy.
        This section does shows that the number of exemplars per annotation has a significant impact on
          identification accuracy.

    \subsection{Failure cases}\label{sub:exptfail}  
        
        In this subsection we investigate the primary causes of identification failure by considering individual
          failure cases.
        When investigating the cause of a failure case we consider two matches from the query annotation to a
          \name{}:
        (1) the match from the query annotation to the \groundfalse{} \name{} at rank $1$, and
        (2) the match from the query annotation to the \groundtrue{} \name{}.
        %We manually label the \groundtrue{} match and the \groundfalse{} match in each failure case to indicate
        %  the factors that heuristically appear to cause identification to fail.
        %We accumulate the frequency of these cases into a histogram to illustrate the significance of each type
        %  of failure case.
        %The failure case histograms are shown in~\cref{fig:TagExpt}.
        %\TagExpt{}
        The following list gives a definition for each type of failure case and an example.
        The first three types of failure cases denote type $2$ errors (false negatives) where the \groundtrue{}
          match fails to produce a high score.
        The last three types of failure cases denote type $1$ errors (false positive) where the \groundfalse{}
          match produces a score that is too high.

        \begin{itemln}

            \item Viewpoint:
            This failure case denotes that there is a viewpoint difference between the query and its
              \groundtrue{} match.
            Viewpoint differences are among the most common causes of failure in all of the datasets.
            A viewpoint failure case is caused by an out-of-plane rotation between the query features and the
              \groundtrue{} database features.
            Out-of-plane rotations of a feature can cause significant difference in appearance and exacerbates
              errors in feature localization, causing inconsistency between feature descriptions.
            The differences in descriptors leaves the approximate nearest neighbor algorithm unable to establish
              the correct correspondence, which ultimately causes identification failure.

            \FailViewpoint{}

            Note that these viewpoint failures cases originate from a test that controls for viewpoint.
            We have found that about half of these cases are due to viewpoint mislabelings.
            The pairs in the other half have correct viewpoint labels, yet a mild viewpoint difference still
              causes the initial assignment of feature correspondences to fail.
            An example illustrating a failure case due to viewpoint is shown in~\cref{fig:FailViewpoint}.

            \item Occlusion:
            This label denotes that either the query or the \groundtrue{} annotation is occluded.
            Objects like grass, tree branches, and other animals can obscure a feature causing it to appear
              dissimilar or mask it entirely.
            This is a much more significant problem for species with relatively few distinctive features like
              plains zebras.
            An example illustrating a failure case due to occlusion is shown in~\cref{fig:FailOcclusion}

                \FailOcclusion{}

            \item Quality:
            This label denotes that either the query or the \groundtrue{} annotation is blurred or distorted.
            Some of these cases are due to an annotation having an ``ok'' quality label, when in fact it should
              be labeled ``poor'' or ``junk''.
            An annotation with low quality may generate fewer, larger, less distinct, and distorted features.
            This case occurs due to mislabeling of the image quality.
            An example illustrating a failure case due to quality is shown in~\cref{fig:FailQuality}.

                \FailQuality{}

            %\item Lighting:
            %This label denotes that either the query or the \groundtrue{} annotation is poorly illumination or
            %  shadowed.
            %Feature extraction produces too few and unreliable features in under exposed images.
            %Non-uniform illumination and shadowing produces noisy intensity gradients that interfere with feature
            %  description.
            %Ideally, a classifier could be trained to determine which images are poorly illuminated and select an
            %  appropriate preprocessing step.
            %An example illustrating a failure case due to lighting is shown in~\cref{fig:FailLighting}

            %    \FailLighting{}

            %\item Scenery Match:
            %This denotes a case where the algorithm produces correspondences between shared background features
            %  between a query annotation and \aan{\groundfalse{}} annotation.
            %These cases are typically between pairs of annotations with small (less than $10$ minutes) time
            %  deltas.
            %For plains and Grévy's zebras, these cases are mostly eliminated using a foreground weighting
            %  algorithm.
            %However, for new species --- when a foregroundness measure has not been trained --- this will still
            %  be a problem.
            %An example illustrating a failure case due to a scenery match is shown in~\cref{fig:FailScenery}

            %  \FailScenery{}

            \item \Photobomb{}:
            A case where the \groundtrue{} animal is seen in the foreground or background of \aan{\groundfalse{}}
              annotation.
            This case is not technically a false positive because the algorithm is correctly matching the same
              individual.
            However, this is a problem for identification because failure to detect a \photobomb{} will cause two
              different individuals to be incorrectly marked as the same \name{}.
            This could potentially start a cascading ``snowball'' effect.
            An example illustrating a failure case due to \photobombing{} is shown in~\cref{fig:FailPhotobomb}

              \FailPhotobomb{}
        \end{itemln}


        The above failure cases show that the main causes of algorithm failure are due to viewpoint, occlusion,
          and quality.
        This identification algorithm itself does not seek to correctly identify low quality annotations, however
          the larger system should be able to flag and either fix (\eg{} by applying histogram equalization on a
          case-by-case basis to a poorly illuminated annotations) or remove such annotations.
        We approach the problem of viewpoint as a data issue.
        By adding more exemplars to the database we expect to improve matching accuracy between annotations with
          small viewpoint differences.
        %Scenery matches and \photobombings{} also cause false positives, but the foregroundness weighting mostly
        %  eliminates scenery matches.
        To account for \photobombings{}, a classifier could be trained based on the timestamps between
          annotations, and the spatial distribution of feature correspondences within the annotations.
        This would also help to further reduce failures due to scenery matches.
        
    %\subsection{Score separability}\label{sub:exptsep}  
    %    In this subsection we investigate identification accuracy in terms of the scores returned along with each
    %      \name{} in the ranked list.
    %    This is in contrast to the results presented in previous experiments where accuracy is evaluated only in
    %      terms of ranking.
    %    It is important to look at the overall scores of the algorithm because in a deployment setting a query
    %      annotation may not have corresponding \groundtrue{} database annotation and the top ranked \name{} would
    %      always be incorrect.
    %    Ideally, the scores would be used to decide if each name in the ranked list is either \groundtrue{} or
    %      \groundfalse{}.
    %    However, if the scores are to be used in a decision mechanism, they must have a high degree of
    %      separability.

    %    We run experiments to test the degree to which \groundtrue{} and \groundfalse{} scores can be separated
    %      by a binary classifier.
    %    We consider two types of scores:
    %    (1) \groundtrue{} scores --- the scores of between the query and its \groundtrue{} name (even if it is
    %      not ranked first), and
    %    (2) \groundfalse{} scores --- the scores between the query and the highest ranked \groundfalse{} name
    %      (the \groundfalse{} name is ranked $1$\st{} for failure cases and $2$\nd{} for success cases).
    %    The experiments in this subsection are run using the \timectrl{} annotation configuration and the best
    %      pipeline configuration for each species.
           
    %    Results are reported in the form of two plots:
    %    The first shows a histogram of the scores.
    %    The second plot shows an ROC curve where the true positive rate (sensitivity / recall) is plotted as a
    %      function of the false positive rate (fall-out).
    %    The area under the curve (AUC) is reported above the graph.
    %    The AUC is a standard measure used to evaluate a binary classifier.
    %    It represents the probability that a random \groundtrue{} name receives a higher score than a random
    %      \groundfalse{} name.
    %    The results of the separability experiment are shown in~\cref{fig:ScoreSep}.

    %    %The results of the separability experiment for plains zebras are shown
    %    %  in~\cref{fig:PZScoreAll}; Grévy's zebras are shown in
    %    % ~\cref{fig:GZScoreAll}; and Masai giraffes are shown in
    %    % ~\cref{fig:GIRMScoreAll}.

    %    \ScoreSep{}

    %    The results show that a threshold could be set to automatically accept high scoring names, if a small
    %      amount of false positives are acceptable.
    %    However, there is still a significant intersection between the \groundtrue{} and \groundfalse{} cases for
    %      plains and Grévy's zebras.
    %    For Masai giraffes the intersection is smaller, but there is also less data available.
    %    Ideally, we would like to find a threshold at which no false positives are accepted.

    %    In all datasets there does not exist any threshold able to automatically reject a true negative without
    %      causing false negatives (this is because some correct matches receive scores of zero).
    %    There are thresholds that can be set to automatically accept true positives without causing any false
    %      negatives.
    %    Unfortunately, the percentage of automatically accepted true positives is low.
    %    For plains zebras, a threshold of $6.1$ automatically accepts $\frac{3}{475} = 0.6\percent$ \groundtrue{}
    %      matches.
    %    For Grévy's zebras, a threshold of $2.55$ automatically accepts $\frac{15}{300} = 5.0\percent$
    %      \groundtrue{} matches.
    %    For Masai giraffes, a threshold of $3.1$ automatically accepts $\frac{15}{35} = 42.8\percent$
    %      \groundtrue{} matches.

    %    After manually labeling the failure cases we have found that the reason for this low acceptance rate is
    %      due to \photobombings{}.
    %    If failure cases due to \photobombings{} are ignored, there is significant improvement.
    %    For plains zebras, a threshold of $.86$ automatically accepts $\frac{180}{463} = 38.8\percent$
    %      \groundtrue{} matches.
    %    For Grévy's zebras, a threshold of $2.55$ automatically accepts $\frac{35}{299} = 11.7\percent$
    %      \groundtrue{} matches.
    %    For Masai giraffes, a threshold of $0.7$ automatically accepts $\frac{26}{35} = 76.4\percent$
    %      \groundtrue{} matches.
    %    \Photobombing{} cases tend to produce the highest false positive matching score because they are
    %      technically correct matches --- from a feature perspective --- and are scored appropriately.
    %    Furthermore, \photobombings{} tend to occur be between images with small time-deltas which increases
    %      visual similarity and increases the scores of the feature matches.
    %    Building a classifier to detect \photobomb{} cases would be a simple way to significantly reduce the
    %      amount of manual verification needed.

    %    We must note an important caveat to developing a decision mechanism based on the LNBNN identification
    %      scores.
    %    The scores computed by the single image identification algorithm depend on all of the database
    %      annotations we match against.
    %    As the images in the database change, normalizing features used to compute the LNBNN scores may change as
    %      well.
    %    Furthermore, as the database size grows it is likely that fewer matches will be discovered.
    %    Therefore, any threshold set on these scores is only valid in the context of a specific static database.
    %    The problem of developing a dynamic decision mechanism is addressed in the next chapter.

    %\subsubsection{Why is individual animal identification hard?}\label{sub:whyhard}
    %    %Setup paragraph.
    %    Even when ignoring \photobombings{}, the amount missed true positives and the number of manual
    %      verifications necessary is still unsatisfactory.
    %    It seems that there is an underlying difficulty in generating the initial feature matches in many cases.
    %    To illustrate this difficulty, consider the Liberty Buildings dataset~\cite{brown_discriminative_2011}
    %      commonly used in descriptor learning.
    %    This dataset contains a large number of corresponding patches from architectural structures computed
    %      using stereo matching.
    %    SIFT descriptors are computed for each patch, and the L2-distance between \groundtrue{} and a set of
    %      \groundfalse{} patch descriptors is computed.
    %    Pairs of \groundtrue{} and \groundfalse{} descriptors can be similarly computed for an animal dataset as
    %      the set of spatially verified foreground features from correct individuals and the set of feature matches
    %      between incorrect individuals.
    %    Examples of patches from both the Liberty dataset and the plains dataset are shown
    %      in~\cref{fig:PzVsLibertyPatches}.
    %    \Cref{fig:PzVsLiberty} compares the separability \groundtrue{} and \groundfalse{} \emph{patches} based on
    %      the L2-distance between SIFT descriptors from the Liberty dataset and the plains zebra dataset.

    %    There is a high degree of separability between the patches from the Liberty dataset (an ROC AUC of
    %      $0.96$) and a low degree of separability between patches from the plains zebra dataset (an ROC AUC of
    %      $0.724$).
    %    Consider the histogram of plains zebra scores in~\cref{fig:PzVsLiberty}.
    %    The incorrect matches also appear to be much closer in the plains dataset when compared to the buildings
    %      dataset.
    %    This is likely because are the most difficult incorrect matches in the dataset and are likely to be
    %      correspondences between non-distinctive descriptors.
    %    The histogram of correct scores appears to be bimodal.
    %    This is evidence of two things:
    %    (1) there are significantly more non-distinctive correct matches than distinctive ones, and
    %    (2) there are matches incorrectly marked as correct.

    %    Even if the modes of the correct match histogram were separated, there are significantly fewer ``good''
    %      correct matches for zebras than there are for buildings.
    %    In~\cref{subsec:dcnndiscuss} we noted a failed attempt to learn new convolutional descriptors to replace
    %      SIFT{}.
    %    This patch separability experiment shows the reason for the failure.
    %    The problem of learning descriptors to match animals seems to be more difficult than learning descriptors
    %      to match buildings.

    %    \PzVsLibertyPatches{}

    %    \PzVsLiberty{}


    \subsection{Experimental conclusions}\label{sub:exptsum}  

        In this section we have evaluated our baseline algorithm under restrictive conditions to control for the
          effects of time, quality, and viewpoint.
        Based on the results of these experiments we are able to make the following observations and conclusions.
        \begin{itemln}
            %% DATASET SAMPLING MATTERS
            %\item The baseline experiment shows that it is important to
            %  control for time, because near-duplicate images 
            %  the influence of near-duplicate images on matching
            %  accuracy.

            \item \textbf{Identification accuracy improves with more exemplars}:
            % NUM EXEMPLARS MATTERS
            The name scoring experiment and the $\K$ experiment show that the number of \exemplars{} per database
              \name{} is the most significant factor that impacts identification accuracy.

            \item \textbf{Foregroundness weighting reduce scenery matches}:
            % FOREGROUNDNESS GOOD
            Identification accuracy significantly improves by $2-4$ percentage points when using foregroundness
              weighting.
            We have found that enabling foregroundness weighting eliminates nearly all failure cases due to
              scenery matches without significantly affecting other results.

            \item \textbf{Viewpoint and occlusion are the most difficult imaging challenges}:
            % VIEWPOINT HARD
            The viewpoint experiment and the failure cases show that there is a significant loss in accuracy when
              matching annotations from different viewpoints.
            Viewpoint seems to be the most difficult challenge across all species, however the failure cases show
              that occlusion is a more significant issue for plains zebras.
            This may be because Grévy's zebras have distinctive patterns in many places and
              thus can be matched when only part of the animal is visible.

            \item \textbf{Invariance settings are data dependent}:
                The invariance experiment shows that
                % AFFINE GOOD FOR GZ CIRCLE GOOD FOR PZ
                affine invariance produces better results for Grévy's zebras, whereas circular
                  keypoints lead to more accurate results for plains zebras.
                % AQH GOOD FOR PZ
                This experiment also demonstrated that the query-side rotation heuristic improves accuracy by
                  adding a small amount of orientation invariance to feature localization, while using full
                  orientation invariance causes a drop in accuracy.
                %This means that it may not be feasible to use more than a
                %  single canonical viewpoint in the final population
                %estimation.

              \item \textbf{The choice of \K{} has a minor impact}: 
                The $\K$ experiment shows that identification accuracy is not significantly influenced by the
                  choice of $\K$ for plains zebras, but for Grévy's zebras the most accurate results were obtained
                  with $\K\tighteq1$.
                This is likely because the features from plains zebras are less distinguishing than features from
                  Grévy's zebras.
                Therefore, the correct match of a plains zebra feature is less likely to be its closest neighbor.
                Furthermore, the size of the database does not seem to strongly influence the optimal choice of
                  $\K$.
                However, note that most tests were run on different sizes of large databases, and the choice of
                  $\K$ using small databases was not investigated.

              \item \textbf{\Nsumprefix{} is slightly better than \csumprefix{} \namescoring{}}:
                % NSUM > CSUM
                The scoring mechanism experiment shows that the \nsumprefix{} scoring mechanism is slightly more
                  accurate than the \csumprefix{} scoring mechanism.
                We hypothesize that this effect may be more significant in conditions with more viewpoint
                  variation.

              \item \textbf{LNBNN scores are not enough for automated decision}:
                % Separability
                The separability experiment shows that is a reasonable separation between the scores of
                  annotations seen from the same viewpoint.
                Furthermore, if the effect of \photobombings{} can be reduced it becomes possible to set an
                  threshold to automatically accept high scoring identification results.
                However, there are still a significant number of correct results with non-separable scores.
                Furthermore, it is still unclear to what degree the scores depend on the database size.
                For these reasons we conclude that the decision mechanism should be independent of the single
                  image identification algorithm.
                %More experimentation is needed before threshold levels can
                %  be confidently set for databases of arbitrary sizes.
        \end{itemln}
