
\section{Experiments}\label{sec:experiments}

    This section presents an experimental evaluation of the identification algorithm using annotated images of
      plains zebras, Grévy's zebras, Masai giraffes, and humpback whales.
    The input to each experiment is
    (1) a dataset,
    (2) a subset of query and database annotations from the database,
    (3) a pipeline configuration.
    The datasets are described in~\cref{sub:datasets}.
    The subsets of query and database annotations are carefully chosen to measure the accuracy of the algorithm
      under different conditions and to control for time, quality, and viewpoint.
    The pipeline configuration is a set of parameters --- \eg{} the level of feature invariance, the number of
      nearest neighbors, and the \namescoring{} mechanism --- given to the identification algorithm.
    We will vary these pipeline parameters in order to measure their effect on the accuracy of the ranking
      algorithm.

    For each query annotation, the identification algorithm returns a ranked list of \names{} with a score for
      each name.
    The accuracy of identification is measured using the cumulative match
      characteristic~\cite{decann_relating_2013} which can be understood as the probability that a query correctly
      finds a match at a specified rank under the assumption that a correct match exists in the database.
    We are primarily returned with only the first point in this curve --- the fraction of queries with a correct
      result at rank $1$ --- because often a user of the system will only review the first result of a query.

    Additionally, we like to automatically review results returned by the ranking algorithm, so we will measure
      the separability of correct and incorrect results using the scores returned by the ranking algorithm.
    The separability of scores is measured by first recording the score of the correct \name{} and the
      highest scoring incorrect \name{} for each query and then plotting the receiver operator characteristic
      (ROC) curve and measuring the area under the curve (AUC).
    %Other measures that are reported include the percentage of true positive
    %  and false positive results that fall within certain categories (\eg
    %  temporal windows).

    The outline of this section is as follows.
    First, \cref{sub:datasets} introduces and describes each dataset.
    Our first experiment in \cref{sub:exptbase} establishes the accuracy of our ranking algorithm on several
      datasets using a default pipeline configuration.
    We then compare our approach to an alternative \cref{sub:exptsmk} SMK approach.
    The next subsections perform in depth experiments on the parameter settings of our algorithm.
    \Cref{sub:exptfg} tests the effect of the foregroundness weight on identification accuracy.
    \Cref{sub:exptinvar} investigates the effect of the level of feature invariance and viewpoint.
    \Cref{sub:exptscoremech} compares the \csumprefix{} and the \nsumprefix{} \namescoring{} mechanism.
    \Cref{sub:exptk} varies the $\K$ parameter (the number of nearest neighbors used in establishing feature
      correspondences) and investigates the relationship between $\K$ and database size in terms of both the number
      of annotations and the number of exemplars per name.
    \Cref{sub:exptfail} discusses the failure cases of our ranking algorithm.
    \Cref{sub:exptsep} presents an evaluation of the score separability for the pipeline configuration with the
      highest accuracy determined for each species.
    Finally,~\cref{sub:exptsum} summarizes this section.


    \subsection{Datasets}\label{sub:datasets}

        All of the images in the datasets used in these experiments were taken by photographers in the field.
        Each dataset is labeled with groundtruth in the form of annotations with name labels.
        Annotations (bounding boxes) have been drawn to localize animals within the image.
        A unique \name{} label has been assigned to all annotations with the same identity.
        Some of this groundtruth labeling was generated independently.
        However, large portions of the datasets were labeled with assistance from the matching algorithm.
        While this may introduce some bias in the results, there was no alternative because the amount of time
          needed to independently label a large dataset is prohibitive.

        There are two important things to note before we describe each dataset.
        First, in order to control for challenging factors in the images such as quality and viewpoint some
          experiments sample subsets of the datasets we describe here.
        Second, note that there do exist labeling errors in some datasets.

        \DatabaseInfo{}

        \timedist{}

        The number of names, annotations, and their distribution within each database are summarized in the
          following tables.
        In these tables we distinguish between \glossterm{singleton} and \glossterm{resighted} names.
        Singleton names are individuals sighted only once, \ie{} contain only a single encounter.
        Resighted names contain more than one encounter.
        We make this distinction because resighted names have known correct matches across a significant time
          delta.
        Note, that singleton names may still have more than one annotation, but those annotations are all from
          the same encounter.
        We have pre-filtered each database to remove annotations that are unidentified, are missing timestamps,
          or are labeled as ``junk'' quality.

        \Cref{tbl:DatabaseStatistics} summarizes the number of annotations and individuals in each database as
          well as the number of times (distinct encounters) each individual was sighted.
        \Cref{tbl:AnnotationsPerQuality} summarizes the quality labels of the annotations.
        \Cref{tbl:AnnotationsPerViewpoint} summarizes the viewpoint labels of the annotations.
        Distributions of when images in each dataset were taken are illustrated in \cref{fig:timedist}.
        The name and a short description of each dataset is given in the following list.

        \begin{itemln}
            \item \textbf{Plains zebras}.
            Our plains zebras dataset is an aggregate of several smaller datasets.
            There is variation in how the data was collected and preprocessed.
            Some of the images are cropped to the flank of the animal, while others are cropped to encompass the
              entire body.
            The constituent datasets were collected in Kenya at several locations including Nairobi National
              Park, Sweetwaters, and Ol Pejeta.
            More than $90\percent$ of the groundtruth generated for this dataset was assisted using the matching
              algorithm.
            This dataset contains many imaging challenges including occlusion, viewpoint, pose, quality, and time
              variation.
            There are some annotations in this dataset without quality or viewpoint labelings and some images
              contain undetected animals.
            This data was collected between 2006 and 2015, but the majority of the data was collected in
              2012--2015.

            \item \textbf{Grévy's zebras}.
            This is another aggregated dataset.
            The original groundtruth for this dataset was generated independently of the matching algorithm,
              however the matching algorithm revealed several groundtruth errors that have since been corrected.
            The Grévy's dataset was collected in Mpala, Kenya.
            Most of the annotations in this database have been cropped to the animal's flank.
            This dataset contains a moderate degree of pose and viewpoint variation as well as occlusion.
            This data was collected between 2003 and 2012, but the majority was collected in 2011 and 2012.

            \item \textbf{Masai Giraffes}.
            These images of Masai Giraffes were all taken in Nairobi National Park during the \GZC{} between
              February 20, 2015 and March 2, 2015.
            All groundtruth was established using the matching algorithm followed by manual verification.
            This dataset contains a high degree of pose and viewpoint variation, as well as occlusion.
            Because of their long necks, it is difficult to ensure that only a single giraffe appears in each
              annotation.
            This results in many \glossterm{photobombs} --- pairs of annotations where a background animal in one
              annotation matches the foreground animal in the other --- when matching.

            \item \textbf{Humpback Whales}.
            The humpback dataset was collected by FlukeBook over nearly 15 years.
            Images were contributed by both marine and citizen scientists.
            The original groundtruth was established using both manual and automated methods that are disjoint
              from these techniques considered here, however our software was used to correct mistakes.
            The annotations in this dataset have not been manually reviewed.
            Some are cropped to the fluke while others encompas the entire image.
            Quality and viewpoint labels do not exist for this dataset.
        \end{itemln}

    \subsection{Baseline experiment}\label{sub:exptbase}
      
        \BaselineExpt{}

        This first experiment determines the accuracy of the identification algorithm using the baseline pipeline
          configuration.
        The baseline pipeline configuration uses affine invariant features oriented using the gravity vector,
          $\K\tighteq4$ as the number of feature correspondences assigned to each query feature, and \nscoring{}
          (\nsum{}).
        In this test we control for several biases that may be introduced by carefully selecting a subset of our
          datasets.
        We only use annotations that
        (1) are known (\ie{} have been assigned a name),
        (2) are comparable the species primary viewpoint (\eg{} left, front-left, and back-left for plains
          zebras),
        (3) have not been assigned a quality of ``junk''.
        Furthermore, to account for the fact that some \names{} contain more annotations than others, we
          constrain our data selection such that there is only one correct exemplar in the database for each query
          annotation.

        Of these annotations, we group them into encounters.
        For each encounter we sample one annotation with the highest quality.
        Names with only one encounter are added to the database as distractors.
        For the other names, we randomly sample two encounters --- regardless of quality --- one for the database
          and one to use as a query.
        This defines a set of query and database annotations that are separated by time, testing the ability of
          our system to match animals across gaps of time using only a single image per individual.
        The CMC curves for this baseline test are illustrated in~\cref{fig:BaselineExpt}.

        The results of this baseline experiment demonstrates that our algorithm is able to reliably find matching
          annotations in a database with many other images.
        The accuracy is over $60\percent$ for all species considered.
        Subsequent experiments will restrict our focus to Grévy's plains zebras in order to investigate detailed
          parameter choices of the ranking algorithm as well as alternative ranking algorithms.

    \subsection{SMK as an alternative}\label{sub:exptsmk}  

        Before we investigate the parameter choices of the LNBNN ranking algorithm, we briefly evaluate the
          performance of an alternative ranking algorithm, namely VLAD-flavored SMK.
        The SMK algorithm is a vocabulary based algorithm, that is representative of more traditional approaches
          to instance recognition problems.
        In contrast to the raw descriptors used in LNBNN, SMK assigns each descriptor to a visual word and builds
          a weighted histogram of visual words (accumulated residual vectors in the case of VLAD) to represent each
          annotation as a sparse fixed length vector.
        We have experimented with several configurations of VLAD and report our best results here.

        \SMKExpt{}

        In our SMK implementation, we pre-trained an $8000$ word vocabulary using mini-batch k-means on the
          stacked descriptors from all database annotations.
        Note that typically the vocabulary is trained using a disjoint external dataset in order to prevent
          overfitting.
        However, we \naively{} train using the database annotations to be indexed, understanding that this will
          inflate the accuracy measurements.
        Each word in the vocabulary is weighted with its inverse document frequency.
        We use the vocabulary to compute an inverted index that maps each visual word to annotations containing
          that word in the database.
        Initial feature correspondences for a descriptor are computed using single assignment to a visual word
          and then creating a correspondence to every feature in that word's inverted index.
        We use spatial verification to filter spatially invalid correspondences, and re-score the remaining
          matches.

        The results of the SMK experiment are illustrated in \cref{fig:SMKExpt}.
        The query and database annotations are the same in each experiment.
        Despite the bias in the SMK vocabulary, our measurements show that LNBNN provides the most accurate
          rankings.
        For plains zebra's there is a difference of $8\percent$ in the number of correct matches at rank $1$, and
          for Grévy's zebras the difference is $6\percent$.


    \subsection{Foregroundness experiment}\label{sub:exptfg}

        %\ForegroundExpt{}
        \FGIntraExpt{}

        In this experiment we test the effect of our foregroundness weights --- weighting the score of each
          features correspondence with a foregroundness weight --- on identification accuracy.
        When foregroundness is enabled (\pvar{fg=T}), each feature correspondence is weighted using a
          foregroundness measure learned using a deep convolutional neural network~\cite{parham_photographic_2015}.
        When disabled (\pvar{fg=F}), the weight of every correspondence effectively becomes $1$.

        Running this experiment with using the query / database sample as outlined in the baseline experiment
          does not result in a noticeable difference in scores because the purpose of the foregroundness measure is
          to down weight matches between scenery objects (\eg{} trees, grass, bushes) that appear in multiple
          annotations.
        The baseline database sample contains only a single images from each encounter and only two encounters
          per individual.
        This means that it will be unlikely for an annotation in the query set and another annotation in the
          database set to have a similar background.

        To more clearly illustrate the effect of the foregroundness measure we use a different sampling strategy.
        We group all encounters by which occurrence they belong to.
        Annotations within the same occurrence are more likely to share background.
        We sample query and database annotations from within occurrences to simulate matching annotations within
          an encounter.
        We do not limit the number of exemplars in this test to ensure that annotation pairs that share common
          scenery exist.
        We perform this test over multiple occurrences and aggregate the results.
        Therefore, the reported database size will be an average, and the query size is the sum of all unique
          query annotations.
        %We sort the occurrences by size in descending order, and then iterate through each occurrence.
        %We sample the highest quality annotation in encounter in the occurrence as long as that name has not been
        %  sampled more than two times.
        %At the end of this process we have at most two annotations for each name.
        %We randomly choose annotations to be query and database annotations from each name with two annotations.
        %The rest are used as confusers.
        %We execute the ranking algorithm twice with foregroundness both enabled and disabled.
        %The rest of the pipeline configuration is the same as the baseline test.

        The accuracy of the foregroundness is illustrated in~\cref{fig:FGIntraExpt}.
        The results show that using foregroundness weights improve the number of correct results at rank $1$ by a
          significant margin for both species.
        In the higher ranks using using the \pvar{fg=T} line occasionally dips below the \pvar{fg=F} line because
          sometimes the foregroundness mask covers distinguishing keypoints, but this is neither significant nor
          common.
        Therefore we find it beneficial to always include foregroundness when a trained estimator is available.

        %foreground weights occasionally dips below Occasionally the accuracy of using foreground weights Occasionaly toNote that towards higher ranks 
        %N
        %For plains zebras, using the foregroundness measure results in a significant $3.79\percent$ increase in
        %  identification accuracy.
        %For Grévy's zebras there is also a significant $3.3\percent$ increase.
        %This experiment clearly shows the importance of eliminating background feature correspondences.
     
    \subsection{Invariance experiment}\label{sub:exptinvar} %
        In this experiment we vary the feature invariance configuration.
        This influences the location, shape, and orientation of keypoints detected in each annotation, which in
          turn influences which regions in each annotation are matchable using SIFT descriptors extracted at each
          keypoint.
        The best invariance settings will be depend on properties of the data.

        In our experiments we test different settings by enabling (denoted as T) or disabling (denoted as F) the
          parameters affine invariance (AI), and our query-side rotation heuristic (QRH).
        Initially we also tested rotation invariance, but found that it provided the poorest results for all
          datasets by a significant margin, likely because the gravity vector assumption is mostly satisfied in all
          images.
        Therefore, we exclude rotation invariance from our experiments.

        In configurations where \pvar{AI=F}, keypoints are circular with a radius defined by the scale at which
          it was detected.
        When \pvar{AI=F}, the keypoint shape is adapted into an ellipse to normalize for small viewpoint changes.
        When \pvar{QRH=F}, each keypoint is assigned its orientation as normal, but when \pvar{QRH=T}, each
          keypoint in a query annotation is replaced by three keypoints, one rotated slightly to the left, another
          slightly to the right, and the last is the original keypoint.
        The four specific configuration that we test are outlined in the following list:

        \begin{itemln}

            \item \NoInvar{} (\pvar{AI=F,QRH=F}): % 
                This configuration uses circular keypoints and assumes the gravity vector.

            \item \AIAlone{} (\pvar{AI=T,QRH=F}): % 
                This is the baseline setting that assumes the gravity vector and where each feature's shape is skewed
                  from a circle into an ellipse.

            \item \QRHCirc{} (\pvar{AI=F,QRH=T}): %
                This is a novel invariance heuristic where each {database} feature assumes the gravity vector, but
                  {query} feature is $3$ orientations:
                the gravity vector and two other orientations at $\pm15\degrees$ from the gravity vector.
                Ideally, this will allow feature correspondences to be established between features seen from
                  slightly different orientations.

            \item \QRHEll{} (\pvar{AI=T,QRH=T}): %
                This is the combination of \QRHCirc{} and \AIAlone{}.

        \end{itemln}

        \InvarExpt{}

        % Invar Conclusions
        The example in~\cref{fig:kptstype} illustrates the difference between \AIAlone{} and \QRHCirc{} features
          for plains and Grévy's zebras.
        The accuracy of the invariance experiment is shown in~\cref{fig:InvarExpt}.
        For plains zebras, the \QRHCirc{} scores are significantly better than all other invariance settings.
        Interesting, affine invariance results in worse performance when QRH is on, but if the QRH is off then
          affine invariance improves accuracy.
        This suggests that the QRH better handles matching the coarse patterns seen on the plains zebras across
          pose and viewpoint variations than using affine invariance, which can tend to adapt itself around
          non-distinctive diagonal stripes.
        Even though affine keypoints provide more precise localization, the area they describe is often smaller
          than a circular keypoint.
        It makes sense that affine keypoints would not describe coarse features as well as a circular keypoint
          covering a larger area.
        %
        The results for Grévy's zebras demonstrate similar levels of accuracy for \AIAlone{} and \QRHEll{}.
        Affine invariance seems to be the most important setting for matching Grévy's zebras.
        The distinctive details on Grévy's zebras are finer then plains zebras and are well captured by affine
          keypoints.
        While the QRH does improve accuracy for Grévy's zebras the density of the distinctive keypoints means
          that it is less important because it is more likely that a two annotations will have at least one
          distinctive region aligned an in common.

        \kptstype{}

    \subsection{Scoring mechanism experiment}\label{sub:exptscoremech}  

        % TODO: change experiment so only one annotation per name is chosen for
        % each confuser

        % Database setup for name scoring
        The purpose of the scoring mechanism is to aggregate scores of individual feature correspondences across
          multiple annotations into a single score for each name --- \ie{} an annotation-to-name similarity, which
          is analogous to the image-to-class distance used in~\cite{boiman_defense_2008}.
        We test the identification accuracy of the two name scoring mechanisms that were described earlier
          in~\cref{subsec:namescore}:
        (1) \cscoring{} (denoted as \csum{}) and
        (2) \nscoring{} (denoted as \nsum{}).

        \NScoreExpt{}

        Because the scoring mechanism is meant to take advantage of multiple database annotations, we vary the
          number of \exemplars{} per database \name{} (\pvar{dpername}) between $1$, $2$, and $3$.
        Varying the number of \exemplars{} will cause each database to contain a different number of annotations.
        To normalize difference in database size we include additional confuser annotations (annotations that do
          not match any query) in the smaller databases to maintain a constant database size across experiments.
        Each \exemplar{} is chosen from a separate encounter.

        The accuracy of the scoring mechanism experiment is shown in~\cref{fig:NScoreExpt}.
        The results of this test does suggest that \nsum{} results in slightly more accurate ranking, but the
          overall difference in accuracy is relatively small (about $1-3\percent$).
        Intuitively, the \nsum{} scoring should produce better ranks because it can combine scores from multiple
          correspondences to different correct annotations.
        Note that when $\pvar{dpername}=1$, the \csum{} and \nsum{} scores might still be different because
          \nsum{} multiple correspondences to a name which may be generated when $\K > 1$ or when \pvar{QRH=T}.

        Perhaps the more interesting result of this experiment is the effect of increasing the number of
          exemplars in the database from $1$ to $2$.
        There is a drastic improvement in ranking accuracies in both species.
        The accuracy of plains zebras increases by $10\percent$ and for Grévy's zebras the gain is almost
          $20\percent$.
        It makes sense that this should be the case.
        If there are more examples of an individual in the database then the probability that the query is
          similar to at least one of them should increase as long as there is sufficient variation.
        This suggests that even if a new query new annotation initially fails to rank the correct result,
          subsequent annotations added to the system of the same individual will be more likely to correctly match
          a previous annotation.
        As more annotations of that individual are added the likelihood that the ranking algorithm will make a
          connection between all instances of that individual will increase.


    \subsection{K experiment}\label{sub:exptk}  

        % Introduce varied parameters
        In this experiment we investigate the effect of $\K$ (the number of nearest neighbors used in
          establishing feature correspondences, which was discussed in~\cref{sub:featmatch}) on identification
          accuracy.
        We vary $\K$ between the values $1, 2, 4$, and $6$.
        In all of these experiments we set the number of normalizing neighbors to be $\Knorm=1$.

        \KExptA{}
        \KExptB{}

        Two database factors that may influence the best choice of $\K$ are the number of annotations in the
          database and the number of annotation per name in the database.
        If there are more correct matches for a query annotation it would be beneficial to allow it to match more
          annotations.
        Likewise, if there are more overall annotation in the database, then it might be beneficial to search
          deeper into all of the database descriptors to find the correct matches.
        Therefore, in addition to varying $\K$ we also vary the number exemplars per name (\pvar{dpername}) and
          the overall number of annotations in the database (\pvar{dsize})

        We use a protocol similar to the one used in the scoring mechanism experiment to sample databases.
        The difference is that we use the extra confusers annotations to vary the total number of annotations in
          the database.
        However, controlling for these factors constrains the number of annotations we can use.
        For Grévy's, we can vary the total database size between $476$ and $774$.
        For plains we have more confuser annotations allowing us to test database sizes of $578$ and $1650$.
        We vary the number of \exemplars{} per name between $1$ and $2$

        The results of this experiment are illustrated in ~\cref{fig:KExptA,fig:KExptB}.
        Similarly to the previous experiment, the number of exemplars per name is the most significant variable
          impacting accuracy.
        Furthermore, when there are more exemplars in the database the choice of $\K$ starts become less
          significant.
        The results also show that accuracy does slightly decrease when the database becomes larger, but
          magnitude of the decrease is between $1\percent$ and $3\percent$.
        Interesting the optimal choice of $\K$ is not consistent between species when there is only one exemplar
          per name.
        For Grévy's zebras using a lower $\K$ results in better results, but for plains zebras there is a
          signifiant loss when $\K=1$ and the database size is large.
        This is likely due to the nature of the distinguishing patterns on the different zebras.
        When matching the detailed patterns of the Grévy's zebras, it is better to use a low $\K$ to reduce
          noise, but for coarser plains zebras patterns a low $\K$ might not find a correct match immediately.
        Thus, the choice of $\K$ is a trade-off between precision and recall that depends on the type of texture
          patterns that are being matched.
        
        %Our second test varies the size of the database as well as the value of $\K$.
        %The effect of $\K$ and database size on matching accuracy is shown in~\cref{fig:DBSizeExpt}.
        %The results for all species show that the number of \exemplars{} per \name{} is the most important factor
        %  in this experiment.
        %Interestingly, the number of annotations in the database is only a minor factor in identification
        %  accuracy.
        %% plains
        %The results for plains zebras show a small positive relationship between the number of annotations in the
        %  database and $\K$.
        %This may be because many plains zebra features are not globally distinctive in a large database and a
        %  feature's correct correspondence may not be the nearest neighbor.
        %For smaller database sizes lower values of $\K$ produce more accurate results.
        %% Grévy's
        %For Grévy's zebras lower values of $\K$ seem better for all database sizes.
        %It also seems that the best values of $\K$ should be set to the number of \exemplars{} per \name{}, which
        %  is expected when there is little confusion between features.
        %% giraffes
        %For Masai giraffes, the amount of data again makes it difficult to draw conclusions.

        %\DBSizeExpt{}

        Overall the experiments on the setting of $\K$ does not yield definitive choice for this parameter.
        However, it appears that $\K$ only has a small influence on identification accuracy.
        This section does shows that the number of exemplars per annotation has a significant impact on
          identification accuracy.

    \subsection{Failure cases}\label{sub:exptfail}  
        
        We no investigate the causes of identification failure and consider example failure cases.
        When investigating the cause of a failure case we consider both
        (1) the matches between the query annotation and the incorrect \name{}  at rank $1$ and
        (2) the matches between the query annotation and the correct \name{} that appears further down the ranked
          list.
        We identify the main 3 failure cases as:
        (1) unaligned annotations,
        (2) quality factors, and
        (3) non-primary correspondences.
        The remainder of this subsection defines, discusses, and provides examples of these failure cases.

        %The first three types of failure cases denote type $2$ errors (false negatives) where the correct match
        %  fails to produce a high score.
        %The last three types of failure cases denote type $1$ errors (false positive) where the incorrect match
        %  produces a score that is too high.

        \subsubsection{Alignment}

            \FailViewpoint{}
            
            When two annotations are not aligned (ignoring translation and small scale differences), there can be
              significant differences in appearance that can cause inconsistency in feature localization and
              description.
            There are two major causes of alignment error:
            (1) viewpoint variations which cause out-of-plane rotations and
            (2) pose variations which can cause local non-rigid non-linear transformations of distinguishing
              features.
            These issues cause variations in feature description which renders the approximate nearest neighbor
              algorithm unable to establish the correct correspondence.
            Furthermore, non-projective transformations between annotations can cause homography-based spatial
              verification to discard correctly established correspondences.
            Failing to establish correspondences and incorrectly discarding them ultimately results in
              identification failure.

            The example in~\cref{fig:FailViewpoint} illustrates a failure case due to a difference in viewpoint
              and pose.
            To address matching across different viewpoints and poses it helps to choose an appropriate level of
              feature invariance (like affine invariance and the query-side rotation heuristic), but these only
              work up to a point.
            However, in practice the animal identification problem is not a one-shot identification challenge.
            Given multiple annotations of an individual we expect that the matching algorithm will be able to
              overcome viewpoint and pose differences by matching annotations with intermediate positions.

        \subsubsection{Quality factors}
            Factors such as low resolution, blurring, poor exposure, lighting (shadows / non-uniform
              illumination), and occlusion can significantly reduce the density of distinctive features on an
              annotation.
            Note that lighting and occlusion (scene quality factors) should be distinguished from the other
              factors (capture quality factors) because they are related to the scene itself rather than a poor
              capturing of that scene.
            Annotations with low capture quality tend to generate fewer, larger, less distinct, and distorted
              features.
            Annotations with low scene quality tend to have their distinguishing features distorted or masked by
              grass, tree branches, shadows, and other animals.
            This is a significant problem for species with relatively few distinctive features like plains
              zebras.
            The example in~\cref{fig:FailOcclusion} illustrates a failure case due to occlusion, and
              \cref{fig:FailQuality} illustrates failure case due to low resolution.

            \FailOcclusion{}
            \FailQuality{}

            In some cases low quality annotations can be still be matched, but in the worst case all distinctive
              features are missing and there is no way to visually identify the individual.
            Therefore, the best way to handle these annotations is either to ignore them entirely, or to first
              attempt to match them, but then discard them if they cannot be matched.

          \subsubsection{Non-primary correspondences}
            Sometimes an annotation bounding box cannot be placed tightly around an animal (this happens often
              for some species like giraffes), which means that other objects will appear in the background.
            Similarly, objects that occlude the animal will be in the foreground.
            Ideally, the primary animal in each annotation would be segmented, but when simply matching raw
              annotations non-primary correspondences may be formed.
            This results in the photobomb and scenery-match failure cases.

            \FailScenery{}
            \FailPhotobomb{}

            Photobombs are caused by correct correspondences a non-primary animal (seen either in the foreground
              or background) in an annotation.
            Likewise, scenery matches are caused by matches in the background landscape.
            Both cases are most commonly caused by pairs of annotations with the same occurrence, but photobombs
              can occur over larger time deltas.
            The example in~\cref{fig:FailPhotobomb} illustrates a failure case due to a photobombing background
              animal, and \cref{fig:FailScenery} illustrates a scenery match.
            For plains and Grévy's zebras, most scenery matches are be eliminated using the foregroundness
              measure, but the problem remains in databases without a trained foregroundness estimator.
            Accounting for photobombs is a more challenging problem because a simple patch based classifier
              cannot distinguish a primary feature from a  secondary feature without having information about the
              animal identity.
            However, there are some patterns that photobomb matches present that we seek to take advantage of
              later in \cref{sec:learnpb}.

    \subsection{Experimental conclusions}\label{sub:exptsum}  

        In this section we have evaluated our ranking algorithm on multiple species, compared it to an
          alternative ranking algorithm, and evaluated detailed parameter choices.
        Our experiments were performed under restrictive conditions to control for the effect of time, database
          size, and number of exemplars.
        Based on the results of these experiments we are able to make several observations and conclusions.

        Our experiments with comparing the SMK and LNBNN ranking algorithm demonstrated that LNBNN achieved
          better ranking accuracy.
        LNBNN does not quantize descriptor and therefore it is able to distinguish more subtle descriptor
          details.
        Because LNBNN does not require an expensive pre-training phase it makes it make it ideal to rank the
          databases on the scales considered in this thesis.
        However, we note that SMK is more efficient on larger scales, and it may be necessary to consider when
          databases become very large.

        %\item \textbf{Identification accuracy improves with more exemplars}:
        % NUM EXEMPLARS MATTERS
        In most experiments we evaluated our ranking algorithm as if it were addressing a single-shot
          identification problem.
        This was to establish the performance of the algorithm when an individual has only been seen once
          before.
        However, in practice this will not be the norm.
        The name scoring and $\K$ experiments demonstrated that the ranking accuracy significantly increases
          with the number of exemplars per database name.
        We will use this observation to address the challenges of matching through viewpoint and occlusion by
          taking advantage of multiple images of an individual in~\cref{chap:graphid}.

        %\item \textbf{Foregroundness weighting reduce scenery matches}:
        %% FOREGROUNDNESS GOOD
        %Identification accuracy significantly improves by a few percentage points when using foregroundness
        %  weighting.
        %We have found that enabling foregroundness weighting eliminates nearly all failure cases due to
        %  scenery matches without significantly affecting other results.

        %\item \textbf{Invariance settings are data dependent}:
        We also saw that the best choice for feature invariance is data dependant.
        The invariance experiment demonstrated that affine invariance produces better results for Grévy's zebras,
          whereas circular keypoints lead to more accurate results for plains zebras.
        This experiment also showed that the query-side rotation heuristic improves accuracy by adding a small
          amount of orientation invariance to feature localization.
        Likewise, the $\K$ experiment shows that identification accuracy is not significantly influenced by the
          choice of $\K$ for plains zebras, but for Grévy's zebras the most accurate results were obtained with
          $\K\tighteq1$.
        This is likely because the features from plains zebras are less distinguishing than features from Grévy's
          zebras, hence the correct match of a plains zebra feature is less likely to be its closest neighbor.
        Both the choice of $\K$ and invariance settings should be evaluated on a per-dataset basis and there is
          likely benefit to performing identification using multiple parameter choices.
        %Furthermore, the size of the database does not seem to strongly influence the optimal choice of $\K$.
        %However, note that most tests were run on different sizes of large databases, and the choice of $\K$
        %  using small databases was not investigated.
