\section{Image feature detection}\label{sec:featuredetect}

    Before an image can be analyzed, it must be broken down into
      smaller components.
    An image's visual appearance can be captured using a combination of
      local image patterns --- \glossterm{patch based features}.
    The most informative patch based features are typically centered on
      simple image structures such as junctions, corners, edges, and
      blobs~\cite{tuytelaars_local_2007}.
    If these features be can be reliably detected, localized, and
      described then image matching can be posed as a problem in matching
      sets of features.
    This section describes works related to detecting features in an
      image, and~\cref{sec:featuredescribe} will discuss how detected
      features are then described.

    The region where a feature is detected is called a
      \glossterm{keypoint}.
    The simplest definition of a keypoint is just an $xy$-location in
      an image.
    However, images contain information at multiple scales, therefore a
      keypoint is typically associated with a scale.
    The scale of a keypoint is a non-negative real number that defines
      the level of detail at which to interpret the underlying image
      information.
    A keypoint with a scale can be thought of as a circular region with
      a radius that is the scale multiplied by some constant (\eg{}
      $3\sqrt{3}$ is the constant used to determine a keypoint's radius
      in~\cite{perdoch_efficient_2009}).
    To account for changes in viewpoint and pose, it is also common to
      augment features with an orientation and shape.
    Adding these properties is said to add invariance to the feature.
    Invariant features can provide similar descriptions of the same
      semantic image region under different viewing conditions.
    However, adding invariance can causes features to lose
      distinguishing information~\cite{mikolajczyk_comparison_2005,
      tuytelaars_local_2007, perdoch_efficient_2009,
      lowe_distinctive_2004}.

    Many detectors have been developed to detect patch based feature
      keypoints~\cite{mikolajczyk_comparison_2005,
      tuytelaars_local_2007}.
    Algorithms such as Harris, SUSAN, and FAST detect
      corners~\cite{harris_combined_1988, mikolajczyk_indexing_2001,
      smith_susannew_1997, rosten_machine_2006}.
    Blobs and corners can be detected with
      Hessian~\cite{beaudet_rotationally_1978,
      lindeberg_shapeadapted_1994} or difference of
      Gaussians~\cite{gaussier_neural_1992, lowe_distinctive_2004}
      detectors.
    There are also region based detectors:
    maximally stable extremal regions~\cite{matas_robust_2004},
      saliency based methods~\cite{buoncompagni_saliencybased_2015} and
      superpixel based methods~\cite{ren_learning_2003,
      mori_recovering_2004}.
    Some applications choose to skip keypoint detection and use a
      uniform grid of dense features~\cite{liu_sift_2008,
      revaud_deep_2015, iscen_comparison_2015}.
    Other applications, such as face recognition, use specialized
      keypoint detectors~\cite{dantone_realtime_2012,
      berg_tomvspete_2012}.
    There currently exists no principled method for selecting the
      appropriate feature detector.
    Different feature detectors perform differently given the
      application~\cite{tuytelaars_local_2007}.

    This section describes the representation of an image over multiple
      scales, the detection of features to sub-pixel and sub-scale
      accuracy, and the adaption of features to specify orientation and
      shape.
    We focus on the Hessian-based keypoint because it has been
      experimentally shown to be a reliable choice for instance
      recognition~\cite{tuytelaars_local_2007}.

   \subsection{Scale-space}
        Scale-space theory describes image features as existing at
          multiple scales~\cite{lindeberg_scalespace_1993}.
        The same point on an object seen close up appears quite
          different compared to when it is at a distance.
        For example, a zebra in the distance may appear to have two
          stripes that are connected, but when the animal appears closer
          it becomes clear that the stripes are actually disconnected.
        %This problem is addressed by detecting features at multiple scales.
        Multi-scale detection is formalized by the theory of
          scale-space, which parameterizes a continuous signal, $f$, with
          a scale, $\scale$.
        The original signal is said to exist at scale $0$.
        Convolving the original signal with a Gaussian kernel produces
          coarser scales.

        Let $f$ be a continuous $2$-dimensional signal that defines an
          image.
        Let vector $\pt=\ptcolvec$ be a location in the image.
        The function $g(\scale)$
        %= \frac{1}{\TAU\scale^2} \exp{-(\vec{i} \cdot \vec{i})/2\scale^2}$
        is the isotropic 2D Gaussian kernel.
        The scale-space representation of a continuous image (for any
          non-zero scale) takes the form:
        $\img(\pt, \scale) = g(\scale) \conv f(\pt)$, where $\conv$ is
          the convolution operator.
        However, we do not have access to a continuous representation
          of an image.
        Therefore, in practice, the continuous Gaussian kernel is
          replaced with the discrete Gaussian kernel.
        This can be efficiently implemented as a discrete convolution
          with the $1$-dimensional discrete Gaussian kernel in the
          $x$-direction and then in the $y$-direction, because the
          discrete Gaussian kernel is separable in orthogonal
          directions~\cite{lindeberg_scalespace_1993}.
        Using the definition of an image at a single scale the next
          step is to represent an image a multiple scales.

        %\begin{equation} 
        %   \label{eqn:imgscaleparam}
        %   \img(\pt, \scale) = g(\scale - 1) \conv \rawimg(\pt)
        %\end{equation}

        %In this case, it has been shown that the diffusion equation 
        %We are only given a discrete set of pixels that have already been
        %  sampled at some scale, which can be interpreted as scale $1$:
        %$\rawimg(\pt) = g(1) \conv f(\pt)$.
        %Scales less than $1$ can not be reconstructed, however scales greater
        %  than $1$ can be computed due to the semi-group and cascading smoothing
        %  property of the Gaussian kernel \cite{lindeberg_scalespace_1993}.
        %  (The semi-group property of the Gaussian kernel is not preserved after discretization! (pg 94))


        %The scale-space representation of a raw image, $\rawimg$, takes the
        %  form:

       \paragraph{Gaussian pyramid}

           \newcommand{\downsamp}[2]{#1[::\tightpad#2,::\tightpad#2]}

           %\devcomment{FIXME, need to consolidate a bit more}

            % See page 39 of Scale-space theory.
            %The continuous image signal is defined to be the zeroth scale $\img(\pt, 0) = \rawimg(\pt)$. 
            The discrete scale-space representation of an image is
              efficiently implemented using a Gaussian pyramid.
            An scale-space pyramid consists of $L$ levels.
            Each level covers an octave.
            Starting from the base of each level with scale parameter
              $\sigma$ the next octave is reached when $\sigma$ doubles.
            There are $s$ intervals represented within each octave.
            A Gaussian pyramid is illustrated
              in~\cref{fig:ScaleSpaceFigure}.

            \ScaleSpaceFigure{}

            The pyramid's base, %
            $\img(\pt, 1) = g( 1 ) \conv \rawimg(\pt)$ %
            is the $\ell=0$\th{} level of the pyramid, and is computed by blurring
              the original image (sometimes with small initial blurring) with
              $\sigma=1$.
            Subsequent levels of the pyramid are produced by doubling sigma,
              thus the $\ell$\th{} level of the pyramid is $\img(\pt, 2^\ell)$.

            A property of discrete scale-space is that after
              appropriate smoothing downsampling the image by half is
              equivalent to doubling sigma.
            Let
            % ---
            $\img_\ell(\pt) = \downsamp{\rawimg}{2^{\ell}}({\pt} / {2^{\ell}})$ 
            % ---
            denote the raw image downsampled by a factor of $2^{\ell}$
              using Lanczos resampling.
            Now, each level of the pyramid can be written as %
            $\img(\pt, 2^\ell) = g( 1 ) \conv \rawimg^\ell$.
            Given the raw image at level $\ell$, the scale
              corresponding to $\sigma$ can be written as a relative
              scale
            % ---
            $\sigma_\ell = \sigma / 2^\ell$.
            % ---
            Thus, a discrete image at any scale can be efficiently computed
              as:
            \begin{equation}
                \img(\pt, \sigma) =
                    g(\sigma_\ell) \conv \img_\ell(\pt)
            \end{equation}
            Discrete convolution is applied using a window of size
              $\floor{6\sigma_\ell + 1} + (1 -
              (\modfn{\floor{6\sigma_\ell + 1}}{2}))$.
            Interpolation between discrete values of $\pt$ is used to
              sample intensity at sub-pixel accuracy.

             %$\ell \in \Int_{0+}$.
            A scale between two levels of the pyramid is called an
              interval.
            Typically, $s$ intervals --- with relative scales $2^{0/s},
              2^{1/s}, \ldots 2^{s/s}$ --- are computed to represent the
              octave between level $\ell$ and $\ell + 1$.
            If differences between scales are needed, then the scales
              $2^{-1/s}$ and $2^{1 + 1/s}$ are also
              computed~\cite{lowe_distinctive_2004}.

            %An image at any scale $\sigma$ is efficiently computed with respect
            %  to its level $\ell = \txt{max}(0, \floor{\log_{2}(\sigma)})$.

            \begin{comment}

            t = 20
            x = np.arange(t ** 2).reshape((t, t))

            l = 3
            pt1 = np.array([4, 2, 4, 2, 8, 8])
            pt2 = np.array([4, 2, 2, 4, 4, 4])
            print(x)
            print(xL)
            l2 = (2 ** l)
            xL = x[::l2, ::l2]
            pt1L = (pt1 / l2).astype(np.int32)
            pt2L = (pt2 / l2).astype(np.int32)
            res1 = xL[pt1L, pt2L]
            res2 = x[pt1, pt2]
            print('res1 = %r' % (res1,))
            print('res2 = %r' % (res2,))
            
            \end{comment}
            %The $\ell$\th base octave is:
            %$\img(\pt, 2^\ell)$,
            %$\img(\pt, 2^\ell) = \img(2^{\ell}\pt, 2^{\ell - 1})$
            %$\img(\pt, 2^\ell) = \opname{subsample}(\img(\pt, 2^{\ell - 1}))$
            %$\img(\cdot, 2^\ell) = \img(\cdot, 2^{\ell - 1})[::\tightpad2,::\tightpad2]$
            %$\img(\pt, 2^\ell) = \img(2 \pt, 2^{\ell - 1})[::\tightpad2,::\tightpad2]$
            %$\img(\pt, 2^\ell) = \img_{[::2,::2]}(\pt / 2, 2^{\ell - 1})$
            %, where $\ell$ is a non-negative integer.

            %For example:
            %$\img(\pt, 4.2) = g(4.2) \conv \rawimg(\pt) = g(.2)
            %  \conv \img(\pt, 4)$.

    \subsection{Hessian keypoint detection}

        Hessian based keypoint detection searches for extrema of the
          Hessian operator in both space and
          scale~\cite{beaudet_rotationally_1978,
          lindeberg_shapeadapted_1994}.
        The Hessian detector can qualitatively be viewed as a blob
          detector, but it also detects corners which may appear as blobs
          in scale-space~\cite{tuytelaars_local_2007}.
        The Hessian keypoint detector will compute a response value for
          each point in scale space indicating how blob-like each pixel
          is.
        The extrema of this response defines a set of Hessian
          keypoints.
        Post processing removes non-robust keypoints and localizes all
          other keypoints to sub-pixel and sub-scale accuracy.

        \paragraph{Hessian response}
            Let subscripts denote the partial derivatives of the image
              intensity (\eg{} $\img_{x}$ is the first $x$ derivative,
              $\img_{xx}$ is the second $x$ derivative, and $\img_{xy}$
              is the first derivative in both $x$ and $y$).
            The Hessian is a matrix of second order partial derivatives
              and is defined at each point in scale space.
            \begin{equation}
                \hessMAT = 
                \BIGMAT{
                \img_{xx}(\pt, \scale) & \img_{xy}(\pt, \scale) \\
                \img_{xy}(\pt, \scale) & \img_{yy}(\pt, \scale) } 
            \end{equation}\label{eqn:hessianmatrix}  

            %(derivatives are computed in scale-space over an integration scale
            %$\scale_I$). 

            The initial response of the detector at each point is the
              determinant of the Hessian matrix.
            This response is computed for level and every pixel in the
              scale-space pyramid.
            At coarser scales the Hessian response weakens, so to
              ensure that responses between scales are comparable, the
              initial response is scale normalized by multiplying with
              $\sigma^2$ (See~\cite{lindeberg_feature_1998} for more
              details about the choice of this normalization factor).
            The extrema of this space defines a set of candidate
              keypoints, $\kpts'$.
            \begin{equation}
                \kpts' = \argextrema{\pt,\scale} \paren{\scale^2 \detfn{\hessMAT}} 
            \end{equation}
            %\devcomment{Is this 2D or 3D extrema detection.  What would need
            %to change in my math if it was 3D?} 
            A point in this 3D space is a maxima/minima if its scale
              normalized value is greater/less than the scale normalized values
              of all its neighbors in the pyramid --- \ie{} the $8$ neighbors in
              its current interval, its $9$ neighbors in the next interval, and
              its $9$ neighbors in the previous interval.

        \paragraph{Edge filtering}
            Edge responses are not robust --- \ie{} the same point can
              not localized reliably in two views of the same scene ---
              due to their elongated nature.
            Because of this, the extrema that appear too edge-like are
              filtered using a threshold $t_{\tt{edge}}$ which is
              compared to the ratio of the Hessian's squared trace and
              the determinant.
            \begin{equation}
                \kpts = \{(\pt, \scale) \in \kpts' \where
                  \frac{\trfn{\hessMAT}^2}{\detfn{\hessMAT}} > t_{\tt{edge}}\}
            \end{equation}

        \paragraph{Sub-pixel and sub-scale localization}
            To compensate for the discrete nature of pixel images, each
              keypoint detection is localized to sub-pixel and sub-scale
              accuracy.
            The importance of feature localization is demonstrated
              in~\cite{ke_pcasift_2004}, where descriptors were computed
              on the normalized vectors of patch gradients using only
              principal component analysis
              (PCA)~\cite{jolliffe_principal_2014}.
            Despite the simplicity of the descriptors the authors were
              still able to effectively match two images due to the
              robust localization of the features.

            Sub-pixel and sub-scale localization transforms a keypoint
              $\kp_0$ into $\kp^*$ using an iterative process.
            At each iteration $i$, a 2\nd{} order Taylor expansion,
              centered at $\kp_i = (\pt_i, \scale_i)$, approximates the
              scale normalized Hessian response:
            $T_i(\pt, \scale) \approx \scale^2 \detfn{\hessMAT}$.
            The keypoint is updated to the position of the maximum
              response of the Taylor expansion:
            $\kp_{i + 1} = \argmax{\kp} T_i(\kp)$.
            This process iterates until convergence.
            If the process does not converge before a threshold number
              of iterations, the keypoint is deemed not robust and thrown
              out.

    \subsection{Affine adaptation}
        So far, the keypoints we have described correspond to circular
          regions where the pixel radius is some multiple of the scale.
        To account for small affine changes seen in non-planar objects
          (like zebras), the shape of each circular keypoint is adapted
          into an ellipse.

        An affine shape $\vmat=\vMATRIX$ is estimated (as a lower triangular
          matrix) for each keypoint using an iterative technique involving the
          second moment
          matrix~\cite{lindeberg_shapeadapted_1997,baumberg_reliable_2000,mikolajczyk_comparison_2005}.
        The affine shape matrix transforms an ellipse into a unit circle.
        Note that because the matrix is lower triangular one of its
          eigenvectors points in the downward direction.
        Thus the shape has no influence on the orientation of the keypoint.
        For each point in scale space the second moment matrix is evaluated
          as:
        \begin{equation}\label{eqn:secondmoment}
                \momentmat 
                \tighteq 
                \MAT{ 
                \img_x^2(\pt, \scale)      & \img_x(\pt, \scale) \img_y(\pt, \scale) \\
                \img_x(\pt, \scale) \img_y(\pt, \scale) & \img_y^2(\pt, \scale) }
        \end{equation}

        The goal is to ``stabilize'' each keypoint shape by searching for the
          transformation, $\vmat^*$, that causes the second moment matrix to
          have equal eigenvalues.
        For each keypoint, its elliptical shape is initialized as a circle
          $\vmat_0=\eyetwo$.
        For each iteration $i$:

        \begin{enumln}

            \item Compute the second moment matrix,
              $\warpedmomentmat{\vmat_i}$, at the warped image patch.

            \item Check if the keypoint shape is stable.
            A keypoint shape is stable if the eigenvalue ratio of the second
              moment matrix is close to $1$.
            If the keypoint has been stable for two consecutive iterations, then
              accept $\vmat^* \leftarrow \vmat_{i}$ and stop iteration.
            Otherwise if the number of iterations, $i$, is greater than some
              threshold, then stop and discard the keypoint.

            \item Update the affine shape  using the rule
                % ---
                $\vmat_{i + 1} = \sqrtm{\warpedmomentmat{\vmat_i}} \vmat_i$.
        \end{enumln}

        The matrix $\vmat$ only defines the transformation from an
          ellipse to a circle.
        The standard representation of an ellipse is a conic of the
          form $\mat{E} = \vmat^T\vmat$.
        This means that $\vmat$ is only defined up to an arbitrary
          rotation~\cite{mikolajczyk_comparison_2005,perdoch_efficient_2009}.
        Thus we can freely rotate $\vmat$ into a lower triangular
          matrix.
        This ensures that one of its eigenvectors is pointing downwards
          --- \ie{} in the direction of the ``gravity
          vector''~\cite{perdoch_efficient_2009}.
        Assuming the gravity vector removes a dimension of invariance.
        To allow for the specification of keypoint orientation, the
          keypoint representation can be extended with a parameter
          $\theta$ that defaults to $0$.

    \subsection{Orientation adaptation}

        The keypoint orientation is defined using the parameter
          $\theta$.
        Without orientation adaptation a keypoint can be assumed to be
          aligned with the ``gravity vector'' --- \ie{}
          $\theta=0$~\cite{perdoch_efficient_2009}.
        Otherwise, an orientation must be computed.
        A common method for determining a keypoint's orientation is to
          use the dominant gradient orientation.

        To compute a keypoint's dominant orientation the pixels around
          a keypoint vote into a fine-binned orientation
          histogram~\cite{lowe_distinctive_2004}.
        A pixel's vote is weighted by its gradient magnitude multiplied
          by its Gaussian weighted distance to the keypoint center.
        The dominant orientation $\ori \in \rangeinex{0,\TAU}$ is
          chosen as the peak of this histogram.
        If there is more than a single peak it is common to create a
          copy of the keypoint for each maxima in this histogram.
        This process is illustrated in~\cref{fig:testfindkpdirection}.

        \testfindkpdirection{}

        %In some applications the objects being described do not agree with
        %  the gravity vector.
        %In this case it is not ideal to assume the gravity vector.
        %Again we wish to note that increasing invariance of a keypoint
        %  reduces its discriminative power  \cite{perdoch_efficient_2009}.

    \subsection{Discussion --- detector and invariance choices}
        %\paragraph{How should image features be detected?}
        To identify individual animals, features must be detected in
          distinguishing areas of an animal.
        For a feature to be useful, it must be detected in the multiple
          images of the same individual despite variations in viewpoint,
          pose, lighting, and quality.
        In our baseline algorithm we choose to use a Hessian based
          detector~\cite{perdoch_efficient_2009, lindeberg_feature_1998}
          because it generally produces a large number of features and
          has been experimentally shown to be repeatable, accurate, and
          adaptable to multiple degrees of
          invariance~\cite{tuytelaars_local_2007}.
        %Because the choice of feature detection depends on the application
        %  it may be useful to experiment with different degrees of invariance
        %  as well as combining keypoint results from multiple detectors.

        %\paragraph{How much invariance should features have?}
        Once a keypoint is detected, it is described using a keypoint
          description algorithm.
        It is desirable for a keypoint description to be invariant to
          small changes in viewpoint, pose, and lighting.
        Accurate localization of a keypoint in scale and space helps to
          ensure that similar images contain similar features.
        Sometimes, it is beneficial to further localize a keypoint in
          shape and orientation, thus adding invariance to the feature.
        However, if too much invariance is used, it may not be possible
          to distinguish between semantically different features.

        It is a challenge to choose the correct level of invariance
          when computing features.
        Often an application chooses one of two extremes.
        Consider the computation of keypoint orientation.
        Standard methods for orientation invariance assume patches can
          freely rotate, when in fact they may be constrained by the
          orientation of surrounding
          patches~\cite{lowe_distinctive_2004}.
        On the other side extreme is the ``gravity
          vector''~\cite{perdoch_efficient_2009}, which globally enforces
          all keypoint to have a downward orientation.
        This may be a safe assumption when working with image features
          from rigid objects taken in an upright position, but may not be
          correct when dealing with non-rigid objects like zebras.
        %Orientation invariance assumes that orientation is a local property
        %  of a patch, but the orientation of a patch is usually not
        %  independent of its surrounding patches on an object.
        In our experiments in~\cref{sub:exptinvar} we test different
          degrees on invariance.
        This test includes a novel method that achieves a middle ground
          between full orientation invariance and the gravity vector.

