
\begin{comment}
fixtex --reformat --fpaths chapter4-pairclf.tex --print
fixtex --fpaths chapter4-pairclf.tex --outline --asmarkdown --numlines=999 

fixtex --fpaths chapter4-pairclf.tex --outline --asmarkdown --numlines=999 --shortcite -w && ./checklang.py outline_chapter4-pairclf.md
https://www.languagetool.org/
\end{comment}

\newcommand{\nan}{\text{nan}}

\chapter{Pairwise classification}\label{chap:pairclf}

In this chapter we consider the problem of verifying if two annotations are from the same animal or from different
animals. By addressing this problem we improve upon the ranking algorithm from \cref{chap:ranking} --- which ranks
the \names{} in a database based on similarity to a query --- by making semi-automatic decisions about results
returned in the ranked lists. The algorithms introduced in this chapter will assign a confidence to results in the
ranked list, and any pair above a confidence threshold can be automatically reviewed. We will demonstrate that our
decision algorithms can significantly reduce the number of manual interactions required to identify all individuals
in an unlabeled set of annotations.

To make semi-automatic decisions up to a specified confidence we develop a \emph{pairwise probabilistic
  classifier} that predicts a probability distribution over a set of events given two annotations (typically a
  query annotation and one of its top results in a ranked list).
Given only the information in two annotations,  there are three possible decisions that can be made.
A pair of annotations is either:
\begin{enumln}
    \item incomparable --- the annotations are not visually comparable,

    \item positive --- the annotations are visually comparable and the same individual, or

    \item negative --- the annotations are visually comparable and different individuals.
\end{enumln}
Two annotations can be incomparable if the annotations show different parts or sides of an animal, or if the
  distinguishing information on an animal is obscured or occluded.
The positive and negative states each require distinguishing information to be present.
These mutually exclusive ``match-states'' are illustrated in \cref{fig:MatchStateExample}.
The multi-label classifier then predicts the probability of each of the three states, with the probabilities
  necessarily summing to $1$.

\MatchStateExample{}

To construct a pairwise probabilistic classifier we turn towards supervised machine learning, which requires that
  we:
\begin{enumin}
    \item determine a set of labeled annotation pairs for training, 

    \item construct a fixed-length feature vector to represent a pair of annotations,  and

    \item choose a probabilistic learning algorithm.
\end{enumin}
The first requirement can be satisfied by carefully selecting representative annotations pairs, and the last
  requirement is satisfied by many pre-existing algorithms (\eg{} random forests and neural networks).
It is the second requirement --- constructing an appropriate fixed-length feature vector --- that is the most
  challenging.
If given enough training data and a technique to align the animals in two annotations, using image data with a
  Siamese or triplicate network~\cite{taigman_deepface_2014,schroff_facenet_2015} might appropriate, but without
  both of these pre-conditions we must turn towards more traditional methods.
Recall from \cref{sec:annotrepr} that our annotation representation is an unordered bag-of-features, which cannot
  be directly fed to most learning algorithms.
Therefore, we develop a method for constructing a fixed length \glossterm{pairwise feature vector} for a pair of
  annotations.
This novel feature vector will take into account local matching information as well as more global information
  such as GPS and viewpoint.
A collection of these features from multiple labeled annotation pairs are used to fit a random
  forest~\cite{breiman_random_2001} which implements our pairwise classifier.
We choose to use a random forest classifier, in part because they are fast to train, robust to overfitting, and
  naturally output probabilities in a multiclass setting, and in part because they can handle (and potentially take
  advantage of) missing data --- \ie{} \nan{} values in feature vectors --- using the ``separate class''
  method~\cite{ding_investigation_2010}.
  

A final concern investigated in this chapter is the issue of image challenges that may confound the match-state
  pairwise classifier.
Recall from~\cref{sub:exptfail}, {photobombs} --- pairs of annotations where feature correspondences are caused
  by a secondary animal --- which are the most notable cause of such a challenge.
By most accounts photobombs appear very similar to positive matches, and this similarity could confuse the
  match-state classifier.
However, because photobombs are inherently a pairwise property between annotations, it should be possible to
  learn a separate classifier explicitly tasked with the challenge.
Therefore, we also learn a photobomb classifier using the same sort of pairwise feature vector and random forest
  classifier.
This supporting classifier will allow us to increase the accuracy of our identification by restricting automatic
  classification to pairs where the decision is straightforward.


This outline of this chapter is as follows.
\Cref{sec:pairfeat} details the construction of the feature vector that we use as input to the pairwise
  classifier.
\Cref{sec:learnclf} describes the process of collecting training data and learning the match-state pairwise
  classifier.
\Cref{sec:learnpb} extends this approach to predict secondary attributes (\eg{} is a pair a photobomb) beyond
  just the matching state.
\Cref{sec:pairexpt} presents a set of experiments that evaluate the pairwise classifier.
\Cref{sec:pairconclusion} summarizes and concludes this chapter.


\section{Constructing the pairwise feature vector}\label{sec:pairfeat}

In order to use the random forest learning algorithm to address the problem of pairwise verification, we must
  construct a feature vector to representing a pair of annotations that contains information able to differentiate
  between each class.
In contrast to the unordered bag-of-features used to represent an annotation, the dimensions in this feature
  vector must be ordered and each dimension should correspond to a specific measurement.
In practice this means that the feature vector must be ordered and have a fixed length.

We construct this feature vector to contain both global and local information.
Global information is higher level and serves to augment visual information.
The local information aggregates statistics about feature correspondences between the two annotations.
The local and global vectors are constructed separately and then concatenated to form the final pairwise feature
  vector.
The remainder of this section discusses the construction of these vectors.

\subsection{The global feature vector}

The global feature vector contains information that will allow the classifier to take advantage of semantic
  labels and non-visual attributes of our data to solve the verification problem.
Semantic labels such as quality and viewpoint are derived from visual information and can provide abstract
  knowledge to help the classifier make a decision.
Non-visual attributes such as GPS and timestamp can be extracted from EXIF metadata and may help determine facts
  not discernible from visual data alone.
The global feature vector is derived from the following attributes extracted from each annotation:
\begin{enumln}

    \item Timestamp, represented in POSIX format as a float.

    \item GPS latitude and longitude, represented in radians as two floats. 

    \item Viewpoint classification label, represented as a categorical integer ranging from $1$ to $8$.

    \item Quality classification label, represented as a categorical integer ranging from $1$ to $5$.
\end{enumln}
We gather the GPS and timestamp attributes from image EXIF data, and the viewpoint and quality labels are outputs
  of the deep classifiers discussed in \cref{subsec:introdataprocess}.
The GPS and timestamp attribute inform the classifier of when its not possible for two annotations to match
  (\eg{} when a pair of annotations is close in time but far in space).
The viewpoint and quality attributes should help the classifier predict when pairs of annotations are not
  comparable --- forcing there to be stronger evidence to form a match, such as strong correspondences on a face
  turned toward the camera in both a left and right side view.
An example illustrating such a case where two annotations with different viewpoints are a positive match is
  illustrated in \cref{fig:LeftRightFace}.

\LeftRightFace{}

These four ``unary'' attributes are gathered for each annotation.
Thus, for each attribute we have two measurements, but we do not use them directly because the ordering of the
  annotations in each pair is arbitrary.
For each unary attribute, we either ignore it (as in the case of GPS and time) or record the minimum of the two
  values one feature dimension and the maximum in another (as is done with viewpoint and quality).
This results in $4$ unary measurements, $2$ for viewpoint and $2$ for quality.

The remaining dimensions of the global feature vector are constructed by encoding relationships between pairs of
  unary attributes using distance measurements.
In the case of GPS coordinates we use the haversine distance (as detailed in \cref{app:occurgroup}), but for all
  other measures we use the absolute difference of their values.
In the case of viewpoint this absolute difference is cyclic  --- \ie{} the distance between viewpoint $x$ and $y$
  is $\min(|x - y|, 8 - |x - y|)$.
This results in $4$ pairwise measurements, one for each global attribute.
Lastly, we include the ``speed'' of the pair, which is the GPS-distance divided by the time-delta.
Thus, there are a total of $4 + 4 + 1 = 9$ global measurements.

In the event that an attribute is not provided or not known (\eg{} the EXIF data is missing) a measurement cannot
be made, so a \nan{} value is recorded instead. To apply random forests learning, these \nan{} values must be
handled by either modifying the learning algorithm or replacing them with a number. Ding and Simonoff investigate
several methods for doing this in~\cite{ding_investigation_2010}, and they conclude that the best choice is
application dependent. For our application we choose the ``separate class'' method because it is simple and in
their experiments it performs the best when \nan{} values are in both the training and testing data.

The separate class method simply replaces all \nan{} measurements with an extremely large number.
This means that whenever a decision node applies its test to a \nan{} value, the result will always be the same.
In this way the \nan{} values are essentially treated as a separate category because a test can always be chosen
  that separates the measured and unmeasured data.
In the case that a \nan{} measurement in a feature dimension is informative (\eg{} if images without timestamps
  are less likely to match other annotations), the random forest can take advantage of that dimension.
However, in the more likely case that the same \nan{} measurement is uninformative, the dimension can still be
  used, but it is penalized proportional to the fraction of samples where it takes a \nan{} value.
This captures the idea that a feature dimension is less likely to be informative if it cannot be measured
  reliably.
However, if that feature dimension is highly informative for samples where it has a numeric value, then the
  random forest can still make use of it, and the samples with \nan{} values can be split by later nodes.

\subsection{The local feature vector}
The local feature vector distills two orderless bag-of-features representations into a fixed length vector
  containing matching information about a pair of annotations.
Three steps are needed to construct the local feature vector.
First we determine feature correspondences between the two annotations.
Then for each correspondence we make several measurements (\eg{} descriptor distance and spatial position).
Then we aggregate these measurements over all correspondences using summary statistics (\eg{} mean, sum, std).
Thus, the total length of the feature vector is the number of measurements times the number of summary statistics
  used.

To determine feature correspondences between two annotations, $\qaid$ and $\daid$, we use what we refer to as a
  one-vs-one matching algorithm.
Each annotation's descriptors are indexed for fast nearest neighbor search~\cite{muja_fast_2009}.
Keypoint correspondences are formed by searching for the reciprocal nearest neighbors between annotation
  descriptors~\cite{qin_hello_2011}.
For each feature in each correspondence, the next nearest neighbor is used as a normalizer for Lowe's ratio
  test~\cite{lowe_distinctive_2004}.
Because matching is symmetric, each feature correspondences is associated with two normalizing neighbors.
The feature / normalizer pair with the minimum descriptor distance is used as a normalizing pair.
If the descriptor distance between correspondences is divided by the distance between the normalizing pair is
  above a threshold, the correspondence is regarded as non-distinct and removed.
For the simplicity of the description we consider just one ratio threshold for now, but later we will describe
  this process using multiple thresholds.
Spatial verification~\cite{philbin_object_2007} is applied to further refine the correspondences.
This results in a richer set of correspondences between annotations $\qaid$ and $\daid$ than would be found using
  the ranking algorithm.
After the one-vs-one matching stage, measurements are made at each feature correspondence.

Consider a feature correspondence between two features $i$ and $j$ with descriptors $\desc_i$ and $\desc_j$.
Let $\descnorm_i$ be the normalizer for $i$, and let $\descnorm_j$ be the normalizer for $j$.
Note that while $i$ is from $\qaid$, its normalizer, $\descnorm_i$, is a descriptor from $\daid$.
Let $c \in \curly{i, j}$ indicate which feature / normalizer pair is used in the ratio test, %
$c = \argmin{c \in {i, j}}{\elltwo{\desc_c - \descnorm_c}}$.
Given these definitions, the measurements we consider are:

\begin{itemln}

    \item Foregroundness score:
    This is the geometric average of the features' foregroundness measures, $\sqrt{w_i w_j}$.
    %along with the individual foregroundness weights $w_i$ and $w_j$.
    This adds $1$ measurement, denoted as $\tt{fgweight}$, for each correspondence.

    \item Correspondence distance:
    This is the Euclidean distance between the feature correspondence, $\elltwo{\desc_i - \desc_j} / Z$.
    This serves as a measure of visual similarity between the features.
    (Recall $Z\tighteq\sqrt{2}$ for SIFT descriptors).
    This adds $1$ measurement, denoted as $\tt{match\_dist}$, for each correspondence.

    \item Normalizer distance:
    This distance between a matching descriptor and the normalizing descriptor, %
        $\elltwo{\desc_c - \descnorm_c} / Z$.
        This serves as a measure of visual distinctiveness of the features.
    We also include a weighted version of this measurement by multiplying it with the foregroundness score.
    This adds $2$ measurements, denoted as $\tt{norm\_dist}$ and $\tt{weighted\_norm\_dist}$, for each correspondence.

    \item Ratio score:
    This is the one minus the ratio between the distance between the correspondence, and the distance to the
      normalizer, %
    $1 - \elltwo{\desc_i - \desc_j} / \elltwo{\desc_c - \descnorm_c}$.
    This weights the similarity against the distinctiveness.
    Note that this is one minus the measure used to filter correspondences in the ratio test.
    We subtract the original ratio value from one in order to obtain a score that varies directly with
      distinctiveness.
    We also include a weighted version of this measurement by multiplying it with the foregroundness score.
    This adds $2$ measurements, denoted as $\tt{ratio\_score}$ and $\tt{weighted\_ratio\_score}$, for each correspondence.
    %Note, we subtract the ratio from one to ensure that larger score are better.
    %We also include a weighted version of the ratio and normalizer distance by multiplying this weight by the
    %  respective value.
        
    \item Spatial verification error:

        This is the error in location, scale, and orientation, $( %
            \Delta \pt_{i, j}, %
            \Delta \scale_{i, j}, %
            \Delta \ori_{i, j})$, as measured using~\cref{eqn:inlierdelta}.
            This adds $3$ measurements, denoted as $\tt{sver\_err\_xy}$, $\tt{sver\_err\_scale}$, and
              $\tt{sver\_err\_ori}$, for each correspondence.

    \item Keypoint relative locations:

        These are the xy-locations of the keypoints divided by the width and height of the annotation  %
        $x_i / w_{\qaid}, y_i / h_{\qaid}$ and $x_i / w_{\daid}, y_j / h_{\daid}$.
        This adds $4$ measurements, denoted as $\tt{norm\_x1}$, $\tt{norm\_y1}$, $\tt{norm\_x2}$, and
          $\tt{norm\_y2}$, for each correspondence.
        Note that unlike the global quality and viewpoint measures, we do not make an effort to account for the
          arbitrary ordering of annotations when recording these local features.
        This is to preserve the association between the spatial dimensions of each annotation.
        The same is true for the next feature.

    \item Keypoint scales:

        These are the keypoint scale parameters $\sigma_i$ and $\sigma_j$ that indicate the size of each keypoint
          with respect to its annotation.
        This adds $2$ measurements, denoted as $\tt{scale1}$ and $\tt{scale2}$, for each correspondence.

\end{itemln}

Once these $15$ measurements have been made for each keypoint correspondence we summarize them using summary
  statistics.
We consider the sum, median, mean, and standard deviation over all correspondences.
%We consider the sum, inverse-sum, mean, and standard deviation over all correspondences.
We have also considered taking values from individual correspondences based on rankings and percentiles with
  respect to a particular attribute (\eg{} ratio score), however we found that these provided little information in
  practice.
The resulting measurements are stacked to form the local feature vector.
This results in $15 \times 4 = 60$ measurements.
A final step we have found useful is to append an extra dimension simply indicating the total number of feature
  correspondences.
So, in total there are $61$ summary statistics computed for a set of feature correspondences.

As previously noted this process is repeated for multiple threshold values of the ratio test.
In our implementation we use values of $0.5$, $0.6$, $0.7$, and $0.8$.
Once we have assigned feature correspondences, we filter these correspondences using the ratio test with the
  maximum value of the ratio threshold.
Note that these threshold points are with respect to the original ratio values, and not the ratio scores used in
  the feature vector.
Then the remaining feature correspondences are spatially verified as normal.
At this point, we create a group of correspondences for each value of the ratio threshold.
The member of each group are all correspondences with a ratio score less than that value.
The local feature vector is constructed by applying summary statistics to each of these groups and then
  concatenating the results, so in essence the size of the local feature vector is multiplied by the number of
  ratio thresholds used.
Thus the total number of local measurements is $4 \times 61 = 244$.

% NOTE:
%Once we have assigned feature correspondences, we create a group of correspondences for each ratio value.
%The member of each group are all correspondences with a ratio score less than that value.
%Each group is then spatially verified, and the union of the groups is the final set of correspondences.
%When measuring spatial verification errors, each keypoint may be associated with more than one.
%Therefore, we use the minimum error over all values of the ratio threshold.

The reason we decided to use multiple ratio thresholds is because we noticed that some positive annotation pairs
  had all of their correspondences filtered by the ratio test, but if we increased the threshold then the overall
  classification performance decreased.
Including larger values of the threshold ensures that most pairs of annotations generate at least a few
  correspondences, while smaller threshold ensure that information in highly distinctive correspondences are
  separated out and considered by the classifier.
This softens the impact of the ratio test's binary threshold and adds robustness to viewpoint and pose variations
  that may cause correspondences to appear slightly less distinctive.

\FloatBarrier{}
\subsection{The final pairwise feature vector}

The final pairwise feature vector is constructed by concatenating the local and the global vector.
This results in a $253$ dimensional vector containing information that a learning algorithm can use to predict if
  a pair of annotations is positive, negative, or incomparable.
Of these dimensions, $9$ are from global measurements and $244$ are from local measurements.
The example in~\cref{fig:PairFeatVec} illustrates part of a final feature vector.

%\PairFeatVec{}
\begin{figure}
\begin{minted}[gobble=4]{python}
    OrderedDict([('global(qual_min)',    3),
                 ('global(qual_max)',    nan),
                 ('global(qual_delta)',  nan),
                 ('global(gps_delta)',   5.79),
                 ('len(matches[ratio < .8])',        20),
                 ('sum(ratio[ratio < .8])', 10.05),
                 ('mean(ratio[ratio < .8])',         0.50),
                 ('std(ratio[ratio < .8])',          0.09)])
\end{minted}
\caption[\caplbl{PairFeatVec}A pairwise feature vector]{\caplbl{PairFeatVec} %
% ---
Example of a small pairwise feature vector containing local and global information.
In the constructed pairwise feature contains $253$ dimensions.
Note the summary statistics in this example are all computed for correspondences with a ratio measurement that is
  less than $0.8$.
% ---
}
\label{fig:PairFeatVec}
\end{figure}

%This results in a final descriptor vector where  that can be quite large (thousands of dimensions) and some dimensions
%  might not be useful.
%Therefore, we prune the feature vector using only the most useful of the proposed dimensions.


\section{Learning the match-state classifier}\label{sec:learnclf}

    Having defined the pairwise feature vector the only remaining steps to constructing the pairwise classifier
      are to select a sample of labeled training data and choose a probabilistic learning algorithm.
    We will use the random forest learning algorithm, which is implemented in Scikit
      Learn~\cite{pedregosa_scikit_learn_2011}.

    The random forest learning algorithm is well understood, so we only provide a brief overview.
    A random forest is constructed by learning a forest of decision trees.
    Learning begins by initializing each decision tree as a single node.
    Each root node is assigned a random sample of the training data with replacement, and then a recursive
      algorithm is used to grow each root node into a decision tree.
    Each iteration of the recursive algorithm is given a leaf node, and will choose a random test to split the
      training data at the node into two child nodes.
    The random test is constructed by choosing a random threshold and a random subset of feature dimensions as
      candidates.
    Each candidate feature dimension splits the training data, and the one that results in the largest decrease
      in class-label entropy is chosen as the test for this node.
    The algorithm is then recursively executed on the right and left node until a leaf is assigned fewer than a
      minimum number of training examples.
    To learn our random forest classifiers we use $256$ decision trees.
    To select a test for a node, the number of candidate features dimensions we choose is the square root of the
      total number of feature dimensions.
    We stop growing a branch if a leaf node contains $5$ or fewer samples.
    Each decision tree predicts a probability distribution over all classes for a testing example by descending
      the tree, choosing left or right based on the test chosen at the node until it reaches a leaf node.
    The predicted probabilities are the proportions of training class-labels at that leaf node.
    The probability prediction of the random forest is the average of the probabilities predicted by all decision
      trees.

    Now that we have chosen a learning algorithm, the last remaining step to explain is the selection of training
      data and generation of labels.
    Recall that our the purpose of our classifier is to output a probability distribution over three labels:
    positive, negative and incomparable.
    Given a pair of annotations we need to assign one of these three labels using ground-truth data.
    In recent versions of our system, this ground-truth label is stored along with each unordered pair of
      annotations that has been manually reviewed, but because this is a new feature there does not exist many
      pairs with the explicit three-state label.
    Therefore, we must make use of data contained in previous versions of the system that simply assign a name
      label to each annotation.
    This allows us to determine if an annotation pair are the same or different animals, but it does not allow us
      to determine if the pair is comparable.
    To account for this we use heuristics to assign the incomparable label using the viewpoints, and if either
      annotation is not assigned a viewpoint it is assumed that they are comparable because most images in our
      datasets are taken from a consistent viewpoint (\ie{} collection events were designed to reduce
      incomparability).
    Thus, training labels are assigned to a pair as follows:
    use the explicit labels if they exist, otherwise heuristically decide if the pair is comparable based on
      viewpoint information, if not then return incomparable, otherwise return positive if the annotations share a
      name label and negative if they do not.

    In order to select pairs from our ground truth dataset, we sample representative pairs of annotations guided
      by the principal of selecting examples that exhibit a range of difficulties~\cite{shi_embedding_2016} (\eg
      hard-negatives and moderate positives).
    We use the LNBNN ranking algorithm to estimate how easy or difficult it might be to predict the match-state
      of a pair.
    Pairs with higher LNBNN ranks will be easier to classify for positive examples and will be more difficult for
      negative examples, and lower ranks will have the reverse effect.

    Specifically, to sample a dataset for learning, we first rank the database for each query image using the
      ranking algorithm.
    We partition the ranked lists into two parts:
    a list of correct result matches and a list of incorrect matches.
    We select annotations randomly and from the top, middle, bottom of each list.
    For positive examples we select $4$ from the top, $2$ from the middle, $2$ from the bottom, and $2$ randomly.
    For negative examples we select $3$ from the top, $2$ from the middle, $1$ from the bottom, and $2$ randomly.
    If there are not enough examples to do this, then all are taken.
    We included all pairs explicitly labeled as incomparable because there is only a small number of such
      examples.
    If this was not the case, then we would include an additional partition for incomparable cases.


\section{Secondary classifier to address photobombs}\label{sec:learnpb}
    It is useful to augment the primary match-state pairwise classifier with a secondary classifier able to
      determine if a pair of annotations contains information that might confuse the main classifier.
    These confusing annotation pairs should not be considered for automatic review.
    In annotation one of the most challenging of these secondary states is one that we refer to as a {photobomb}.
    A pair of annotations is a photobomb if a secondary animal in one annotation matches an animal in another
      annotation (\eg see Figure~\ref{fig:PhotobombExample}).
    Only the primary animal in each annotation should be used to determine identity, but photobombs provide
      strong evidence of matching that can confuse a matching algorithm.

    \PhotobombExample{}

    During events like the \GZC{} we labeled several annotation pairs as photobombs.
    Using these labels we are able to construct a classifier in the same way that we constructed the primary
      match-state classifier.
    Here we can select all pairs of annotations labeled as photobombs for positive training data.
    For negative training data we must be careful only to choose pairs that have been explicitly reviewed as not
      a photobomb.
    We attempt to balance the ratio of positive, negative, and incomparable examples in the negative training
      data.

\section{Pairwise classification experiments}\label{sec:pairexpt}

    We evaluate the pairwise classifiers on two datasets, one of plains zebras and the other of Grévy's zebras.
    These datasets were previously described in \cref{sub:datasets}.
    We will use these datasets to sample a set of annotation pairs, from which we will train and test our
      classifiers.
    The plains zebra dataset has $1202$ names with $5720$ annotations to sample from, and the Grevy's zebras
      dataset has $771$ names with $2283$ annotations.
    For each dataset we choose a sample of annotation pairs as detailed in \cref{sec:learnclf}.
    This results in a sample of $47312$ pairs for plains zebras, with $16583$ positive pairs, $30376$ negative
      pairs, and $353$ incomparable pairs.
    For Grévy's zebra we sample $18010$ pairs, where $5002$ are positive, $13008$ are negative, and $0$ are
      incomparable.
    Because our datasets only have a small number of labeled incomparable and photobomb cases, our experiments
      will primarily focus on the important question of separating positive from negative matching states.
    %When interpreting the results of these experiments it is important to note that the plains zebra dataset
    %  contains only $54$ explicitly labeled incomparable cases and $286$ labeled photobombs, while the Grévy's
    %  zebra dataset contains $0$ labeled incomparable cases and $79$ labeled photobombs.
    %For plains zebras we can increase the number of incomparable cases to $353$ using by using viewpoint
    %  heuristics.
    %Therefore, our experiments will primarily focus on the important question of separating positive from
    %  negative matching states.

    After sampling, we have a set of annotation pairs and each is associated with a ground-truth matching state
      label of either positive, negative, or incomparable.
    Additionally, each pair is also labeled as either a photobomb or not a photobomb.
    For each pair we construct a pairwise feature vector as described in \cref{sec:pairfeat}.
    We have found that, for plains zebras, it is important to use orientation augmentation when computing
      one-vs-one matches.
    In this case we should not use the second nearest neighbor as the normalizer for the ratio test, because the
      augmented keypoints will have similar descriptors.
    We account for this by using the $3\rd$ nearest neighbor as the normalizer instead.

    We split this dataset of labeled annotation pairs into multiple disjoint training and testing sets using
    grouped stratified $k$-fold cross validation (we use $3$ folds). First, we enforce that each sample (a pair of
    annotations) within the same name or between the same two names must be placed in the same group. Then, the
    cross validation is constrained such that all samples in a group are either in the training or testing set for
    each split. In other words, this means that a specific individual cannot appear in both the training and
    testing set, which helps ensure that our results will generalize to new individuals.

    For each cross validation split, we train the matching state and photobomb-state classifier on the training set
    and then predict probabilities on each sample in the testing set. Because the cross validation is $k$-fold and
    the splits are disjoint, we make a single prediction for each sample for each classifier. The result is that
    each sample in our dataset will be assigned unbiased probabilities.

    We will compare these predictions with the scores generated by LNBNN, so we must additionally generate an
      LNBNN score for each pair.
    This is done by issuing each annotation as a query.
    Any (undirected) pair in our dataset that appears as a query / database pair in the ranked list is assigned
      that LNBNN score.
    If a pair appears twice in the ranked list, then the maximum of the two scores is used.
    Any pair that does not appear in the ranked list (because LNBNN failed to return it) is implicitly given a
      score of zero.

    We have now predicted match-state probabilities, photobomb-state probabilities, and one-vs-many LNBNN scores
      for each pair in our dataset.
    We now analyze the results for each classifier in the following subsections.
    Our tests will compare the match-state classifier and the LNBNN ranking algorithm.
    However, note that the scores from the LNBNN ranking algorithm can only be used to distinguish positive cases
      from non-positive pairs.
    Unlike the match-state classifier, the LNBNN scores cannot be used to tell if a non-positive case is negative
      or incomparable.
    Later in \cref{chap:graphid} it will be vitally important to distinguish these cases, but for now, in order
      to fairly compare the two algorithms, we only consider positive probabilities from the match-state
      classifier.

    In our experiments we will measure the raw number of classification errors and success in a confusion matrix,
      and then summarize these numbers using standard classification metrics.
    We will then compare our classifier to LNBNN in two ways.
    First, we will compare the original LNBNN ranking against a re-ranking using the positive probabilities.
    Second, we will compare the ability of each algorithm to predict if a pair is positive or not.
    We will look at the distribution of positive and non-positive scores as well as the ROC curves.
    After comparing to LNBNN, we inspect the importance of each feature dimension of our pairwise feature vector.
    Finally, we will present the several examples of failure cases to illustrate where improvements are can be
      made.

    \FloatBarrier{}
    \subsection{Matching state}

        The primary classifier predicts the matching state (positive, negative, incomparable) of a pair of
        annotations. To demonstrate the effectiveness of our multiclass classifier we report the confusion matrix
        of class predictions for plains and Grévy's zebras in \cref{tbl:ConfusionMatch}.

        Using this confusion matrix we compute the average precision, recall, and Matthews correlation
          coefficient (MCC)~\cite{powers_evaluation_2011} for each class.
        These numbers are reported in \cref{tbl:EvalMetricsMatch}.
        The MCC provides a measurement of overall multiclass classification performance unbiased by class
          distribution.
        An MCC ranges from $+1$, indicating perfect predictions, to $-1$, indicates pathological inverse
          predictions, with $0$ being random uninformed predictions.
        % FIXME: ensure these numbers match the tables
        The MCC of $0.86$ for Grévy's zebras and $0.91$ for plains zebras, indicating that our classifiers have
          strong predictive power.

        \ConfusionMatch{}

        \EvalMetricsMatch{}

        In addition to being strong predictors of match state, the positive probabilities from our classifiers
          can be used to re-rank the ranked list produced by LNBNN.
        Using the procedure from our experiments in~\cref{sec:rankexpt}, we issue each annotation in our testing
          set as a query and obtain a ranked list.
        Using the fraction of correct results found at each rank, we construct a cumulative match characteristic
          (CMC) curve~\cite{decann_relating_2013}.
        We denote this CMC curve as \pvar{ranking}.
        Then we take ranked lists and compute match-state probabilities for each pair of query/database
          annotations.
        We use the positive probabilities to re-rank the lists and construct another CMC curve denoted as
          \pvar{rank+clf}.
        The results of this experiment are illustrated in~\cref{fig:ReRank}, and clearly demonstrate that the
          number of correct matches returned at rank $1$ is improved by re-ranking with our pairwise classifier.
        

        \ReRank{}
        
        \FloatBarrier{}
        \paragraph{Binary positive classification}
        Although the accuracy of multiclass predictions is important, we are most concerned with distinguishing
          positive pairs from the other classes.
        Ideally our learned classifiers will result in superior separation when compared to using just the LNBNN
          scores.
        To test this we plot histograms of scores (LNBNN vs positive probability) in \cref{fig:PositiveHist}.
        It is immediately noticeable that the pairwise scores seem to have a much better separating in addition
          to being in the interpretable range of zero to one.

        %These results demonstrate the advantage of using the pairwise classifier
        %of the pairwise classifier
        %that the pairwise classifier results in
        %interpretable scores

        We can make a more direct and precise comparison by considering both LNBNN scores and pairwise
          probabilities as binary classifiers.
        The separability of each method is measured using the area under an ROC curve.
        The ROC curves for plains and Grévy's zebras are illustrated in \cref{fig:PositiveROC}.
        For plains zebras, the pairwise AUC of $0.96$ is better than the LNBNN AUC of $0.94$.
        For Grévy's zebras the pairwise AUC of $0.99$ is convincingly better than the LNBNN AUC of $0.90$.

        \PositiveHist{}

        \PositiveROC{}


        %\FloatBarrier{}
        \paragraph{Feature importance}

        To gain an intuition for which features are most important we create a word cloud where the size of the
          text is proportional to the importance of the feature as measured using the ``mean decrease impurity''
          (MDI)~\cite{louppe2014understanding}.
        The MDI is a measure of feature importance that is computed during the training phase.
        As a decision tree is grown, the number of training samples of each class that reach a particular node
          are recorded.
        This is used to compute the impurity of each node, which is the entropy of class labels.
        The fraction of total samples that reach a node is its weight.
        The weighted impurity decrease of a node is its weighted impurity minus the weighted sum of its
          children's impurity.
        The MDI for a single feature dimension in a single tree is computed as the weighted impurity decrease of
          all nodes using that feature.
        The overall MDI for the forest is obtained by averaging over all trees.

        The word cloud is illustrated in \cref{fig:MatchWordCloud}.
        The numeric importance of the top features is recorded in \cref{tbl:ImportantMatchFeat}.

        For plains zebras, the global viewpoint measures and the local scales of corresponding keypoints are the
          most important features.
        This makes sense because the global viewpoint helps distinguish negative from incomparable cases, and the
          keypoint scale helps determine if matches are being made on a coarse level (\eg{} matching general zebra
          shapes) or a fine detailed level (\ie{} capturing the smaller features that actually distinguish
          individuals).
        For Grévy's zebras, different statistics about the ratio measures dominate.

        \MatchWordCloud{}
        \ImportantMatchFeat{}

        We also consider if removing the least important feature dimensions might have a positive impact on
          classification performance.
        To measure this, we consider a greedy algorithm.
        First we learn a random forest on a training set and compute the MCC on a test set.
        Then we measure the least important feature dimension as the one with the lowest MDI and remove it from
          the dataset.
        We repeat these two steps until there is only a feature dimension remaining.
        In~\cref{fig:Prune} we plot the MCC as a function of the number of features.

        The results of this experiment indicate that there is little to no increase in classification accuracy
          from pruning features dimensions.
          However, once the number of feature dimensions falls below ${\sim}50$, the performance starts to noticeably
          degrade.
        This suggests that removing any of these top $50$ features would degrade performance. 

        For Grévy's zebras these $50$ dimensions are composed of $2$ global measures:
        $\tt{delta\_gps}$ and $\tt{speed}$, and statics about $9$ local measurements:
        $\tt{match\_dist, norm\_dist, norm\_y1, norm\_y2, ratio, scale1, sver\_err\_ori, sver\_err\_scale,}$ and
          $\tt{sver\_err\_xy}$.

        However, one weakness of this test is that the MDI does not measure correlation between features.
        It might be the case that it if two feature are highly correlated (\eg{} the mean and median ratio
          values), it might not be necessary to include both.
        Correlated feature dimensions are also known to decrease the accuracy of individual decision trees.
        However, it is also known that this effect is alleviated by the randomization process and when averaging
          over multiple decision trees~\cite{louppe2014understanding}.
        It is for this reason and the fact that the $MCC$ does not significantly improve that we chose to use all
          $253$ dimension of our feature vector in the other experiments.

        \MatchPrune{}
        
        \paragraph{Failure cases}

        Lastly we investigate several failure cases.
        First, it is not surprising that incomparable pairs like the one illustrated in \cref{fig:PairFailIN}
          might be labeled as incomparable because of the lack of incomparable training data.
        The previously discussed failure ranking cases from ~\cref{sub:exptfail} such as viewpoint and quality
          are inherently challenging and also cause the pairwise classifier to fail as illustrated
          by~\cref{fig:PairFailPN}.
        Lastly, sometimes the pairwise classifier is simply unable to key in on cues that a set of one-vs-one
          correspondences is not distinctive like when it predicts that the pair in ~\cref{fig:PairFailNP} is
          positive instead of negative.
        However, note that in all of these examples that the non-extreme probabilities assigned to each state
          demonstrate that the classifier is not confident in these predictions.
        We will take advantage of this in the next chapter.
        

        %illustrate failure cases: TODO Match State Failure Cases
        \PairFailIN{} 

        \PairFailPN{}

        \PairFailNP{}

        \FloatBarrier{}


    %---------
    \FloatBarrier{}
    \subsection{Photobomb state}
        We perform a subset of similar experiments to demonstrate the effectiveness of our photobomb classifier.
        The overall performance is given in the classification confusion matrix illustrated in
          \cref{tbl:ConfusionPhotobomb} and evaluation metrics are given in \cref{tbl:EvalMetricsPhotobomb}.

        Due to the small amount of available training data the performance of the photobomb-state classifier is
          weaker than the match-state classifier.
        For plains zebras the MCC of $0.64$ demonstrates that the photobomb classifier has moderate predictive
          power.
        For Grévy's zebras the MCC of $0.31$ demonstrates weak but significant predictive power.

        While the small number of training examples makes it difficult to draw conclusions, there are two
          observations we can make.
        First, plains zebras have more photobomb examples than Grévy's zebras, and the photobomb-state MCC is
          also much higher for that dataset.
        This suggests, that with more training data the quality of the classifier could improve considerably.
        The second observation is that the number of false negatives is lower than both the number of true
          positives and false positives --- \ie{} the precision of the classifier is high.
        This means that the classifier correctly flags most photobomb cases it considers.
        Therefore, it can be used to prevent the match-state classifier from automatically classifying these
          pairs as positive.
        However, in its current state, it also incorrectly prevents automatic classification of a significant
          number of pairs.

        %The overall quality of the secondary classifier is not ideal for both species, but the small amount of
        %  training data makes it difficult to draw conclusions from these experiments.

        %As the important features in~\cref{tbl:ImportantPBFeat} illustrate, the photobomb classifier seems to
        %  rely on global information such as time delta, GPS delta, and speed, because photobombs are more likely
        %  when taking images of several animals in a small time period.
        %The classifier also makes use of the spatial position of the feature correspondences, which can be
        %  indicative of a photobomb (\eg{} when all the matches are in the top left corner of an annotation).
        %This suggests that with more training data the classifier could be trained reliably used to predict when
        %  a match is a photobomb.

        \ConfusionPhotobomb{}

        \EvalMetricsPhotobomb{}

        %\ImportantPBFeat{}

    \FloatBarrier{}
    \subsection{Classifier experiment conclusions}
        In these experiments we have demonstrated that our pairwise match-state classifier is able to reliably
          separate positive from negative and incomparable cases.
        When compared to the LNBNN scores from~\cref{chap:ranking}, not only do these probabilities have more
          predictive power, they are interpretable, always ranging between $0$ and $1$.
        In practice, we have found that a validation dataset can be used to select a threshold --- to
          automatically classify pairs --- where the false positive rate is sufficiently low.
        %In practice, we have found these probabilities to be well calibrated, meaning that a validation dataset
        %  can be used to select a threshold --- to automatically classify pairs --- where the false positive rate
        %  is sufficiently low.
        Because the positive cases are well separated from the negative and incomparable cases, a significant
          number of automatic reviews is possible.
        Furthermore, because the classifier was trained using hard, moderate, and easy training examples, it is
          able to correctly re-rank results of the ranking algorithm, where the correct match was ranked in highly
          but not rank $1$.
        Lastly, because the classifier predicts probabilities independent of their position in the ranked list,
          it can be used to determine when a query individual is new --- \ie{} does not have a correct match in the
          database.

        The performance of secondary photobomb classifier is weaker, but this is likely due to a small amount of
          training data.
        Even in its weak state, it can be used to prevent automatic review of some photobomb cases.
        Because the photobomb classifier can only prevent automatic decision and does not make them, the cost of
          including it in our algorithms is small, and by doing so we will increase the amount of labeled training
          data from which a stronger photobomb classifier can be bootstrapped.
        %Therefore, the pairwise algorithm can be used to re-rank the of the ranking algorithm.


\section{Summary of pairwise classification}\label{sec:pairconclusion}

    In this chapter we have constructed a verification mechanism that can predict the probability that a pair of
    annotations is positive, negative, or incomparable. We have also constructed a secondary classifier that can
    predict when --- namely in the case of photobombs --- a pair of annotations might confuse the primary
    match-state classifier. This was done by constructing a feature vector that contains matching information about
    a pair of annotations. We have constructed a representative training set by selecting hard, moderate, and easy
    training examples. Our classifiers were learned using the random forest learning algorithm. Our experiments
    demonstrate that the match-state classifier is able to strongly separate positive and negative cases. The
    performance of the photobomb classifier was weaker, but could likely be improved with more training data.

    Based on our experiments, it is clear that the ranking algorithm is improved by an automatic verifier, but by
      themselves ranking and verification are not enough to robustly address animal identification.
    There is no mechanism for error recovery, nor is there a mechanism for determining when identification is
      complete.
    This means, the automatic review threshold must be set conservatively to avoid any errors, which results in
      more work for a human reviewer.
    These issues are addressed in \cref{chap:graphid} using a graph-based framework to manage the identification
      process.
    This framework will detect when errors have occurred, recover from the errors, and stop the identification
      process in a timely manner.
