%%% Experiment results

%
\begin{comment}
# Summarize Commands
python -m ibeis.scripts.gen_cand_expts --exec-parse_latex_comments_for_commmands --fname figdefexpt.tex
./regen_figdefexpt.sh --noshow
gvim ~/code/ibeis/regen_figdefexpt.sh
\end{comment}


% -------------------------------
% --- Baseline Experiments ---
% -------------------------------


\begin{comment}                                                                                                                                       
\end{comment}
                                                                                                                                                      
\begin{comment}                                                                                                                                       
python -m ibeis Chap3.draw_agg_baseline --diskshow

python -m ibeis Chap3.draw_all --dbs=GZ_Master1,PZ_Master1,GIRM_Master1
python -m ibeis Chap3.draw_all --db GZ_Master1
python -m ibeis Chap3.draw_all --db PZ_Master1
python -m ibeis Chap3.draw_all --db GIRM_Master1
\end{comment}

\newcommand{\BaselineExpt}{
    \begin{figure}[ht!]\centering
        \begin{subfigure}[h]{\textwidth}\centering\includegraphics[width=\textwidth]{figuresY/agg-baseline.png}\end{subfigure}
        \caption[\caplbl{BaselineExpt}Baseline experiment]{\caplbl{BaselineExpt}
    % ---
    The baseline experiment is a high-level indicator of the ranking accuracy of each species.
    We measure ranking accuracy using a single query and database annotation --- selected from different
      encounters --- per individual.
    The number of query annotations (\pvar{qsize}) and database annotations (\pvar{dsize}) are given for each
      species in the legend.
    % ---
        }
        \label{fig:BaselineExpt}
    \end{figure}
}

\begin{comment}                                                                                                                                       
python -m ibeis Chap3.draw_all --dbs=GZ_Master1,PZ_Master1
\end{comment}                                                                                                                                       



\begin{comment}

ibeis Chap3.measure smk --db=GZ_Master1
ibeis Chap3.draw smk --db=GZ_Master1 --diskshow

ibeis Chap3.measure smk --db=PZ_Master1
ibeis Chap3.draw smk --db=PZ_Master1 --diskshow

ibeis Chap3.measure smk --dbs=GZ_Master1,PZ_Master1
ibeis Chap3.draw smk --dbs=GZ_Master1,PZ_Master1 --diskshow
\end{comment}
\newcommand{\SMKExpt}{
    \begin{figure}[ht!]\centering
        \begin{subfigure}[h]{\textwidth}\centering\includegraphics[width=\textwidth]{figuresY/PZ_Master1/smk.png}\caption{plains zebras}\label{sub:SMKExptA}\end{subfigure}
        \begin{subfigure}[h]{\textwidth}\centering\includegraphics[width=\textwidth]{figuresY/GZ_Master1/smk.png}\caption{Grévy's zebras}\label{sub:SMKExptB}\end{subfigure}
    \caption[\caplbl{SMKExpt}SMK experiment]{\caplbl{SMKExpt}
    % ---
    The (VLAD based) SMK algorithm compared to our LNBNN ranking algorithm.
    The results demonstrate that LNBNN outperforms the ranking accuracy of SMK.
    The number of query/database annotations (\pvar{qsize} / \pvar{dsize}) are
    shown in the lower left.
    % ---
        }
        \label{fig:SMKExpt}
    \end{figure}
}



\begin{comment}
python -m ibeis Chap3.measure foregroundness --dbs=GZ_Master1,PZ_Master1
python -m ibeis Chap3.draw foregroundness --dbs=GZ_Master1,PZ_Master1 --diskshow

python -m ibeis -e draw_rank_cmc --db GZ_Master1   -a timectrl   -t baseline:fg_on=[True,False]  --show
\end{comment}
\newcommand{\ForegroundExpt}{
    \begin{figure}[ht!]\centering
        \begin{subfigure}[h]{\textwidth}\centering\includegraphics[width=\textwidth]{figuresY/PZ_Master1/foregroundness.png}\caption{plains zebras}\label{sub:ForegroundExptA}\end{subfigure}
        \begin{subfigure}[h]{\textwidth}\centering\includegraphics[width=\textwidth]{figuresY/GZ_Master1/foregroundness.png}\caption{Grévy's zebras}\label{sub:ForegroundExptB}\end{subfigure}
        \caption[\caplbl{ForegroundExpt}Foregroundness experiment]{\caplbl{ForegroundExpt}
            % ---
            Weighting the score of the feature correspondences using foregroundness results in more accurate
              identifications.
            % ---
        }
        \label{fig:ForegroundExpt}
    \end{figure}
}


\newcommand{\FGIntraExpt}{
    \begin{figure}[ht!]\centering
        \begin{subfigure}[h]{\textwidth}\centering\includegraphics[width=\textwidth]{figuresY/PZ_Master1/foregroundness_intra.png}\caption{plains zebras}\end{subfigure}
        \begin{subfigure}[h]{\textwidth}\centering\includegraphics[width=\textwidth]{figuresY/GZ_Master1/foregroundness_intra.png}\caption{Grévy's zebras}\end{subfigure}
        \caption[\caplbl{FGIntraExpt}Foregroundness experiment]{\caplbl{FGIntraExpt}
            % ---
            Applying foregroundness weights to feature correspondences improves the identification accuracy at
              the top rank by filtering matches in scenery.
            This experiment was performed by matching annotations within serveral occurrences.
            Thus, in this experiment \pvar{qsize} is a sum and \pvar{dsize} is an average.
            % ---
        }
        \label{fig:FGIntraExpt}
    \end{figure}
}

% -------------------------------
% --- Invariance Experiments ----
% -------------------------------


\newcommand{\InvarExpt}{
    \begin{figure}[ht!]\centering
        \begin{subfigure}[h]{\textwidth}\centering\includegraphics[width=\textwidth]{figuresY/PZ_Master1/invar.png}\caption{plains zebras}\label{sub:InvarExptA}\end{subfigure}
        \begin{subfigure}[h]{\textwidth}\centering\includegraphics[width=\textwidth]{figuresY/GZ_Master1/invar.png}\caption{Grévy's zebras}\label{sub:InvarExptB}\end{subfigure}
        \caption[\caplbl{InvarViewExpt}Feature invariance experiment]{\caplbl{InvarExpt}
            % ---
            Results of the feature invariance experiment, testing the effect of affine invariance (AI) and the
              query-side rotation heuristic (QRH).
            For plains zebras circular keypoints with the QRH are the most accurate.
            For Grévy's zebras enabling affine invariance works the best.
            The number of query/database annotations (\pvar{qsize} / \pvar{dsize}) are shown in the lower left.
            % ---
        }
        \label{fig:InvarExpt}
    \end{figure}
}

% -------------------------------
% --- K Experiments
% -------------------------------

% --- Explicit K 


%\MultiImageCommandII{KExpt}{1}{Results of the $\K$ experiment}{
%    % ---
%    Identification accuracy using different values of $\K$ (the number of
%      nearest neighbors assigned to each query feature).
%    \Cref{sub:KExptA} illustrates that the most accurate setting for the less
%      distinctive plains zebras is $\K\tighteq7$ when considering the \names{}
%      ranked first.
%    If the top two ranked \names{} are considered, then $\K\tighteq4$ provides
%      the most accurate results.
%    \Cref{sub:KExptB} illustrates that the most accurate setting for the more
%      distinctive Grévy's zebras is $\K\tighteq1$ when considering the \names{}
%      ranked first, and $\K\tighteq2$ when considering names ranked first or
%      second.
%    \Cref{sub:KExptC} illustrates that both $\K\tighteq1$ and $\K\tighteq2$
%      provide the most accurate results for Masai giraffes.
%%    % ---
%}{figuresX/expt_PZKTime.png}{figuresX/expt_GZKTime.png}{figuresX/expt_GIRMKTime.png}


% --- Database Size Experiments (with time)


\begin{comment}
python -m ibeis Chap3.measure kexpt --dbs=GZ_Master1,PZ_Master1
python -m ibeis Chap3.draw kexpt --dbs=GZ_Master1,PZ_Master1 --diskshow
\end{comment}
\newcommand{\KExptA}{
    \begin{figure}[ht!]\centering
        \centering\includegraphics[width=\textwidth]{figuresY/PZ_Master1/kexpt.png}
        \caption[\caplbl{KExptA}The $K$ experiment for plains zebras]{\caplbl{KExptA}
            % ---
            Identification accuracy for plains zebras using different values of $\K$ (the number of nearest
              neighbors assigned to each query feature), different numbers of exemplars (\pvar{dpername}), and
              different database sizes (\pvar{dsize}).
            %Note that the scores reported here are higher than the baseline for
            %  the same reasons as explained in~\cref{fig:DBSizeExpt}.
            % ---
        }
        \label{fig:KExptA}
    \end{figure}
}
\newcommand{\KExptB}{
    \begin{figure}[ht!]\centering
        \centering\includegraphics[width=\textwidth]{figuresY/GZ_Master1/kexpt.png}
        \caption[\caplbl{KExptB}The $K$ experiment for Grévy's zebras]{\caplbl{KExptB}
            % ---
            Identification accuracy for Grévy's zebras using different values of $\K$ (the number of nearest
              neighbors assigned to each query feature), different numbers of exemplars (\pvar{dpername}), and
              different database sizes (\pvar{dsize}).
            %Note that the scores reported here are higher than the baseline for
            %  the same reasons as explained in~\cref{fig:DBSizeExpt}.
            % ---
        }
        \label{fig:KExptB}
    \end{figure}
}

%\MultiImageCommandII{DBSizeExpt}{1}{Results of the database size experiment}{
%    % ---
%    Accuracy of rank $1$ identifications as a function of $\K$ with
%      different database sizes.
%    The value of $\K$ with the maximum accuracy is denoted in the
%      legend.
%    An asterisk ``*'' denotes that multiple values have this score.
%    While $\K$ does have an impact on matching accuracy, the
%      \emph{number of annotations per name} is the more important factor.
%    \Cref{sub:DBSizeExptA} shows the results for plains zebras.
%    \Cref{sub:DBSizeExptB} shows the results for Grévy's zebras.
%    \Cref{sub:DBSizeExptC} shows the results for Masai giraffes.
%    Note that the overall accuracy of this test is higher than the
%      baseline because this test requires multiple annotations per
%      \name{}.
%    This results in higher scores because of bias due to a smaller test
%      size and the groundtruth bias (\ie{} the identification algorithm
%      has already worked well on these \names{} with multiple
%      annotations).
%    % ---
%}{figuresX/expt_PZDBSizeTime.png}{figuresX/expt_GZDBSizeTime.png}{figuresX/expt_GIRMDBSizeTime.png}


% -------------------------------
% --- Namescore Experiments ----
% -------------------------------

\begin{comment}
python -m ibeis Chap3.measure nsum --dbs=GZ_Master1
python -m ibeis Chap3.measure nsum --dbs=GZ_Master1,PZ_Master1
python -m ibeis Chap3.draw nsum --dbs=GZ_Master1,PZ_Master1 --diskshow
\end{comment}

\newcommand{\NScoreExpt}{
    \begin{figure}[ht!]\centering
        \begin{subfigure}[h]{\textwidth}\centering\includegraphics[width=\textwidth]{figuresY/PZ_Master1/nsum.png}\caption{plains zebras}\label{sub:NScoreExptA}\end{subfigure}
        \begin{subfigure}[h]{\textwidth}\centering\includegraphics[width=\textwidth]{figuresY/GZ_Master1/nsum.png}\caption{Grévy's zebras}\label{sub:NScoreExptB}\end{subfigure}
        \caption[\caplbl{InvarViewExpt}Name scoring experiment]{\caplbl{NScoreExpt}
            % ---
            Results of the name scoring mechanism experiment.
            There is a clear separation between identification accuracy when the number of exemplars per name is
              $1$ compared to when it is $3$.
            Feature based name scoring (\nsum{}) is slightly more accurate than scoring using the annotation
              based name scoring (\csum{}).
            The number of query /database annotations (\pvar{qsize} / \pvar{dsize}) are shown in the lower left.
            Database size was normalized using confusors.
            %Note that the scores reported here are higher than the baseline for
            %  the same reasons as explained in~\cref{fig:DBSizeExpt}.
            % ---
        }
        \label{fig:NScoreExpt}
    \end{figure}
}


% -------------------------------
% --- Score Separability Experiments ----
% -------------------------------


% --- all cases
\begin{comment}
python -m ibeis -e scores --db PZ_Master1   -a timectrl -t best --filt : --hargv=scores  --prefix "Separability " --label PZScoreAll  
python -m ibeis -e scores --db PZ_Master1   -a timectrl -t best --filt :without_tag=photobomb --hargv=scores  --prefix "Separability " --label PZScoreAll  
\end{comment}

\begin{comment}
python -m ibeis -e scores --db GZ_Master1   -a timectrl -t best     --filt : --hargv=scores --prefix "Separability "  --label GZScoreAll
python -m ibeis -e scores --db GZ_Master1   -a timectrl -t best     --filt :without_tag=photobomb --hargv=scores --prefix "Separability "  --label GZScoreAll
python -m ibeis -e rank_cmc --db GZ_Master1   -a timectrl -t best best:chip_sqrt_area=700 --filt : --show
\end{comment}

\begin{comment}
python -m ibeis -e scores --db GIRM_Master1 -a timectrl1h -t best   --filt : --hargv=scores --prefix "Separability "  --label GIRMScoreAll
\end{comment}
\MultiImageCommandII{ScoreSep}{1}{
    The score separability for each species 
}{
    % ---
    The score separability for the best time controlled configuration of each
      species.
    \Cref{sub:ScoreSepA} shows the results for plains zebras.
    \Cref{sub:ScoreSepB} shows the results for Grévy's zebras.
    % ---
}{figuresX/expt_PZScoreAll.png}{figuresX/expt_GZScoreAll.png}

% -------------------------------
% --- Tag Histograms  ---
% -------------------------------

\begin{comment}
python -m ibeis -e taghist --db PZ_Master1   -a timectrl -t best --filt :fail=True --no-wordcloud --hargv=tags  --prefix "Failure " --label PZTags  --figsize=10,3  --left=.2
\end{comment}

\begin{comment}
python -m ibeis -e taghist --db GZ_Master1   -a timectrl -t best     --filt :fail=True --no-wordcloud --hargv=tags --prefix "Failure "  --label GZTags  --figsize=10,3   --left=.2
\end{comment}

\begin{comment}
python -m ibeis -e taghist --db GIRM_Master1 -a timectrl1h -t best   --filt :fail=True --no-wordcloud --hargv=tags --prefix "Failure "  --label GIRMTags  --figsize=10,3   --left=.2
\end{comment}

\MultiImageCommandII{TagExpt}{1}{
    Primary causes of identification failure
}{
    % ---
    A histogram indicating the primary causes of identification failures.
    Occlusion and viewpoint seem to be the primary causes of failure.
    \Cref{sub:TagExptA} shows the failure case histogram for plains zebras.
    \Cref{sub:TagExptB} shows the failure case histogram for Grévy's zebras.
    % ---
}{figuresX/expt_PZTags.png}{figuresX/expt_GZTags.png}


\begin{comment}

# Categorize and tag errors

python -m ibeis -e cases --db PZ_Master1   -a timectrl   -t best --filt :sortdsc=gfscore,fail=None,with_tag=BadTail --show
python -m ibeis -e cases --db PZ_Master1   -a timectrl   -t best --filt :sortdsc=gfscore,fail=None --show
python -m ibeis -e cases --db GZ_Master1   -a timectrl   -t best --filt :sortdsc=gfscore,fail=True --show
python -m ibeis -e cases --db GIRM_Master1 -a timectrl1h -t best --filt :sortdsc=gfscore,fail=None --show

python -m ibeis -e cases --db GIRM_Master1 -a timectrl    -t best --show  --filt :sortdsc=gfscore,fail=True
python -m ibeis -e cases --db GIRM_Master1 -a timectrl1h  -t best --show  --filt :sortdsc=gfscore,fail=True
python -m ibeis -e cases --db GIRM_Master1 -a timectrl    -t best --show  --filt :sortdsc=gfscore,fail=True

python -m ibeis -e cases --db GIRM_Master1 -a viewdiff  -t Ell --show  --filt :orderby=gfscore,reverse=1,fail=True

# Find untagged cases
python -m ibeis.dev -e cases --db PZ_Master1  -a timectrl   -t best --filt :sortdsc=gtscore,fail=True,max_tags=0 --show
python -m ibeis.dev -e cases --db GZ_Master1  -a timectrl   -t best --filt :sortdsc=gtscore,fail=True,max_tags=0 --show
python -m ibeis.dev -e cases --db GIRM_Master1  -a timectrl   -t best --filt :sortdsc=gtscore,fail=True,max_tags=0 --show

# Specialized untagged cases
python -m ibeis.dev -e cases --db PZ_Master1  -a timectrl   -t best --filt :sortdsc=gfscore,fail=True,min_gtscore=.0001 --show
python -m ibeis.dev -e cases --db PZ_Master1  -a timectrl   -t best --filt :sortdsc=gfscore,fail=True,max_gf_tags=0,max_gt_tags=0 --show
python -m ibeis.dev -e cases --db PZ_Master1  -a timectrl   -t best --filt :sortdsc=gfscore,fail=True,min_gtscore=.0001,max_gf_tags=0 --show

python -m ibeis.dev -e cases --db GZ_Master1  -a timectrl   -t best --filt :sortdsc=gfscore,fail=None,max_gf_tags=0,max_gt_tags=0 --show
python -m ibeis.dev -e cases --db GZ_Master1  -a timectrl   -t best --filt :sortasc=gtscore,fail=None,max_gf_tags=0,max_gt_tags=0 --show
python -m ibeis.dev -e cases --db GZ_Master1  -a timectrl   -t best --filt :sortdsc=gfscore,fail=True,max_gf_tags=0,max_gt_tags=0 --show
python -m ibeis.dev -e cases --db GZ_Master1  -a timectrl   -t best --filt :sortdsc=gfscore,fail=True,max_gt_tags=0 --show


# Find Pair Cases without Corresponding Single Tag
python -m ibeis.dev -e cases --db PZ_Master1  -a timectrl   -t best --filt :sortdsc=gfscore,fail=True,with_tag=Viewpoint,max_gtq_tags=0 --show
python -m ibeis.dev -e cases --db GZ_Master1  -a timectrl   -t best --filt :sortdsc=gfscore,fail=True,with_tag=Viewpoint,max_gtq_tags=0 --show
python -m ibeis.dev -e cases --db GIRM_Master1  -a timectrl   -t best --filt :sortdsc=gfscore,fail=True,with_tag=Viewpoint,max_gtq_tags=0 --show

python -m ibeis.dev -e cases --db PZ_Master1  -a timectrl   -t best --filt :sortdsc=gfscore,fail=True,with_tag=Quality,max_gtq_tags=0 --show
python -m ibeis.dev -e cases --db GZ_Master1  -a timectrl   -t best --filt :sortdsc=gfscore,fail=True,with_tag=Quality,max_gtq_tags=0 --show
python -m ibeis.dev -e cases --db GIRM_Master1  -a timectrl   -t best --filt :sortdsc=gfscore,fail=True,with_tag=Quality,max_gtq_tags=0 --show
\end{comment}
