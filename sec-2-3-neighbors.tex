    \section{Approximate nearest neighbor search}\label{sec:ann}  

        In computer vision applications it is often necessary to search a database of high dimensional
        vectors~\cite{shakhnarovich_nearest_neighbor_2006, datar_locality_sensitive_2004, muja_fast_2009,
        kulis_kernelized_2012, weiss_spectral_2009}. Patch descriptor vectors like SIFT are constructed such that
        the distance (under some metric) between vectors is small for matching patches and large for non-matching
        patches. Thus, finding matching descriptor vectors is often framed as a nearest neighbor search
        problem~\cite{lowe_distinctive_2004}. It becomes prohibitively expensive to perform exact nearest neighbor
        search as the size of the database increases. Therefore, approximate algorithms --- which can trade off a
        small amount of accuracy for substantial speed ups --- can be used instead.

        \subsection{Kd-tree}

            A \glossterm{kd-tree} is a data structure used to index high dimensional vectors for fast approximate
            nearest neighbor search~\cite{bentley_multidimensional_1975}. A kd-tree is an extension of a binary
            tree to multiple dimensions. Each non-leaf node of the tree is assigned a dimension and threshold
            value. The node splits data vectors between the left and right children by comparing the value of the
            data vector at the assigned dimension to the assigned threshold.

            \paragraph{Building a kd-tree index}
            Indexing a set of vectors involves first choosing a dimension and threshold to split the data into two
            partitions. Then this procedure is recursively to each of the partitions. A common measure for choosing
            the dimension is to choose the dimension with the greatest variance in the data. The threshold is then
            selected as the median value of the chosen dimension.

            \paragraph{Augmenting a kd-tree index}
            It is possible to augment an existing kd-tree by adding and removing vectors. Addition of a vector to a
            kd-tree is performed by appending the point to its assigned leaf. Removal of points from a kd-tree is
            done using lazy deletion --- \ie{} by masking the removed data. To avoid tree imbalance, a kd-tree is
            re-indexed after the number of points added or removed passes a threshold. Any masked point is deleted
            whenever the tree is re-indexed.

            \paragraph{Searching a kd-tree index}
            Searching for a query point's exact nearest neighbor in a kd-tree has been shown to take expected
            logarithmic time for low ($k < 16$) dimensional data~\cite{friedman_algorithm_1977}. However, for
            higher dimensional data this same method takes nearly linear time~\cite{sproull_refinements_1991}. This
            is because a query point and its nearest neighbor might be on opposite sides of a partition.
            Therefore, searching for nearest neighbors is typically done by approximate search using a priority
            queue~\cite{beis_shape_1997}.
            A priority queue orders nodes to further search based on their distance to the query vector. 
            The search returns the best result after a threshold number of checks have been made.
            % $ ~\cite{beis_shape_1997}.

            Search accuracy is improved by using multiple randomized kd-trees~\cite{silpa_anan_optimised_2008}. If
            a single kd-tree has a probability of failure $p$, then $m$ independently constructed trees have a
            $p^m$ probability of failure. For each kd-tree a random Householder matrix is used to efficiently
            rotate the data. Using a random rotation preserves distances between rotated vectors but does not
            preserve the dimension of maximum variance. This means that each of the $m$ kd-trees yields a different
            partitioning of the data, although it is not guaranteed to be independent. When searching multiple
            random kd-trees a single priority queue keeps track of the next nearest bin boundaries to search over
            all of the trees.

        \subsection{Hierarchical k-means}
            Another tree-based method for approximate nearest neighbor search is the hierarchical k-means. Each
            level in the hierarchical k-means tree partitions the data using the k-means
            algorithm~\cite{lloyd_least_1982} with a small value of $k$ (\eg{} 3). To query a new point it moves
            down the tree into the bin of the closest centroid at each level until it reaches a leaf node.
            Hierarchical k-means was one of the first techniques used to define a visual
            vocabulary~\cite{nister_scalable_2006} --- a structure used for indexing and quantizing large amounts
            of descriptors.
        
        \subsection{Locality sensitive hashing}
            A hashing-based method for approximate nearest neighbor search is locality-sensitive hashing (LSH).
            This method is able to search a dataset of vectors for approximate nearest neighbors in sub-linear
            time~\cite{charikar_similarity_2002, datar_locality_sensitive_2004, kulis_fast_2009,
            kulis_kernelized_2012, tao_locality_2013}. LSH trades off a small amount of accuracy for a large query
            speed-up. A database is indexed using $M$ hash tables. Each hash table uses its own randomly selected
            hash function. For each hash table, a query vector computes its hash and adds the database vectors it
            collided with to a shortlist. The shortlist is sorted by distance and returned as the approximate
            nearest neighbors.

        \subsection{FLANN}
            The fast library for approximate nearest neighbors (FLANN) is a software package built to quickly index
            and search high dimensional vectors~\cite{muja_fast_2009}. The FLANN package implements efficient
            algorithms for hierarchical k-means, kd-trees, and LSH{}. It also implements a hybrid between the
            k-means and kd-tree, as well as configuration optimization, to select the combination of algorithms
            that best reaches the desired speed/accuracy trade-off for a given dataset. Configuration optimization
            is performed using the Nelder-Mead downhill simplex method~\cite{nelder_simplex_1965} with
            cross-validation.

        \subsection{Product quantization}
            Product quantization is a method for speeding up approximate nearest neighbor search of a set of high
            dimensional vectors~\cite{jegou_product_2011,ge_optimized_2013}. Each vector is split up into a set of
            sub-vector components. For each component, the sub-vectors are separately quantized using a
            codebook/dictionary/vocabulary. The pairwise squared distances between centroids in the vocabulary are
            stored in a lookup table. To comparing the distance between two vectors first each vector is split into
            sub-vectors, next the sub-vectors are quantized, and then the squared distances between quantized
            sub-vectors are read from the lookup table. The approximated squared distance between these two vectors
            is the sum of the squared distances between the quantized sub-vectors.

        \subsection{Discussion --- choice of approximate nearest neighbor algorithm}
            In our single annotation identification algorithm a query descriptor searches for its nearest neighbor
            in a database containing all descriptors from all \exemplars{}. Each annotation contains
            \OnTheOrderOf{4} features, which are described with $128$-component SIFT descriptors. Searching exact
            nearest neighbors becomes prohibitive when hundreds or thousands of images are searched. Thus, we turn
            towards approximate nearest neighbor algorithms. In this \thesis{} all of our approximate nearest
            neighbors are found using the multiple kd-tree implementation in the FLANN
            package~\cite{muja_fast_2009}. Using the configuration optimization built into the FLANN package, we
            have found that multiple kd-trees provide more accurate feature matches for our datasets than those
            computed by hierarchical k-means trees or LSH{}. In addition to being fast and accurate, multiple
            kd-trees support efficient addition and removal of points, which is needed in a dynamic
            setting~\cite{silpa_anan_optimised_2008}.
