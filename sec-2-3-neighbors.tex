    \section{Approximate nearest neighbor search}\label{sec:ann}  

        In computer vision applications it is often necessary to search
          a database of high dimensional
          vectors~\cite{shakhnarovich_nearestneighbor_2006,
          datar_localitysensitive_2004, muja_fast_2009,
          kulis_kernelized_2012, weiss_spectral_2009}.
        Patch descriptor vectors like SIFT are constructed such that
          the distance (under some metric) between vectors is small for
          matching patches and large for non-matching patches.
        Thus, finding matching descriptor vectors is often framed as a
          nearest neighbor search problem~\cite{lowe_distinctive_2004}.
        It becomes prohibitively expensive to perform exact nearest
          neighbor search as the size of the database increases.
        Therefore, approximate algorithms --- which can trade off a
          small amount of accuracy for substantial speed ups --- can be
          used instead.

        \subsection{Kd-tree}
        %\chuckcomment{You need a bit more discussion here}

            A \glossterm{kd-tree} is a data structure used to index
              high dimensional vectors for fast approximate nearest
              neighbor search~\cite{bentley_multidimensional_1975}.
            %The kd-tree has been applied to the nearest neighbor search of image
              %descriptors \cite{lowe_distinctive_2004}.
            A kd-tree is subclass of a binary tree.
            Each non-leaf node of the tree is assigned a dimension and
              threshold value.
            The node splits data vectors between the left and right
              children by comparing the value of the data vector at the
              assigned dimension to the assigned threshold.

            \paragraph{Building a kd-tree index}
            Indexing a set of vectors involves first choosing a
              dimension and threshold to split the data into two
              partitions.
            Then this procedure is recursively to each of the
              partitions.
            A common measure for choosing the dimension is to choose
              the dimension with of greatest variance in the data.
            The threshold is then chosen as the median value of the
              chosen dimension.

            \paragraph{Augmenting a kd-tree index}
            It is possible to augment an existing kd-tree by adding and
              removing vectors.
            Addition of a vector to a kd-tree is performed by appending
              the point to its assigned leaf.
            Removal of points from a kd-tree is done using lazy
              deletion --- \ie{} by masking the removed data.
            To avoid tree imbalance, a kd-tree is re-indexed after the
              number of points added or removed passes a threshold.
            Any masked point is deleted whenever the tree is
              re-indexed.

            \paragraph{Searching a kd-tree index}
            Searching for a query point's exact nearest neighbor in a
              kd-tree has been shown to take expected logarithmic time
              for low ($k < 16$) dimensional
              data~\cite{friedman_algorithm_1977}.
            However, for higher dimensional data this same method takes
              nearly linear time~\cite{sproull_refinements_1991}.
            This is because a query point and its nearest neighbor
              might be on opposite sides of a partition.
            %first leaf node checked may not contain
            %  the true nearest neighbor.
            %Exhaustive search of a kd-tree with $N$ data vectors takes
            %  $\bigoh{N}$ comparisons because the first leaf node checked
            %  may not contain the true nearest neighbor.
            % \cite{friedman_algorithm_1977}
            Therefore, searching for nearest neighbors is typically
              done by approximate search using a priority
              queue~\cite{beis_shape_1997}.
              %makes $\bigoh{N\log{N}}$
              %comparisons~\cite{beis_shape_1997}.
            %Approximate nearest neighbor search is performed using the
            %best-bin first algorithm with 
            A priority queue orders nodes to further search based on
              their distance to the query vector. % $ ~\cite{beis_shape_1997}.
            The search returns the best result after a threshold number
              of checks have been made.
            %and yields
            %  good results.

            Search accuracy is improved by using multiple randomized
              kd-trees~\cite{silpaanan_optimised_2008}.
            If a single kd-tree has a probability of failure $p$, then
              $m$ independently constructed trees have a $p^m$
              probability of failure.
            For each kd-tree a random Householder matrix is used to
              efficiently rotate the data.
            Using a random rotation preserves distances between rotated
              vectors but does not preserve the dimension of maximum
              variance.
            This means that each of the $m$ kd-trees yields a different
              partitioning of the data, although it is not guaranteed to
              be independent.
            When searching multiple random kd-trees a single priority
              queue keeps track of the next nearest bin boundaries to
              search over all of the trees.

            %A popular method for indexing high dimensional vectors is the
            %  kd-tree \cite{bentley_multidimensional_1975, lowe_distinctive_2004}.
            %Typically indexing is performed with multiple randomized kd-trees
            %  \cite{silpaanan_optimised_2008}.
            %Each level in the kd-tree splits the database vectors randomly along
            %  the top $D\tighteq5$ dimensions with the highest variance.

        \subsection{Hierarchical k-means}
            Another tree-based method for approximate nearest neighbor
              search is the hierarchical k-means.
            Each level in the hierarchical k-means tree partitions the
              data using the k-means algorithm~\cite{lloyd_least_1982}
              with a small value of $k$ (\eg{} 3).
            %The leafs of the tree defines a centroid  
            To query a new point it moves down the tree into the bin of
              the closest centroid at each level until it reaches a leaf
              node.
            Hierarchical k-means was one of the first techniques used
              to define a visual vocabulary~\cite{nister_scalable_2006}
              --- a structure used for indexing and quantizing large
              amounts of descriptors.
        
        \subsection{Locality sensitive hashing}
            A hashing-based method for approximate nearest neighbor
              search is locality-sensitive hashing (LSH).
            This method is able to search a dataset of vectors for
              approximate nearest neighbors in sub-linear
              time~\cite{charikar_similarity_2002,
              datar_localitysensitive_2004, kulis_fast_2009,
              kulis_kernelized_2012, tao_locality_2013}.
            LSH trades off a small amount of accuracy for a large query
              speed-up.
            A database is indexed using $M$ hash tables.
            Each hash table uses its own randomly selected hash
              function.
            For each hash table, a query vector computes its hash and
              adds the database vectors it collided with to a shortlist.
            The shortlist is sorted by distance and returned as the
              approximate nearest neighbors.

        \subsection{FLANN}
            The Fast Library for Approximate Nearest Neighbors (FLANN)
              is a software package built to quickly index and search
              high dimensional vectors~\cite{muja_fast_2009}.
            The FLANN package implements efficient algorithms for
              Hierarchical k-means, kd-trees, and LSH{}.
            It also implements a hybrid between the k-means and
              kd-tree, as well as configuration optimization, to select
              the combination of algorithms that best reaches the desired
              speed/accuracy trade-off for a given dataset.
            Configuration optimization is performed using the
              Nelder-Mead downhill simplex
              method~\cite{nelder_simplex_1965} with cross-validation.


        \subsection{Product quantization}
            %\chuckcomment{Move down}
            %\devcomment{PQ is a nearest neighbor technique, This seems like the best place for this to fix}
        
            Product quantization is a method for speeding up
              approximate nearest neighbor search of a set of high
              dimensional
              vectors~\cite{jegou_product_2011,ge_optimized_2013}.
            Each vector is split up into a set of sub-vector
              components.
            For each component, the sub-vectors are separately
              quantized using a codebook/dictionary/vocabulary.
            The pairwise squared distances between centroids in the
              vocabulary are stored in a lookup table.
            To comparing the distance between two vectors first each
              vector is split into sub-vectors, next the sub-vectors are
              quantized, and then the squared distances between quantized
              sub-vectors are read from the lookup table.
            The approximated squared distance between these two vectors
              is the sum of the squared distances between the quantized
              sub-vectors.

        \subsection{Discussion --- choice of approximate nearest neighbor algorithm}

            In our single annotation identification algorithm a query
              descriptor searches for its nearest neighbor in a database
              containing all descriptors from all \exemplars{}.
            Each annotation contains \OnTheOrderOf{4} features, which
              are described with $128$-component SIFT descriptors.
            Searching exact nearest neighbors becomes prohibitive when
              hundreds or thousands of images are searched.
            Thus, we turn towards approximate nearest neighbor
              algorithms.
            In this \thesis{} all of our approximate nearest neighbors
              are found using the multiple kd-tree implementation in the
              FLANN package~\cite{muja_fast_2009}.
            Using the configuration optimization built into the FLANN
              package, we have found that multiple kd-trees provide more
              accurate feature matches for our datasets than those
              computed by hierarchical k-means trees or LSH{}.
            In addition to being fast and accurate, multiple kd-trees
              support efficient addition and removal of points, which is
              needed in a dynamic
              setting~\cite{silpaanan_optimised_2008}.

            %This is consistent with other research.


            %Determining how features should 

            %How should image feature be matched?
            %How should can all of this be done efficiently?

            %Hashing for approximate nearest neighbor search is designed to
            %  trade-off accuracy for speed.
            %In our application we would prefer to trade-off speed for
            %  accuracy.

